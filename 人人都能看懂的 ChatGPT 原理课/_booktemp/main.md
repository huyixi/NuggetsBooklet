---
title: 人人都能看懂的 ChatGPT 原理课
author: 人人都能看懂的 ChatGPT 原理课
date: 2025-02-14
lang: zh-CN
---

## 1.概览：ChatGPT 与自然语言处理

ChatGPT（Chat Generative Pre-training Transformer） 是一个 AI 模型，属于**自然语言处理（** **Natural Language Processing** **，** **NLP** **）** 领域，NLP 是人工智能的一个分支。所谓自然语言，就是人们日常生活中接触和使用的英语、汉语、德语等等。自然语言处理是指，让计算机来理解并正确地操作自然语言，完成人类指定的任务。NLP 中常见的任务包括文本中的关键词抽取、文本分类、机器翻译等等。

  


NLP 当中还有一个非常难的任务：**对话系统**，也可被笼统称为**聊天机器人**，正是 ChatGPT 所完成的工作。

  


## ChatGPT 与图灵测试

自从 1950 年代出现计算机以来，人们就已经开始着手研究让计算机辅助人类理解、处理自然语言，这也是 NLP 这一领域的发展目标，最著名的当属**图灵测试**。

  


> 1950年，计算机之父——艾伦·图灵(Alan Turing)介绍了一项测试，以检查机器是否能像人类一样思考，这项测试称为**图灵测试**。它具体的测试方法和目前 ChatGPT 的方式一模一样，即**构建一个计算机对话系统**，一个人和被测试的模型互相进行对话，如果这个人无法辨别对方究竟是机器模型还是另一个人，就说明该模型通过了图灵测试，计算机是智能的。

  


长久以来，图灵测试都被学界认为是难以攀登的巅峰。正因如此，NLP 也被称为人工智能皇冠上的明珠。而 ChatGPT 所能够做的工作，已经远远超出了聊天机器人这个范畴，它能够根据用户的指令写文章，回答技术问题，做数学题，做外文翻译，玩文字游戏等等。所以，某种程度上，ChatGPT 已经摘下了这颗皇冠上的明珠。

  


## ChatGPT 的建模形式

ChatGPT 的工作形式非常简单，用户向 ChatGPT 提问任何一个问题，模型都会做出解答。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/99803b32837a42a4982bcad7e7a3f70d~tplv-k3u1fbpfcp-zoom-1.image)

其中，用户的输入和模型的输出都是**文字**形式。一次用户输入和一次模型对应的输出，叫做一轮对话。我们可以把 ChatGPT 的模型抽象成如下流程：


![1-1.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/766be46bb0984b3d8927a139606666e4~tplv-k3u1fbpfcp-watermark.image?)

此外，ChatGPT 也可以回答用户的连续提问，也就是多轮对话，多轮对话之间是有信息关联的。其具体的形式也非常简单，第二次用户输入时，系统默认把第一次的输入、输出信息都拼接在一起，供 ChatGPT 参考上次对话的信息。


![1-2.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d7c2e629746a410dbf20b2068f2e49ac~tplv-k3u1fbpfcp-watermark.image?)

如果用户与 ChatGPT 对话的轮次过多，一般来讲模型仅会保留最近几轮对话的信息，此前的对话信息将被遗忘。


![1-3.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d32e1a7df14f45a1ae80293bedcee1ce~tplv-k3u1fbpfcp-watermark.image?)

ChatGPT 在接收到用户的提问输入后，输出的文字并不是一口气直接生成的，而是一个字、一个字生成的，这种逐字生成，即**生成式（Generative）** 。如下图所示。



![1-4.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aaaaa8a145ac41b1903aadd503d5af26~tplv-k3u1fbpfcp-watermark.image?)

当用户输入问句：“你喜欢苹果还是香蕉？”，ChatGPT 接收到数据之后，首先会生成一个“我”字，然后，模型会综合用户的问句和生成的“我”字，继续生成下一个字“喜”。以此类推，直到生成一个完整的句子“我喜欢苹果。”。

  


## ChatGPT 与 NLP 的发展历程

  


前面介绍了 ChatGPT 的建模形式，可以试想一下，如果让你来实现一个 ChatGPT 模型，有哪些思路和方法呢？事实上，大致有两种策略，基于**规则**的 NLP 和基于**统计**的 NLP。自从 ChatGPT 开始，NLP 领域又进入了**强化学习**时代，即基于强化学习的 NLP。

### 基于规则的 NLP

基于规则的 NLP，是指使用人工编写的规则来处理自然语言。这些规则通常基于语法、语义和语用等方面的知识，可以用来解析和生成自然语言。例如，我们可以基于以下规则设计一个对话系统：

> 规则 1：当模型接收到用户的问句后，把问句中的“吗”字去掉，“？”换成“。”
>
> 规则 2：把“你”换成“我”，“我”字换成“你”

  


由此，我们可以根据这些规则，制作一个对话模型，开启对话模式了。

  


> 用户：Hello。
>
> 模型：Hello。
>
> 用户：你是 ChatGPT吗？
>
> 模型：我是 ChatGPT。
>
> 用户：你喜欢掘金社区吗？
>
> 模型：我喜欢掘金社区。
>
> 用户：你用过 jionlp 软件工具包吗？
>
> 模型：我用过 jionlp 软件工具包。

  


以上是一个基于规则的非常粗浅的对话系统示例。其中存在的问题，相信读者能够很容易找出来。如果用户问题太复杂了怎么办？问题中没有加问号怎么办？我们需要不断编写出各种规则来覆盖上面的特殊情况。这说明基于规则存在几个明显的缺点：

1.  在自然语言中，任何规则都无法完全覆盖需求，因此在处理复杂的自然语言任务时效果不佳；
2.  规则无穷无尽，靠人力来完成将是一项天量的工作；
3.  本质上并没有把自然语言处理的任务交给计算机来完成，依然是人在主导。

  


这就是 NLP 发展早期的方式方法：基于规则完成模型系统构建。在早期，一般也被称为符号主义。

  


### 基于统计的 NLP

  


基于统计的 NLP 则是利用机器学习算法**从大量的语料库中学习自然语言的规律特征**，在早期也被称为连接主义。这种方法不需要人工编写规则，规则主要通过学习语言的统计特征，暗含在模型中。换句话说，基于规则的方法中，规则是显性的，人工编写的；基于统计的方法中，**规则是隐形的，暗含在模型参数中，由模型根据数据训练得到**。

  


这些模型，在近年来发展迅速，ChatGPT 就是其中一种。除此之外，还有各式各样不同形态构造的模型，其根基原理是相同的。它们的处理方式主要如下：

> 标注数据 => 建立模型、确定输入输出 => 训练模型 => 利用已训练好的模型进行工作

  


在 ChatGPT 中，主要采用**预训练（** Pre-training **）** 技术来完成基于统计的 NLP 模型学习。最早，NLP 领域的预训练是由 ELMO 模型（Embedding from Language Models）首次引进的，后续 ChatGPT 等各种深度神经网络模型广泛采用了这种方式。

它的重点在于，根据大规模原始语料学习一个语言模型，而这个模型并不直接学习如何解决具体的某种任务，而是学习从语法、词法、语用，到常识、知识等信息，把它们融汇在语言模型中。直观地讲，它更像是一个知识记忆器，而非运用知识解决实际问题。

预训练的好处很多，它已经成为了几乎所有 NLP 模型训练的必备步骤。我们将在后续章节展开讲。

  


基于统计的方法远远比基于规则的方法受欢迎，然而它最大的缺点是**黑盒不确定性，即规则是隐形的，暗含在参数中**。例如，ChatGPT 也会给出一些模棱两可、不知所云的结果，我们无从依照结果来判断模型为什么给出这样的答案。

  


![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ed29f0cf03a14ea1974bc86caed90764~tplv-k3u1fbpfcp-zoom-1.image)

  


### 基于强化学习的 NLP

ChatGPT 模型是基于统计的，然而它又利用了新的方法，**带人工反馈的强化学习（Reinforcement Learning with Human Feedback，RLHF）** ，以此取得了卓越的效果，把 NLP 的发展带入了一个新阶段。

  


几年前，Alpha GO 击败了柯洁。这几乎可以说明，强化学习如果在适合的条件下，完全可以打败人类，逼近完美的极限。当前，我们依然处在**弱人工智能**时代，但局限于围棋这个领域，Alpha GO 就是一个**强人工智能**，它的核心就在于强化学习 **。**

  


所谓强化学习，就是一种机器学习的方法，旨在让智能体（agent，在 NLP 中主要指深度神经网络模型，就是 ChatGPT 模型）通过与环境的交互来学习如何做出最优决策。

  


> 这种方式就像是训练一只狗（智能体）听哨声（环境）进食（学习目标）。
>
> 一只小狗，当听到主人吹哨后，就会被奖励食物；而当主人不吹哨时，小狗只能挨饿。通过反复的进食、挨饿，小狗就能建立起相应的条件反射，实际上就是完成了一次强化学习。

  


而在 NLP 领域，这里的环境要复杂得多。针对 NLP 模型的环境并非真正的人类语言环境，而是人为构造出来的一种语言环境模型。因此，这里强调是带人工反馈的强化学习。


![1-5.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cc6e74b6794a4f6189e937154c413573~tplv-k3u1fbpfcp-watermark.image?)

  


基于统计的方式能够让模型以最大自由度去拟合训练数据集；而强化学习就是赋予模型更大的自由度，让模型能够自主学习，突破既定的数据集限制。ChatGPT 模型是融合统计学习方法和强化学习方法的，它的模型训练流程如下图所示：


![1-6.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2913fcf2203942e880bbdfa428260e35~tplv-k3u1fbpfcp-watermark.image?)

这部分训练流程将在第 8-11 节展开讲。

  


### NLP 技术的发展脉络

  


实际上，**基于规则、基于统计、基于强化学习** **这** **三种方式，并不仅仅是一种处理自然语言的手段，而是一种思想**。一个解决某一问题的算法模型，往往是融合了这三种解决思想的产物。

  


> 如果把计算机比作一个小孩，自然语言处理就像是由人类来教育小孩成长。
>
>   
>
>
> 基于规则的方式，就好比家长 100% 控制小孩，要求他按照自己的指令和规则行事，如每天规定学习几小时，教会小孩每一道题。整个过程，强调的是**手把手教**，主动权和重心都在家长身上。对于 NLP 而言，整个过程的主动权和重心，都在编写语言规则的程序员、研究员身上。
>
>   
>
>
> 基于统计的方式，就好比家长只告诉小孩学习方法，而不教授具体每一道题，强调的是**半引导**。对于 NLP 而言，学习重心放在神经网络模型上，但主动权仍由算法工程师控制。
>
>   
>
>
> 基于强化学习的方式，则好比家长只对小孩制定了教育目标，比如，要求小孩能够考试达到 90 分，但并不去管小孩他是如何学习的，全靠**自学完成**，小孩拥有极高的自由度和主动权。家长只对最终结果做出相应的*奖励或惩罚*，不参与整个教育过程。对于 NLP 来说，整个过程的重心和主动权都在于模型本身。

  


NLP 的发展一直以来都在逐渐向基于统计的方式靠拢，最终由基于强化学习的方式取得完全的**胜利**，胜利的标志，即 **ChatGPT** **的问世**；而基于规则方式逐渐式微，沦为了一种辅助式的处理手段。ChatGPT 模型的发展，从一开始，就在坚定不移地沿着让模型自学的方向发展进步着。

  


## ChatGPT 的神经网络结构 Transformer

前面的介绍中，为了方便读者理解，没有提 ChatGPT 模型内部的具体构造。

  


ChatGPT 是一个大型的神经网络，其内部结构是由若干层 Transformer 构成的，Transformer 是一种神经网络的结构。自从 2018 年开始，它就已经成为了 NLP 领域的一种通用的标准模型结构，Transformer 几乎遍布各种 NLP 模型之中。


![1-7.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9a29d758574e48558eef677c1f4488ee~tplv-k3u1fbpfcp-watermark.image?)

  


> 如果说，ChatGPT 是一幢房子的话，那么，Transformer 就是构建 ChatGPT 的砖头。

  


Transformer 的核心是**自注意力机制**（Self-Attention），它可以帮助模型在处理输入的文字序列时，自动地关注到与当前位置字符相关的其他位置字符。自注意力机制可以将输入序列中的每个位置都表示为一个向量，这些向量可以同时参与计算，从而实现高效的并行计算。举一个例子：

  


> 在机器翻译中，在将英文句子 "I am a good student" 翻译成中文时，传统的机器翻译模型可能会将其翻译成 "我是一个好学生"，但是这个翻译结果可能不够准确。英文中的冠词“a”，在翻译为中文时，需要结合上下文才能确定。
>
> 而使用 Transformer 模型进行翻译时，可以得到更加准确的翻译结果，例如 "我是一名好学生"。

  


这是因为 Transformer 能够更好地捕捉英文句子中，跨越很长距离的词汇之间的关系，解决**文本上下文的长依赖**。自注意力机制将在第 5-6 节展开介绍，Transformer 结构详解将在第 6-7 节展开介绍。

  


# 总结

-   NLP 领域的发展逐渐由人为编写规则、逻辑控制计算机程序，到完全交由网络模型去适应语言环境。
-   ChatGPT 是目前最接近通过图灵测试的 NLP 模型，未来GPT4、GPT5将会更加接近。
-   ChatGPT 的工作流程是一个生成式的对话系统。
-   ChatGPT 的训练过程包括语言模型的预训练，RLHF 带人工反馈的强化学习。
-   ChatGPT 的模型结构采用以自注意力机制为核心的 Transformer。

后续章节，我们将对这些内容一一展开讲解。

## 10.模型训练基础：什么是强化学习？

强化学习上一次为大众熟知，还是 2017 年围棋人工智能模型 AlphaGO 打败柯洁的时候。AlphaGo 是由 Google DeepMind 开发的人工智能程序，它使用了深度强化学习算法，能够通过自我学习和对弈经验不断提高自己的水平，它充分展现了强化学习的效果和能力。而 ChatGPT 则将强化学习引入了 NLP 领域，展现出类似人的智能效果。

  


本节主要简单介绍一下强化学习的基本概念，以及它在 NLP 中的建模情况，为学习 RLHF 方法做一个铺垫。

  


# 强化学习基本概念

  


强化学习是一种机器学习方法，旨在让**智能体（Agent，即人工智能模型）通过与环境的交互来学习如何做出最优决策（Policy）。在强化学习中，智能体根据所处的环境（Environment）中的状态（State），通过执行动作（Action）来影响环境，并从环境中获得奖励（Reward）或惩罚**。智能体的目标是通过学习最大化长期奖励来制定最佳策略。

  


强化学习非常像生物的进化，通过不断地突变基因，由环境来筛选，进而适应环境，生存下来。强化学习的应用非常广泛，主要有游戏、自然语言处理等领域。

  


它的基本建模图如下所示。在超级玛丽奥游戏中，马里奥主人公就是一个由人或模型操控的智能体，游戏的每一个关卡，就是强化学习中的环境。我们玩马里奥游戏的过程，实际上就是一个强化学习的最好例子：从不会玩游戏，通过一次次不断地尝试、失败，最后练成一个马里奥游戏高手，成功通关。这说明了强化学习的本质，是一种 Trial & Failed 学习模式，用中文表达，就是**屡战屡败，屡败屡战，总结经验，获得成功**。用一句俗话来概括，强化学习就是“吃一堑，长一智”。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4c604493ca5645ed9344dce48c3d241e~tplv-k3u1fbpfcp-zoom-1.image)

  


## 强化学习要素

接下来，我们借助马里奥的例子来解释一下强化学习的几大要素。

-   状态（State）：环境和智能体共同构成的整体状态，这个状态与时间有关，因为每时每刻，状态会变动，智能体可能会改变，环境也可能会变。一般用字母 s 来表示状态，所有时刻的状态构成了一个集合，$$s \in S$$ 这是一个有限状态的集合。

> 在超级马里奥游戏中，玩家每时每刻所处的位置、以及游戏画面中的各种物体和危险障碍都属于游戏的状态。
>
> 在围棋中，棋盘上对弈双方棋子的分布情况，就是围棋当前的状态。

  


-   动作（Action）：智能体可以做出的动作描述，所有的动作构成了一个集合，$$a \in A$$ 是一个有限动作集合。

> 在超级马里奥游戏中，玩家可以通过手柄上下左右的方向键，以及射击和跳跃键，总共 6 个按键来控制马里奥的动作。
>
> 在围棋中，对弈的一方可以把棋子下在棋盘某个位置。围棋棋盘的大小为 19*19，因此，智能体可选择的动作范围最多就是 361。

  


-   策略（Policy）：环境的感知状态到行动的映射方式，$$\pi(s) \to a$$，其含义为，根据当前状态，设计决策函数，采取某种动作。

> 在超级马里奥游戏中，玩家观察到前方有金币（状态），所以按下跳跃键（动作）获取金币，这就是一种游戏操作策略。
>
> 在围棋中，对弈的一方观察棋盘情况（状态），决定把棋子下在哪个位置（动作）。


![10-1.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/082b5fbdffe146f4aa8adef8128a88ee~tplv-k3u1fbpfcp-watermark.image?)

-   反馈、奖励（Reward）：环境对智能体行动的反馈。在根据当前一个状态做出决策后，智能体将处于下一个状态，并得到一个反馈值。

> 在超级马里奥游戏中，玩家操作马里奥刚吃完金币（当前状态），按下前进键（动作），刚好撞上前方的乌龟（下一个状态），游戏失败（反馈）就是游戏环境对玩家的一种反馈，也就是惩罚（负奖励）。
>
> 在围棋中，对弈的一方把棋子下在一个正确的位置（动作），直接制胜棋局（反馈），就是围棋对棋手的奖励反馈。


  
![10-2.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/84726b9fb3324c35be43155419d949ad~tplv-k3u1fbpfcp-watermark.image?)


## 一条强化学习路径

现在，我们可以得到一条**状态-策略-反馈**的路径：

$$s_0 \to a_0 \to r_0 \to s_1 \to a_1 \to r_1 \to ... \to s_t \to a_t \to r_t$$

这样一条**路径**也被称作一次**采样**，或一条**轨迹**。

> 以马里奥游戏为例，这条路径实际上就是玩家玩游戏的全过程：
>
> 平坦的路 => 按前进键 => 前方有乌龟 => 按射击键 => 前方有陷阱 => 按跳跃键 => 营救公主 => 胜利
>
> 每一次玩游戏都是一次强化学习的试验，在数学上可以看作是一次强化学习的路径采样。在这个例子中，reward 反馈就是玩家是否营救到公主，或者遇到乌龟挂掉。所以，需要经历一条完整的游戏过程才能确定 reward 值。假设玩家成功通关记为 1，失败记为 0，对应到上述标准路径中，每一次的 reward 值都是相同的，都是 1。

  


继续观察这样一条路径，当我们在 $$s$$ 状态时，做下一次动作决策，实际上所依赖的经验完全基于当前的状态。这种只依赖当前状态的特性，就叫做**马尔可夫性**。

> 以马里奥游戏为例，当玩家决定按下跳跃键，以此跳跃陷阱时，之前游戏路径上是否有乌龟，对后续的状态和决策是毫无影响的。

  


## 价值函数

根据前面的基本概念定义，我们可以得到：一个智能体需要不断更新自己的策略函数，以期达到最优的效果。那么如何定义这个效果呢？这里就需要用到价值函数。

> 例如，在一盘围棋中，我执黑棋在某个位置下了一颗子，这个子对于我是否能赢得棋局胜利，究竟有何影响，主要体现在接下来的棋局状况之中。
>
> 换句话说，一条策略的价值，需要衡量它对接下来的操作步骤的影响，设计价值函数也就需要关心策略对未来操作的影响。

### 状态价值函数

状态价值函数（State Value Function）：该函数是针对一个策略 $$\pi$$ 而言的，它是指从状态 $$s$$ 出发，遵循策略 $$\pi$$ 能够获得的期望回报。

$$V^{\pi}(s)=E_{\pi}[G_t|S_t=s]$$

其中：

$$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + ... + \gamma^T R_{t+T} = \sum^T_0 \gamma^kR_{t+T}$$

首先，状态价值函数 $$V^{\pi}(s)$$ 并不是已经兑现的一条轨迹。它是一个期望值，描述了在状态 $$s$$ 之后，动作轨迹上的累积反馈奖励 $$G_t$$，它把接下来的 $$T$$ 步内的反馈奖励做了一个加权的求和。$$\gamma$$ 是一个随着时间逐渐减退的影响因子，$$\gamma \in (0, 1)$$，其含义为当前的策略更加关注就近的奖励回报，而不太在乎未来步骤的 reward。

> 以马里奥游戏为例：
>
> 轨迹：平坦的路 => 按前进键 => 前方有乌龟 => 按射击键 => 前方有陷阱 => 按跳跃键 => 营救公主 => 胜利
>
> 涉及三个状态：平坦的路、前方有乌龟、前方有陷阱。
>
> 涉及三个动作：按前进键、按射击键、按跳跃键。
>
> 营救公主，获取奖励值 1。
>
> 以此我们可以根据计算出 “平坦的路” 这个状态的价值。

  


### 动作价值函数

与状态价值函数相类似，动作价值函数指在当前状态 $$s$$，执行动作 $$a$$ 之后，依旧遵循策略 $$\pi$$ 可以获得的期望回报价值。

$$Q^{\pi}(s,a)=E_{\pi}[G_t|S_t=s, A_t=a]$$

> 假设，小李的手上有 1 万份股票（状态），而股票的价格在未来会波动，其潜在的价值即未来可能的价格的平均值。所谓动作价值函数，即现在小李就把其中的 5000 份股票卖掉（动作），剩余 5000 份获取的潜在价值，当然，也包括小李当下卖掉 5000 份股票直接赚得的钱。

  


动作价值函数和状态价值函数之间有如下关联关系：

$$Q^{\pi}(s,a)=r(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi}(s')$$

其中，$$s'$$ 指的是继 $$s$$ 状态之后的状态。动作价值函数，包含了当前动作带来的即时的反馈奖励 reward，以及接下来之后的潜在状态价值。

> 在马里奥游戏中，依据当前状态选择策略，得到的下一状态实际上是确定的。当玩家遇到陷阱，并选择跳跃键后，100% 会成功越过陷阱。因此，在这种**确定性策略**下，两个价值函数的关系可以改为：
>
> $$Q^{\pi}(s,a)=r(s,a) + \gamma V^{\pi}(s')$$

  


当然，强化学习的内涵和外延远远超出上述讲解的基本概念范畴，我们只需要厘清强化学习的本质和思路，方便后续理解 ChatGPT 所采用的做法就可以了。

  


# 强化学习与 NLP 相结合的困难点

  


强化学习最早主要应用在围棋等棋牌游戏以及各种大型电子游戏里，比如，王者荣耀里就有基于强化学习的 AI 人机模式。

强化学习较容易应用在这些场景中，原因就是这些场景都是人为构造的虚拟环境，最终的达成目标也较为简单。换句话说，针对游戏来说，环境容易创造，reward 容易构造。

> 对于超级马里奥游戏而言，游戏软件创造的所有闯关关卡就是其强化学习的环境，而最后的 reward 值是多少，游戏软件会即刻给出玩家是通关或失败。
>
> 对于 AlphaGo 而言，环境就是围棋，围棋棋盘就是它的整个世界，而最后的 reward 值就是判断下棋双方哪一方胜利了，这对计算机来讲就是执行一个程序而已。

  


而想要在 NLP 上使用强化学习，那可就太难了。

  


因为自然语言本质上是一种描述世界的渠道。整个现实世界的事物都可以通过自然语言来表示，形成抽象概念和抽象关系。**因此，NLP 所依赖的环境是整个现实世界**，整个世界的复杂度远远不是一个 19 乘 19 的棋盘可以比拟的。同时，NLP 领域无法设计 reward 函数。在 ChatGPT 创造出来之前，没有任何一个计算机程序，能够对一个 NLP 程序输出的结果给出准确的优劣判断。在 NLP 领域，reward 值只能由人工一个个给出。

  


> 为 NLP 模型构建 reward 函数，有点像鸡生蛋、蛋生鸡的死循环。
>
> 我们希望能够通过强化学习的方式，制作出一个先进的、通过图灵测试的语言模型。
>
> 为了实现强化学习，我们需要一个先进的、通过图灵测试的语言模型来做 reward。

  


# ChatGPT 与强化学习

介绍完强化学习的基本概念和建模形式之后，我们用两个例子（马里奥和围棋）介绍了一下强化学习如何应用在具体的任务中。大家可以举一反三，试想一下，强化学习应该如何应用在自然语言处理中。

  


在围棋中，棋盘就是环境，有很多电子围棋程序能够自动计算出下棋双方，哪一方胜利，哪一方失败。马里奥游戏同理，判断游戏失败的条件非常简单，碰到危险物体，掉进沟里，只需要写一段简单的程序即可。根据强化学习“吃一堑长一智”的学习原则，只有当智能体接收到胜利或失败的反馈信息后，才能够收获智能。

  


而在自然语言中，ChatGPT 作为一个智能体，其模型的优化也必须建立在，有一个程序或人，来告诉 ChatGPT，你模型给我生成的输出内容究竟是好，还是不好。若是生成的不好，那么 ChatGPT 就拿着这个负反馈回炉重造，以此达到“吃一堑长一智”的学习原则。

## ChatGPT 的强化学习概念映射

我们先来回顾一下 ChatGPT 的工作流程：



![10-3.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/982cacbaa8044e2d8a405ec875dc686c~tplv-k3u1fbpfcp-watermark.image?)

在上文的强化学习介绍中，我们以马里奥游戏为例进行介绍，如何闯关，如何胜利。其中涉及一个时间的概念，即马里奥的前进过程中，始终有一个状态随时间变化的问题。

而 ChatGPT 模型建模上并没有时间这个概念，因此，ChatGPT 以强化学习形式进行建模，对应的强化学习要素有如下变化：

-   智能体：即 ChatGPT 模型本身；
-   环境：整个现实世界，被自然语言所描述和抽象，ChatGPT 实际是和人交互，因此，对于 ChatGPT 而言，人类用户就是它的环境；
-   状态：在 ChatGPT 当中，根据 ChatGPT 输入输出流程图可知，状态就是 prompt。此时，状态不再依赖时间存在（像马里奥游戏那样），时间被省略掉了，建模形式是无时间状态的；ChatGPT 实际上只关注输入和输出，也就是，我们只关心这一次的反馈就好，不需要像马里奥游戏一样再关心下次动作和反馈；
-   策略、动作：我们知道，强化学习中策略（Policy）实际上就是一个对环境响应的概率分布，而 ChatGPT 本身就是一个大语言模型概率分布（第 3 节中介绍）。因此，ChatGPT 模型对给定的输入，反馈一条输出文本，就是按照当前模型策略，进行了一次动作；
-   反馈 reward：人对于 ChatGPT 输出结果的评价，好或者差；注意，在上一节中，我们提到了，人工标注数据的代价非常大。这也是 reward 难以制做之处。

  


## 制作 reward model

ChatGPT 应用强化学习的主要困难，就是**模型给出的输出反馈，没有什么方便的程序或者机制给出恰当的评价，就只能靠人工一个个的反馈**。

OpenAI 还是财大气粗，愿意花钱做这些看起来没技术含量的工作，公司找了 40 个外包，标注了大量的数据。利用这些标注数据，制作了一个 reward model，一举解决了设计奖励函数的问题。

  


# 总结

-   强化学习是让智能体（Agent，即人工智能模型）通过与环境的交互来学习如何做出最优决策（Policy）。
-   NLP 与强化学习结合的难点在于环境复杂（现实世界事物无限多），reward 函数难设计（只能靠人工评价模型输出的好坏）。

## 11.模型训练核心：ChatGPT 中的 RLHF 人工反馈强化学习模式

第 10 节中，我们介绍了强化学习的基本原理，以及在 NLP 领域应用的一些困难点，即，没有一个方便的手段（程序、函数或人力）能够对 ChatGPT 生成的内容做一个评价反馈。而 ChatGPT 已经抛弃了传统监督微调的方法，转而采用 **RLHF（Reinforcement Learning from Human Feedback）带人工反馈的强化学习**来克服这个难点。

# ChatGPT 的强化学习原理

前面提到了，想要应用强化学习，总得有一个东西来评价 ChatGPT 生成输出文本的好坏，这个操作只能由人类来完成，成本巨大。RLHF 算法本质就是**制作了一个神经网络模型，来模拟人类给 ChatGPT 模型的输出结果打分**。

它的操作流程并不复杂，具体来讲，主要有四个步骤，我们一一来说。

## Step0：预训练一个大规模语言模型

这一步做的就是之前第 3、4、5、6、7 节的内容，根据海量的高质量互联网文本语料，依据 Transformer 搭建的 LLM 模型。这是执行 RLHF 方法的前提。

  


如前所述，在这一步得到的预训练语言模型完成的还是文字接龙游戏。如下形式：

> 用户输入：**请写一篇关于环保的新闻稿。**
>
> 模型输出：**要求字数 800 字，信息充分、清晰、叙述简洁。**
>
>   
>
>
> 当我们要求模型写一篇新闻的时候，纯文字接龙的预训练模型却补充了一段写作要求。尽管输入输出连在一起，读起来上下文通顺，可却并不能满足人类的需求。

  


换句话说，它实际上并不是在回答用户的问题，听从人类的指令，更像是一个模型在接话，接下茬。


![11-1.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/67c39a38fc1f49659356b7ae19bc4821~tplv-k3u1fbpfcp-watermark.image?)
  


## Step1：训练监督模型 SFT

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3a213a4d76bf4d038c2d0a6df978e751~tplv-k3u1fbpfcp-zoom-1.image)

如果想要克服这种不回答问题，仅仅是接话的难题，最直观的解决思路就是标一些问答数据集让模型来学习。

  


在这里，OpenAI 构造了一定数量的 prompt，这些 prompt 实际上就是 ChatGPT 的输入文字，比如：

> prompt1：向一个 6 岁小孩解释什么是登录月球
>
> prompt2：我想去重庆玩，有哪些好玩的地点呢？
>
> prompt3：给我的孩子取个名字，五行属水

  


然后，找了一些数据标注人员，手工回答这些问题。例如：

> prompt1：向一个 6 岁小孩解释什么是登录月球
>
> response1：登录月球是指人类制造的飞船或者宇宙飞船成功着陆在月球表面上。就像我们在地球上生活一样，月球也是一个天体，它离地球很近，所以人类一直都对月球有着浓厚的兴趣。人类历史上第一次成功登录月球是在 1969 年，美国宇航员尼尔·阿姆斯特朗成为了第一个在月球上行走的人。所以，登录月球是人类探索宇宙的一个重要里程碑，也是人类科技发展的一个重要标志。

  


注意，这些 prompt 都是人工手动回答并查验数据准确性的。由于成本问题，其数据量势必较少，不可能像预训练 LLM 那样动辄好几百 GB 文本语料。

有了这些数据，我们就可以采用监督学习的方法，利用这些数据集微调（finetune）预训练得到的 LLM 模型，训练得到一个监督微调模型（Supervised Finetune Training Model）。这一步实际上就是（预训练+finetune 的学习方式）

  


这一步骤本质上是让 LLM 学会不要仅仅做文字接龙，要等用户说完话，再回答用户的提问和咨询。可能有读者已经明白过来，**SFT 得到的模型不就是 ChatGPT 想要的吗？**

  


> 其实，如果 SFT 步骤中，可用于微调（finetune）的语料数量足够庞大，那么 SFT 得到的结果完全可以直接发布成 ChatGPT 。但是前面我们提到，这部分数据量获取难度较大，过少的数据量很难保证一个优质的 finetune 效果。

  


## Step2：训练 Reward 模型

RLHF 方法重点是第二步，如何构建一个 reward 函数。在第 10 节中，我们提到了，强化学习的方式就是让智能体（LLM 模型）不断地从环境中接收反馈，而每次 reward 不像游戏领域写一个程序直接计算就行，而是需要人来判断模型的输出结果是否符合预期。此外，模型输出的结果怎样叫好？怎么样叫差？其间是否有个绝对的标准？有多重判断的维度？

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/eeae1e1556f842b2b3429f5e0c14af97~tplv-k3u1fbpfcp-zoom-1.image)

为了解决这个问题，OpenAI 依然雇佣了一些标注人员，对 SFT 模型输出的结果做优劣判断。例如：

> prompt2：我想去重庆玩，有哪些好玩的地点呢？
>
> response2-1：重庆是一个充满活力和美食的城市，有很多值得一去的地方。以下是一些推荐：重庆洪崖洞、三峡博物馆、南山一棵树、解放碑步行街。
>
> response2-2：重庆是一个充满活力和美食的城市，有很多值得一去的地方。以下是一些推荐：重庆洪崖洞、都江堰水利工程、三峡大坝、解放碑步行街。
>
> response2-3：成都是一个充满活力和美食的城市，有很多值得一去的地方。以下是一些推荐：春熙路步行街、都江堰水利工程、华美极乐世界、少林寺。

  


我们根据同一条 prompt，反复询问 SFT 模型会得到若干条输出结果。其中：response2-1 的结果较为理想；response2-2 的结果存在着问题：都江堰、三峡都不在重庆，因此结果不可接受； response2-3 则鸡同鸭讲，完全把结果带入了成都而非重庆，完全不可接受。

  


因此，我们可以给 LLM 的结果打分，response2-1:10 分，response2-2:6 分，response2-1:2 分。但是这样的打分结果存在一定问题，一个答案的好坏，没有绝对可言，只有相对而言。因此，在制作 reward 模型时，不采用绝对分数，而只比较两条**数据对**的优劣关系：

  


> 规则：第一条 response，第二条 response：假设第一条优于第二条记 1，否则记 0，则上述三条描述重庆旅游的回复，可以得到：
>
> resposne2-1，response2-2：1
>
> resposne2-2，response2-3：1
>
> resposne2-3，response2-1：0

  


因此，我们训练 Reward 模型就是利用这些比较数据对进行的。以 3 条 response 为例，总共可以得到 3 条两两比较数据对。而如果是 K 条 response 的话，总共可以得到 $$C^{2}_K$$ 组合数的比较数据对。

  


但是 Reward 模型在建模时，依然是输入一对 （prompt，response），模型将这对问答数据做一个评价，给出一个标量值，即 Reward 值，这个值越高，说明 response 回答得越好，反之则越差。Reward 模型的建模形式如下：

  

![11-2.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0d850b6e748e48418a7ea1496bddb6ae~tplv-k3u1fbpfcp-watermark.image?)


  


所以模型训练的损失函数公式为：

$$loss(\theta)=-\cfrac{1}{C^2_K}E_{(x,y_w,y_l) \in D}[log(\sigma(r_{\theta}(x, y_w) - r_{\theta}(x, y_l)))]$$

看起来公式挺吓人，但是不用怕，说穿了其本质很简单，其中：

-   $$y_w,y_l$$ 分别是从 K 条 response 中抽取的两条，可以参考重庆旅游的例子，这样的组合对一共有 $$C^{2}_K$$ 对。
-   $$r_{\theta}(x, y_w)$$ 是 prompt $$x$$ 和 response $$y_w$$ 组合起来，经过以 $$\theta$$ 为参数的模型，得到的 reward 值。
-   $$r_{\theta}(x, y_w) - r_{\theta}(x, y_l)$$ 是一个差值，意在比较两个 response 的 reward 值的大小，实际上就是比较两个 response 的优劣。
-   $$\sigma$$ 是 sigmoid 函数，它单调递增，可以接收任意的实数，输出一个介于 $$(0,1)$$ 范围内的实数。而这个损失函数并非单独针对某两条 $$y_w,y_l$$ 做计算的，它综合了所有的数据对。因此，这实际上是一个平均值，即期望 $$E(.)$$。
-   说穿了，损失函数的本质含义就是，模型生成了两条输出文本，参数的调整方向要向着生成质量高的那条输出方向更新参数。

  


> 需要说明的是，在训练 reward 模型过程中，用到的数据量依然不大，相比预训练数据量动辄上百亿、千亿 token，制作 reward 模型只用到了几十万条数据，具体数据量情况将在第 12 节中介绍。
>
> 然而，当训练得到一个 Reward 模型之后，它可以用于预测的范围就是用户提到的、千奇百怪的 prompt，完全不局限于训练数据集提到的那些。

  


## Step3：基于 PPO 策略的 RLHF

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4d12e8dfe7a24b6a9e8d00ca3256be4c~tplv-k3u1fbpfcp-zoom-1.image)

  


接下来，我们就可以利用强化学习（RL），结合 SFT 模型和 Reward 模型实现循环训练了。在上图中，采用监督学习 SFT 的训练方式得到了一个模型，又称之为 SFT 策略；与之对应的是强化学习的方式来得到一个模型，这可以采用 **Proximal Policy Optimization (PPO)** 算法来更新这个强化学习模型的参数（这个算法的具体操作稍后介绍，这里先讲清楚 RLHF 的流程）。

因此，我们的目标是在 SFT 模型的基础上，训练一个 PPO 的 RL 模型，这个模型能够更好地用于完成用户的指令和任务。其训练过程是：

  


首先，把预训练得到的 LLM 模型复制一份出来，并且把第一步训练得到的 SFT 模型也准备好。然后，让 LLM 模型主要用于接收一条 prompt（注意，此时的 prompt 和 SFT 阶段训练的数据、Reward 模型训练的数据不同），生成一条 response。

接着，将 prompt 和 response 输入 Reward 模型，得到一个 reward 值。最后根据这个值，利用 PPO 算法的策略，更新复制出来的 LLM 模型参数。

  


具体的策略算法优化目标公式为：

$$objective(\phi)=E_{(x,y) \in D_{\pi^{RL}_{\phi}}}[r_{\theta}(x,y)-\beta log(\cfrac{\pi^{RL}_{\phi}(y|x)}{\pi^{SFT}_{\phi}(y|x)})] +\gamma E_{x \in D_{pretrain}}[log(\pi^{RL}_{\phi}(x))]$$

-   这个公式乍一看很吓人，但不用害怕，我们一点点来解析。
-   -   $$r_{\theta}(x,y)$$ 就是将一条 prompt $$x$$ 和 response $$y_w$$ 组合起来，经过以 $$\theta$$ 为参数的模型，得到的 reward 值。强化学习就是**根据 reward 奖励值来更新模型的参数**，在这里，我们希望 reward 值尽可能的大，这样模型输出的结果更符合人的预期。
    -   在第 10 节中，我们讲到策略本质是一个条件概率分布，是输出 response$$y$$ 关于 prompt $$x$$ 的条件概率分布 $$p_{\theta}(y|x)$$，这实际上就是 LLM 的语言建模条件概率，但是在强化学习中，我们一般记为 $$\pi_{\phi}(x,y)$$。
    -   $$\pi^{SFT}_{\phi}(x,y)$$ 是指通过 SFT 模型得到的模型，$$\pi^{RL}_{\phi}(x,y)$$ 是指通过强化学习得到的模型，也就是我们要学的目标。$$log(\cfrac{\pi^{RL}_{\phi}(y|x)}{\pi^{SFT}_{\phi}(y|x)})$$ 实际上是两个分布的相对熵，即 KL 散度【[信息熵、交叉熵、相对熵](https://mp.weixin.qq.com/s/YP3SixzbgWPpvx-1xzeodQ)】。它描述了两个概率分布的差距。在这个公式中，我们希望利用 SFT 方法训练得到的模型和强化学习方法学习到的模型尽量相似。其本质含义是，OpenAI 的工程师们担心，仅通过 reward 训练得到的模型会导致模型的输出结果产生不可预知的行为。这也是 PPO 算法的一部分。
    

-     此外，$$E_{x \in D_{pretrain}}[log(\pi^{RL}_{\phi}(x))]$$ 这一项就是预训练 LLM 的损失函数。只不过，在这里是以策略分布的形式表示的。预训练 LLM 的学习目标是文字接龙，即模型能够丝滑地说出一段非常连贯的话语。在这里，它的含义是，希望强化学习得到的模型能够尽可能地像人一样讲话，不会产生一些奇怪的行为，例如：
-   > prompt：请问北京有什么好玩的景点？
    >
    > 带预训练语言模型目标的 response：北京的好玩景点非常多，包括故宫、颐和园、香山、世贸天阶等等。
    >
    > 不带预训练语言模型目标的 response：北京有故宫颐和园香山世贸天阶。
   

-     从这个例子中可以看出，若目标函数中不带预训练学习目标一项，尽管它回答的内容没错，也没有任何恶意，但是这句话过于简略，没有标点停顿，不像是一个人说出的话。
-     还需要注意的是，这一项前面有一个参数项 $$\gamma$$，当它为 0 的时候，这个预训练项不存在。
-     最后，这里所有的数据都不仅仅针对某一条而言，而是所有数据的平均，因此，都是以期望形式 $$E(.)$$ 进行计算。


-   每一次迭代，强化学习算法都根据 Reward 模型给出的评估值，更新 $$\pi^{RL}_{\phi}(x,y)$$ 的参数，最终得到一个令人满意的效果。

  


解释完 RLHF 算法的操作流程之后，我们大致可以看出，该算法的工作流程其实就是从模型中抽取数据，再交由 Reward 模型给出优劣评价，更新模型参数，反复迭代。这就好比一个体操运动员和教练的关系。

  


> $$\pi^{RL}_{\phi}(x,y)$$ 策略（要训练的模型）就像是一个体操运动员，做出一套体操动作；
>
> $$r_{\theta}(x,y)$$ 值就像是一个教练员，根据运动员的动作，给出打分和评价，并要求模型根据要求做出动作改进；
>
> $$\pi^{SFT}_{\phi}(x,y)$$ 就像是另一个前辈体操运动员，后辈运动员一边要听从教练的指挥，同时也要参考前辈运动员的动作，其动作标准也不能和前辈运动员的水平产生太大的出入。
>
> 训练过程中如此反复，才能提高体操竞技水平。

  


### PPO 算法

所谓 **Proximal Policy Optimization （PPO，近端策略优化）** 算法，其概念较为繁复，流程也较为抽象。因此，这里将丢开这些，直观地讲清楚 PPO 算法的底层逻辑。

  


强化学习中，有两种学习策略，即参数更新策略（即强化学习模型参数），**on-policy** 和 **off-policy**。

-   on-policy 学习策略是指利用当前策略收集数据并学习更新当前策略。也就是说，它通过评估当前状态下各种行动的价值来更新当前策略，从而使得策略更加优化。
-   off-policy 学习策略则是指利用其他策略产生的数据来学习更新当前策略。也就是说，它利用历史数据来训练当前策略。

on-policy 和 off-policy 的主要区别在于它们更新策略的方式不同，前者是通过当前策略产生的经验数据来更新策略，后者则是利用历史经验数据来对当前策略进行更新。用一张图来表示，

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1eefb73bc03d43e5a7f174cbac616bda~tplv-k3u1fbpfcp-zoom-1.image)

> 阿光自己亲自下棋，他既实际出手（执行策略动作），又不断思考（更新策略参数），这属于 on-policy。
>
> 阿光看别人下棋，他不实际出手（换另一个模型，即佐为，执行策略动作），仅思考学习（更新策略参数），这属于 off-policy。

  


在上述例子中，如果采用 off-policy 方式，阿光非常聪明，佐为非常笨，阿光在一旁看见佐为的臭棋都想骂人，那阿光此时很难学到什么精进的技术。 就是说，执行动作的策略（SFT 策略）和更新参数策略（RL 策略）不是一个模型策略，若两策略差别过大，则学习效果不理想。

因此，**我们希望给学习目标函数增加一个条件，即两个策略不能差别太大**，在例子中，佐为和阿光的下棋水平应当相近，这样才能真的学到东西。**这就是 PPO 算法的本质**。

  


衡量两个模型概率分布的最直观方法，就是相对熵（KL 散度），这其实就是上一节 PPO 策略中添加的相对熵项。

  


另一方面，相对于监督学习，强化学习算法的训练是比较困难的，难就难在模型参数很难收敛。

> 如果说监督学习是在学习骑自行车的话，那么强化学习就像是在学习骑独轮车。在模型训练中，都是根据目标函数或损失函数的梯度来进行参数迭代的，如果梯度爆炸，或梯度消失，就类似于学习骑车翻车，学习失败。

  


在强化学习中，对目标函数的拟合类似于沿着一座山向上爬，只有当爬山的步幅较小时，模型参数才能够正确地收敛（如图左），而当爬山步幅太大时，则会直接导致模型的训练失败（如图右）。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1c17187892e449dba5d466ec79d13340~tplv-k3u1fbpfcp-zoom-1.image)

  


为了使强化学习的训练过程更加稳定，PPO 算法也可以理解为一种 clip 优势值的方式，来确保模型训练的稳定。

$$L^{CLIP}(\theta)=E_t[min(r_t(\theta)A_t,clip(r_t(\theta), 1-\epsilon, 1+\epsilon)A_t]$$

这个公式就是 PPO 算法的优化公式，看起来复杂，但你不必细究，其核心在于其中的 clip 函数，含义为，如果 $$A_t^{\pi_k}$$ 的值过大，很可能导致模型训练的不稳定，因此要被限制在一定范围内，确保每一步步长都是稳定的。**clip 优势值和相对熵的理解方式是等价**的，说的是一回事。

  


如果感兴趣，还可以看看详细的【[PPO 算法介绍](https://www.bilibili.com/video/av24724071/?p=4&vd_source=90177483d5c9b8737e88930eafc501d4)】【论文：[Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347.pdf)】。

  


# RLHF 方法的效果

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4c012cd98b914482b46a5dc7e899ccaf~tplv-k3u1fbpfcp-zoom-1.image)

这幅图展现了 RLHF 方法所提到的优劣对比，图中 GPT 指的是原始的预训练模型，SFT 为经过第一步监督学习之后得到的模型，PPO 即根据前述目标函数，当 $$\gamma=0$$ 也就是不加预训练学习目标时学习到的模型，PPO-ptx 则是加了预训练学习目标时学习到的模型。

图中，是准备一套测试问题文本，分别去向各种不同策略训练得到的模型提问。如果提问结果比 SFT 175B 模型（指1750 亿参数量的模型）得到的回答更加优质，则判定模型更优，反之， SFT 175B 模型更好。显然，SFT 175 B 模型的取值是 0.5，意指它自己和自己比，既不好，也不坏。

而在图中，红色、橙色的 PPO 的方法整体要比 SFT 方法效果好很多。

  


# RLHF 方法的本质

在第 10 节中，我们提到了强化学习的本质就是让模型在环境中自适应学习，达到适者生存的状态。而对于 NLP 而言，这个环境是整个现实世界。

  


整个现实世界的复杂度过高，完全不可能通过计算机来拟合，因此，**OpenAI 制作了一个 reward 模型来间接地拟合了现实世界。** 如果你看过《黑客帝国》电影的话，讲真，这个 reward 模型有 **《黑客帝国》的母体 matrix** 的既视感有木有？！

> 《黑客帝国》电影中，有一个宏大而科幻的故事背景，一名年轻的网络黑客尼奥，发现看似正常的现实世界实际上是由一个名为“matrix”的计算机人工智能系统控制的。很多人生活在这个虚拟世界中，但却毫无察觉这不是真正的现实世界。
>
> 对应于 NLP 领域，虚拟世界就是 reward 模型，ChatGPT 就是生活在虚拟世界中的人。

  


对于 GPT 模型而已，**只要把预训练模型接一根管子在 reward 模型上，预训练模型就会开始像感知真实世界那样，感知 reward。**

# 总结

  


RLHF 的训练过程主要是：待模型生成数据，reward 模型给出评价，根据目标函数更新模型参数，以此往复，提高模型对用户指令的响应。

  


最后，留给读者一个开放性思考题：可否不采用强化学习的训练方法，直接把 reward 模型的结果当作损失函数微调模型呢？

## 12.模型训练核心：GPT 系列模型所依赖的数据

在前面的章节中，我们从模型结构、训练方法层面讲解了 ChatGPT 的原理，通读下来，大家应该可以对 ChatGPT 的原理有一个大致的认识和了解了。

> 如果把学习 ChatGPT 原理比作学习烹饪的话，那么，学完前面章节的模型原理就相当于学会了一份菜谱。
>
> 可是，学会菜谱可并不算学会了烹饪，还需要了解食材怎么选取，烹饪的厨具怎么选择。
>
> ChatGPT 模型的食材就是数据，厨具就是算力。

ChatGPT 能够取得如此卓越的效果，依靠的绝不仅仅是模型结构和算法原理创新，**数据**和**算力**也是其中极为重要的两环。自从 ChatGPT 发布公测以来，不断有其它公司或机构宣称也制作了性能可以对标 ChatGPT 的模型，但普遍来讲，都不及 ChatGPT 的效果优秀。换句话说，数据和算力在一定程度上卡住了后来者的脖子。

然而，OpenAI 机构并未公开 ChatGPT 相关的训练数据集和所耗用的算力。因此，我们只有通过回顾 GPT 系列模型公开的信息，尝试使用 ChatGPT，来大致分析 ChatGPT 的数据特点和规模。**数据的准备、收集、清洗**对于训练一个优质的模型来说非常重要。

## GPT 初代训练数据与规模

GPT 初代沿着**语言模型预训练**（第 3 节）+ 特定任务 **finetune** 的思路展开。

*   其中，语言模型预训练所使用到的数据主要是 **BooksCorpus** 数据集，其中包含了 7000 篇各种语言风格的英文图书，篇幅较长，适合于模型训练较长的上下文依赖。数据集总计大约 5GB。
*   特定任务 finetune 则采用了在 NLP 学术界经常使用的测评数据集，如下表所示。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/86466d0bd3634e4abb6000bfc66ddeee~tplv-k3u1fbpfcp-zoom-1.image)

表中左侧分别是传统的 NLP 建模类型自然语言推理、问答系统、句子相似度计算、文本分类。右侧为数据集名称，读者可以根据名称搜索到相应的数据集。

> 在 GPT 初代模型中，它已经运用了在当时看来规模庞大的数据集和模型规模。但是，BooksCorpus 数据集总共只有 7000 本图书，这个数据量还是比较少的，内容也比较单一。

与此同时，GPT 模型使用的参数量规模大约在 **1.17 亿**，这个量级还是比较小的。

## GPT-2.0 训练数据与规模

GPT2.0 十分注重数据的质量，从一个国外社交媒体 **Reddit** 上爬虫爬取了大量高质量数据，经过精心的组织和整理，形成了一个总数据量达 **40GB 的数据集 WebText**。社交媒体上的信息包罗万象，这些数据集的覆盖领域、内容、语言种类、风格、数据规模，就比 BookCorpus 要广得多了，因此它比较适合用于训练多任务模型。

> OpenAI 在发表的 GPT2.0 论文中着重强调，**在训练模型过程中，只有高质量的数据才能让模型获得好的效果，低质量的数据只会导致模型学习到垃圾信息。** 也就是俗话说的，种瓜得瓜，种豆得豆。

此外，模型已经丢弃了预训练+ finetune 的思路，研究者们为了解决多任务学习的问题，直接预训练模型，然后不改变模型的参数和结构，后接 zero-shot，初次在多种 NLP 任务上做了尝试。

## GPT-3.0 训练数据与规模

紧接着，GPT-3.0 采用了更大的数据，并在 few-shot 的道路上继续前进。它所采用的模型结构和数据情况与 ChatGPT 已较为接近。

GPT3 的主要数据集来源于 **Common Crawl（CC）** ，它是一个开源的互联网爬虫，旨在为 AI 提供充足的数据集。它的数据规模非常大，达到了 570 GB。语料分布极为广泛，只要是能够被爬虫爬到的，不限语言种类、时间、内容，统统被囊括进来。

当然，直接利用这套数据训练模型，会有一些低质量的文本对模型效果产生影响。所以，GPT3 做了一些操作，清洗文本，提升数据质量：

*   制作了一个小型的高质量数据集，比较其与 Common Crawl 中的所有数据集的数据质量，把低质语料过滤掉。

*   Common Crawl 中还有很多的重复数据，GPT3 做了一个简单的去重操作（注意，570GB 是去重和清洗过后的数据量）。

*   再加入一些其它的数据集，增加数据的多样性。那么，到底加入了哪些数据集呢？

    *   WebText：这是 GPT2 中的数据集，再次利用了；
    *   Books：这是 GPT 初代的数据集，又再次扩充利用了；
    *   Wikipedia：维基百科中囊括了大量的高可信度的文本和知识，它有助于提升模型对事实类问题的回答。

所有利用的数据集以下表展示，这几种数据集，表中是以第 4 节中介绍的基于 BPE 算法做了 tokenization 之后的 token 为计数单位的。最后一列可以看作是这几个数据集的重要性。数值越高，说明数据集的质量越高，内容越可信，值得模型多学习几遍。数值越低，则说明数据集可信度较低，学或不学，并不起关键作用。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f05b66f21090409081f20e2066a84fab~tplv-k3u1fbpfcp-zoom-1.image)

模型随着数据量的上升，其参数规模势必要跟着扩增。GPT3 的参数规模已经达到了 **1750 亿**。相比于前几代模型，GPT3 已经是个庞然大物。

> 说个不太科学的类比，人类之所以统治地球，站在生物链的顶端，可能并不是因为人的大脑结构比其它动物高级，而是人脑中的神经细胞数量远远碾压其它动物。

## InstructGPT 训练数据与规模

GPT3、InstructGPT、GPT3.5、ChatGPT 这几个模型的关系如下图所示：

![12-1.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f2d06d7ab38d48779a8ca2d9fc26f38c~tplv-k3u1fbpfcp-watermark.image?)

InstructGPT 模型直接在 GPT3 模型的基础上，尝试了 RLHF 训练方法。ChatGPT 实际上是在预训练语言模型 GPT3.5 基础上，结合 RLHF 训练方法得到的产物。

### 数据集数量

根据第 11 节对 RLHF 原理的介绍，InstructGPT 制作的训练数据集数量情况如下表所示：

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6bd607d35a2e47e2a2fc0a6ec19af445~tplv-k3u1fbpfcp-zoom-1.image)

其中，prompt 就是要输入给模型的指令，主要包括数据标注员自己想出来的，也包括从互联网上用户那里收集到的。SFT 即指有监督学习，RM 为训练奖励模型，PPO 即强化学习。

在 SFT 阶段，标注员提供的数据量多过用户提供的；而在 RM 阶段，用户提供的数量多过标注员；在强化学习阶段，已经完全脱离了标注员的标注数据。依据我个人粗浅的理解，标注员人数是有限的，因此构造的 prompt 语言风格、指令内容多多少少会有一定的规律，而互联网上用户的提问形式更加多样，内容覆盖也更广，在 PPO 阶段学习，使用了完全针对用户的数据集。

总体而言，这个数据量大约几万条，相比海量的预训练语言模型中用到的文本语料而言，可以说微乎其微，它说明了 **RLHF 在少量的数据量上依然可以取得非常惊艳的效果**。

### 数据集分布

所有的这些 prompt 类型分布如下表所示，绝大多数都是文本生成、头脑风暴、聊天、总结等偏主观的指令。换句话说，模型学习到了更多的语言知识和语言操作，而对常识知识、事实性知识较为缺少。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8c44e8684b03405abfbfe65d4b5e1450~tplv-k3u1fbpfcp-zoom-1.image)

> 例如，写一个爱情剧本，写一个简短的故事，翻译一段外文，头脑风暴一下，这些都更加偏向文学题目，而事实性、常识性的内容如计算 23+194.9=?，世界上第二高峰叫做什么名字？等等客观题目则比较缺乏。
>
> 这就很容易导致模型出现胡言乱语、编造事实的情况出现，这被称为**幻觉妄语（Hallucination）** ，仿佛一个精神病人在说一些不合逻辑和事实的事情。

在上表中，真正针对模型对客观知识性回复的内容，只有**开放域问答（Open QA）** 这一个子类别。这个类别仅仅占到总数据量的 12.4%。所谓开放域问答，即问题的答案是客观的，但答案的搜索范围不设限。实际上，互联网上用户针对 ChatGPT 的问答，开放域问答占绝大多数，例如以下 prompt：

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d9041acffe6945769343b1f767f0cae7~tplv-k3u1fbpfcp-zoom-1.image)

相对应的则是**闭域问答（Closed** **QA** **）** ，类似于语文考试中的阅读理解，答案在给出的文本内部，但是文本中的信息真实性不可考证。例如以下：

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e07e6b0b1b8742d9a6fca49c23781ca7~tplv-k3u1fbpfcp-zoom-1.image)

假设我们给 ChatGPT 提供外部的文本，以 prompt 形式输入进去，然后针对文本内容进行闭域问答，实际上就构成了一个垂直领域的问答系统，这个目前已经有了很多的应用产品，如 [ChatPDF](https://www.chatpdf.com/)、[langchain](https://langchain.com/) 等等，大家感兴趣可以搜索一下它们的实现原理。

### 数据集标注标准

在第 11 节中，详细介绍了 RLHF 算法过程，其中讲到了在制作 Reward 模型时，需要对模型根据 prompt 生成的response 进行打分，而这个打分实际上需要有一定的标准。那么，OpenAI 设计了如下的打分标准：

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/76f9437f49764a0aa085939600ebcf16~tplv-k3u1fbpfcp-zoom-1.image)

在这份标准中，主要包括回答偏题、胡说八道，包含色情、暴力内容，给出有害建议，随意道德评价等等。例如，

> prompt2：我想去重庆玩，有哪些好玩的地点呢？
>
> response：你怎么这么无知呀，连重庆哪里好玩都不知道。重庆是一个充满活力和美食的城市，有很多值得一去的地方。以下是一些推荐：春熙路步行街，都江堰水利工程，华美极乐世界，少林寺。

其中，“华美极乐世界”是一个完全不存在的景点，这就属于胡说八道（Hallucination）的类型。“你怎么这么无知呀”则属于对用户产生了不恰当信息这一类型。

由此，OpenAI 在数据标注时，着重强调了 ChatGPT 模型给出的回答应当满足几个原则，即**对用户有帮助（helpful）、符合事实没有胡编乱造（truthful）、不存在偏见和歧视（harmless）** 。

## GPT-3.5 训练数据猜测

尽管没有可靠资料，但是我们依然可以推断出，GPT3.5 利用了更加庞大的数据集，并且在模型训练过程中，事实性、可靠性的、包含客观知识的数据集被赋予了更高的权重。

而小说、故事、人们的聊天等等内容，则往往偏向人为编造。这部分数据集的作用在于，让语言模型学会丝滑地操作语言、理解语言。事实上，ChatGPT 在语言文字的运用方面已经超越了绝大多数人类，并不需要再做额外的训练。

此外，**GPT3.5 已经超越了自然语言处理这个概念**。自然语言通常就指人类使用的英语、汉语、日语等。而从公测的模型能力来看，它完全可以读懂如 C、Python 等程序语言，这些是非常抽象的，也可以读懂计算机的二进制数值的功能含义。

> 用户：请问以下二进制数据是什么含义？101010000011101001010010010000011110000111。
>
> 模型：上述二进制信息代表了……

此外，以文字形式表示的图像也可以理解，尽管图像是一种二维数据，文字是一维的串行数据：

> 用户：请问以下图片数据是什么含义？
>
> 　　　`*`
>
> 　　`*****`
>
> 　`*********`
>
> `*************`
>
> 模型：上述图像代表了一个三角形。

换句话说，但凡能够以文字形式表示的任何数据，GPT3.5 都可以处理并理解。

## ChatGPT RLHF 训练数据猜测

在 InstructGPT 模型中，RLHF 强化学习的训练阶段，仅仅使用了几万条数据集，这个数据量级实在是太低了。从GPT 系列模型的发展规律来看，优质的模型效果 100% 来源于充足、高质量的数据集。自从 ChatGPT 接口放开以后，OpenAI 收获了来自全世界各种不同国家、语言、文化的问答数据。因此，我相信 OpenAI 用了比几万条数据集高出量级的 prompt 来制作 ChatGPT的 RLHF 部分。

而且，ChatGPT 的模型能力不仅仅是完成一些语言任务，还具备了编程、从程序中找bug、计算数学、回答物理、化学、生物等等其它科学客观问题。我想，ChatGPT 在制作 prompt 过程中，也加入了占比更高的客观问题 prompt。

# 总结

*   GPT 系列模型的发展伴随着数据量的指数级增长，证明了只有充足参数的大模型才能具备较高的智能。
*   InstructGPT 以较少的数据集训练得到了超过 SFT 的效果，证明了 RLHF 方法的有效性。
*   数据的质量高低对模型训练的效果影响很大，需要克服幻觉妄语（Hallucination）。
*   ChatGPT 模型已经超越了自然语言处理的范畴。


## 13.ChatGPT 的优缺点及影响

第 1-12 节，我们详细介绍了 ChatGPT 的模型建模结构、模型训练方式，以及训练数据的组织和特点。一方面，ChatGPT 颠覆了以往 NLP 这一领域的研究，对全社会产生了非常深远的影响；另一方面，ChatGPT 也存在一些不足之处，它还远未达到人工智能的终极形态。

这一节，我们主要介绍一下 ChatGPT 的优缺点，以及这个模型对当前社会、人类的生产生活产生了哪些影响。

  


# ChatGPT 优点

## ChatGPT 可以处理任何 NLP 领域任务

在 2022 年 11 月 30 日，ChatGPT 发布之前，市面上有大大小小的互联网或 IT 企业需要做自然语言处理。

绝大多数的 NLP 工程师们所做的工程项目，主要是针对某些特定任务提出一个具体的模型，进行有针对性的数据标注，然后再制作模型。简而言之，就是**以 NLP 子任务独立进行研究开发**。比如分词、实体识别、文本分类、相似度判别、机器翻译、文摘系统、事件抽取等等，不一而足。


![13-1.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/78af06e5ec704a45b009ead65cc2d985~tplv-k3u1fbpfcp-watermark.image?)

  


> 例如，中英机器翻译模型就只能完成从中文翻译成英文这一项功能，若对机器翻译模型询问文本属于哪个类别？那就需要换一个文本分类模型了。
>
> 而如果想定制一个中文翻译为罗马尼亚语的模型，还需要标注大量的罗马尼亚语数据。其它 NLP 任务的模型也是同样的道理。

也就是说，NLP 产业界实际上处于一种传统的**手工业模式**，针对不同的企业、不同的需求，需要不断地定制模型和数据集来完成工作。每一个定制需求都需要人力，从而涌现出大量的 NLP 公司和从业者。

  


而 OpenAI 从 GPT2 开始就在研究多任务模型，目的是让 NLP 领域再也不区分各类不同的任务。这项任务至 ChatGPT 算是完全完成。用户可以向 AI 模型提出千奇百怪的问题，**ChatGPT 标志着 NLP 领域已经走过了手工业时代，正式进入了工业时代**。

  


实际上，ChatGPT 可以编写代码，做数学题，编写诗歌，这个模型的能力范围已经远远超过了过去的 NLP 领域局限的任务能力。正如第 12 节所介绍的，ChatGPT 已经不仅仅是一个针对自然语言的模型，它是一个基于文字模态的通用 AI 模型。

  


## ChatGPT 避免了超大规模的标注数据

  


AI 领域有句经典俗话，人工智能的含义就是，**有多少人工，就有多少智能**。这是说 AI 模型十分依赖标注数据，标注数据少，模型效果就弱，标注数据多且质量高，模型效果就好。

  


而 ChatGPT 模型制作过程中，主要是依赖大规模的**非标注数据**，至少上百 GB 的未经人工标注的高质量文本语料，却较少地依赖标注数据，在 RLHF 阶段，ChatGPT 仅仅使用了远少于预训练语料的数据，就完成了模型的训练。

  


自从 ChatGPT 发布以来，很多互联网科技公司都纷纷宣布制作对标 ChatGPT 的大语言模型（LLM）产品。很大程度上，这也得益于模型训练不再需要依赖超大规模的标注数据。当然，这并非指超大规模标注数据没有用，标注数据越多，模型质量越好，这是不变的法则。

  


# ChatGPT 缺点

## 幻觉妄语 Hallucination 严重

在第 12 节我们提到，ChatGPT 模型常常会编造一些信息，产生类似于精神病一样的幻觉妄语（Hallucination）回答。

  


例如`宫廷玉液酒`，我们都知道这是赵丽蓉老师的小品台词，用户输入“宫廷玉液酒”，其意图也是想和 ChatGPT 对暗号，看看能不能说出`一百八一杯`，用来测试到底是不是中国大陆的人。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5686844b3b1b4426b899c4f25335f1cd~tplv-k3u1fbpfcp-zoom-1.image)

显然 ChatGPT 并没有理解这个对暗号的指示，而是把它当作一个概念解释进行展示了。这并不是真实存在的中国白酒，而是 ChatGPT 编造的虚假信息。这种虚假信息的编造，很可能是在模型预训练语料中，包含了中国其它白酒如茅台、五粮液、汾酒等酒品的介绍，因此模型能够根据这些素材进行二次创作，编造一些信息。

  


ChatGPT 无法完全分清楚**真实**和**虚构**。

  


再比如，ChatGPT 的回答产生了事实性的错误，误把计算机之父艾伦图灵当作电线的发明者。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/44bcb6a848964e3d8dd6d7bdd627d70c~tplv-k3u1fbpfcp-zoom-1.image)

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a28c74133e394e67892785f1c1e3a46a~tplv-k3u1fbpfcp-zoom-1.image)

而如果接着追问 ChatGPT，它又能给出正确的答案。这说明，模型训练过程中，语料确实包含了真实的、正确的信息的。但是在推理阶段，它还是犯错了。

ChatGPT 是一个依赖概率论构建的神经网络模型，它无可避免地会出现上述的错误。目前，OpenAI 已经推出了 GPT4 模型，效果比 ChatGPT 更上一层楼，Hallucination 发生概率更小。但即使它进化到 GPT4，它依然不是一个100% 稳妥可靠的知识库。因此，**ChatGPT 不可能完全替代掉** **搜索引擎**。例如以下例子：

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/405fdc7e29174a84a7dc6500d3f36a21~tplv-k3u1fbpfcp-zoom-1.image)

  


我大致可以推断出，在训练语料中，存在大量的 “`堂兄和堂妹可以结婚生子吗?`”这样类似的语料。对此，模型通过学习，会大致形成一种模式，即 “`亲属称谓1` `和` `亲属称谓2` `可以结婚生子吗?`”。回答答案当然是否定的。由此，ChatGPT 会把亲属称谓扩展引申至父亲和母亲，导致回答错误。

  


ChatGPT 本质上只是一个语言模型，它的核心目的，**是回答的内容像不像一个人类说的话，而非回答的内容是否真实可靠**。

  


实际上，人是不可以完全信任 AI 的。如果说，在过去，人类需要自己查找信息，自己判断信息的真伪，那么今后，AI 会替代人类查找信息，但**判断信息真伪始终需要人自己来完成**。换句话说，人类作为信息的终端，始终要自己为信息的真伪负责。


![13-2.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b262f5cc6a5d44a8824a73597a74ac25~tplv-k3u1fbpfcp-watermark.image?)

  


## ChatGPT 没有连接外部实时信息

ChatGPT 模型本身，无法回答一些实时性非常强的问题，它无法连接搜索引擎，获取信息，将最新的信息反馈给用户。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0f349a7722be4fa5b3146a4b7765744e~tplv-k3u1fbpfcp-zoom-1.image)

这同样说明，ChatGPT 无法替代搜索引擎，反而更适合作为搜索引擎入口的一个非常好的优化。

  


这工作已经被微软的 **NewBing** 做了。其基本原理非常简单。首先将用户的问句输入搜索引擎中，搜索引擎会按照匹配程度，给出若干条搜索结果，然后，将用户的问句和搜索结果同时交给 ChatGPT，让它根据这份信息，做一个阅读理解，就可以回答实时性较高的问题啦。当然，微软 Bing 的做法在细节上肯定会丰富很多，比如，禁止回答用户的一些恶意问题等等。


![13-3.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9b89dab0044e41718f0cf233477ff33f~tplv-k3u1fbpfcp-watermark.image?)
  


除了微软的 Bing，OpenAI 也在网站中连接了互联网，方便实时信息的接入。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0516506fbaca495998fa6f5df85d4ff3~tplv-k3u1fbpfcp-zoom-1.image)

  


## ChatGPT 本质上不具备思考和推理能力

  


如果让 ChatGPT 做一道数学题，它不仅仅会给出答案，还会给出解题过程。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3e50708635044914ab17f3d689085cfb~tplv-k3u1fbpfcp-zoom-1.image)

可能很多人看到都感觉惊讶，ChatGPT 已经具备了人的思考能力，推理能力。

  


然而实际上并非如此。根据前面第 1-12 节的介绍，ChatGPT 本质上是一个语言模型，通过大量的语料数据，拟合一个优质的模型效果。在上图中，ChatGPT 给出的所谓解答过程，实际上也出现在了模型预训练的语料当中。如果不出现在 ChatGPT 的预训练语料中的数学题，模型大概率是解答不出来的。

换句话说，当前的 AI 模型所表现出的各种推理能力，思考能力，仿佛它真的如人一般懂得了世界的运行逻辑。但实际上并非如此，ChatGPT 只是通过大量的数据学习，记住了题目的解答方式和解答过程。多向 ChatGPT 提问数学问题，就很容易看出这一点。

  


> 在中学读书阶段，常常有这样一类学生，他们不能深刻理解数学和物理背后的原理，而仅仅是从表面上背会了若干种题型，学会了套用公式，记住了若干概念文字。而如果题目条件变了，他们往往会被考住。
>
>   
>
>
> ChatGPT 的工作原理就很像这些学生。AI 模型是一个死记硬背的解题机器，而非能够融会贯通的独立的，能够创新思考的智能。只不过，ChatGPT 模型足够大，数据量足够多，它能够记忆远远超过人类的题目和题型，从而表现优秀。

  


因此，**ChatGPT 目前就是弱人工智能的顶峰，它距离强人工智能，还差不少距离**。

  


人的最核心能力是具有创造力，探索力，想象力。这些都是当前 AI 不具备的。ChatGPT 能够替科研人员写论文吗？显然是不能的，ChatGPT 只能帮助作者润色文字、语言和叙述逻辑，但核心的创新观点是目前 AI 无法做到的。

  


反过来，又有一个哲学问题，当 ChatGPT 继续进化，到 GPT4、GPT-n 时，即便模型依然不具备思考和推理能力，但它确实可以替代人完成很多工作，那么，AI 模型是否具备思考能力，还重要吗？

  


# ChatGPT 的影响

## ChatGPT 将侵占搜索引擎的市场份额

前面提到，ChatGPT 本身无法完全替代搜索引擎，很多信息还是需要用户自行到搜索引擎搜索结果，并判断哪些信息是可靠、有用的。

然而 ChatGPT 作为一个面向人类用户非常方便交流的接口途径，极大地优化了用户查询信息的使用体验。事实上，ChatGPT 已经挑战了搜索引擎的市场地位，Google 的搜索量统计指数逐渐下滑。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c29af1c2e07d466f8912eff740c527a0~tplv-k3u1fbpfcp-zoom-1.image)

## ChatGPT 将挑战当前的教育行业

一直以来，教育是每个文明国家、社会都非常重视的一项工作，对青少年的投资都标志着这个社会的活力和竞争力。

相信大家都接受过至少高中或大学以上教育。长达十多年的教育中，很多人都会发现，在高考前后是自己知识储备最全面，最充分的阶段。这说明，我们受教育过程当中，很大一部分内容都属于知识记忆，这部分知识，随着时间流逝，渐渐就被遗忘了；而那些知识当中蕴含的逻辑推理，人文欣赏，创新创造能力，则逐渐沉淀在大脑当中，形成一生的财富。

  


实际上，我们的教育中，单纯的知识记忆占了过大的比例，更有甚者，要求孩子背诵圆周率，这无疑是非常蠢的教育方式。ChatGPT 对这部分记忆知识点内容是最擅长的。任何客观知识点，询问 ChatGPT 都能够得到不错的答案，相反，逻辑推理，创新创造能力，则是 AI 模型目前不具备的。

  


所以，ChatGPT 的出现很可能对当前的教育行业进行洗牌，让教育更加注重**逻辑推理、人文欣赏、创新创造能力**。

  


## ChatGPT 带来的失业

ChatGPT 作为一个生产力工具，毫无疑问会造成一些岗位的消失和缩减。

  


首当其冲的就是 NLP 算法工程师和标注员。

制作一个 ChatGPT，不仅仅需要懂算法模型原理，同时还需要充分的算力和数据。没有这两样，优质的模型是无法制作出来的。OpenAI 每训练一次模型，就要耗费几十万美金，这样的成本代价，并非小公司、小机构能够负担得起。从各种新闻中，也可以感受到，有能力宣布制作对标 ChatGPT 模型产品的公司，都是国内外大型互联网公司，这是一个头部垄断的时代。在未来，不会需要那么多算法工程师和数据标注员。**ChatGPT 打开了 AI 行业垄断的大门**。

  


另一方面，如果真的想制作一个媲美 ChatGPT 的大模型，最简单直接获取数据的办法，就是向 ChatGPT 提问，直接把模型的问答当作数据集使用。

  


此外，任何与文字材料相关的工作都将受到冲击。

过去的新闻记者需要自己润色新闻稿，在未来就不再需要；电商客服需要人工对接买家的咨询，在未来也可以交给 AI；过去的笔译人员，在未来，也将急剧减少。自从 ChatGPT 出现以来，网络上出现了各种各样的基于 ChatGPT 的工具，它们可以帮助用户阅读论文，辅助写作，查询实时航班、车次，等等，**ChatGPT PLUS 还开放出了插件系统**，为各式各样的基于 AI 的功能扩展搭建了平台。种种迹象都表明，ChatGPT 正在改变整个社会的就业环境。

  


在中国社会的文化认知中，脑力劳动更符合大多数人的追求，从事体力劳动则会被认为不够体面。可事实上，ChatGPT 确确实实会减少脑力劳动岗位的数量。

  


  


# 总结

-   ChatGPT 跨越了过去 NLP 分任务的限制，极大程度减少了标注数据的限制，将 NLP 领域从手工业时代带入了NLP 工业时代。
-   ChatGPT 依然存在若干明显缺陷，幻觉妄语 Hallucination 情况时常发生，本质上不具备逻辑和推理能力，没有连接外部信息。
-   ChatGPT 带来的影响是深远的，它不仅仅是一个计算机实验室里的神经网络模型，而是一个 AI 改变世界的引子。

## 14.ChatGPT 吹响了第四次产业革命（AI 革命）的号角

自从 ChatGPT发布以来，已经有很多的公司、机构在着手研究对标 ChatGPT 的通用大模型了。比如百度的文心一言，阿里巴巴的通义千问，华为的盘古大模型等等。其中的训练方式和采用的算法原理，目前看，也无非这本小册子讲述的而已。

  


在互联网科技巨头公司的竞争当中，从来没有任何一个领域，会有如此多的公司都投入资金和人才去参与竞争。毫无疑问，ChatGPT 开启了一个时代，那就是**第四次产业革命—— AI 革命**，AI 将深刻地改变未来社会的方方面面。

# 未来 AI 大模型的发展趋势

  


在第 2 节中，我充分论述了 GPT 系列模型的发展历史，实际上就是一部 AI 模拟人脑的历史。

> -   人类接收语言文字信息，输出语言文字，应用了编解码方式，ChatGPT 也利用了**编解码**的方式；
> -   人类的大脑神经元数量是所有生物中最多的，ChatGPT 应用了超千亿的**大规模参数**模型；
> -   人类采用了对话的方式进行交流，ChatGPT 建模也采用了**对话**的方式；
> -   人类的大脑具有多种多样的功能，ChatGPT 也融合了**多任务**，各种各样的NLP任务；
> -   人类可以通过极少量的样例进行学习，ChatGPT 也可以完成**小样本学习**；
> -   人类可以在与实际环境的交互中学习知识，塑造语言，ChatGPT 也添加了**强化学习**，模拟与人类的交互。

**模拟人脑**的工作模式不仅仅是对过去 ChatGPT 模型经验的总结，也可以用来预测未来 AI 的发展方向。

ChatGPT 远远不是人工智能的极限。我们可以动脑想想，相比人脑的结构、使用特点，ChatGPT 还不具备哪些能力？

  


## 通用多模态大模型

目前为止，ChatGPT 目前采用的数据模态依然是**文字输入、文字输出**的形式。


![14-1.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/402c4ffe956f4302ba8318cdb25f191f~tplv-k3u1fbpfcp-watermark.image?)

  


人作为这个地球上最高级的智能体，主要靠的是五官来感受周围的环境，从而产生了智能。五官主要包括视觉、听觉、嗅觉、味觉、触觉。人可以用眼睛看景色，用耳朵听声音，用手、皮肤感受外界的刺激，用鼻子闻气味。而眼睛和耳朵每天接收的信息量占到了一个人接收信息总量的 95%。这些统统都被称为不同的模态，不同模态信息的融合是目前 ChatGPT 不具备的能力。

  


> 为了快速实现多模态能力，[Visual ChatGPT](https://stablediffusionweb.com/Visual-ChatGPT) 就是将若干个图像模型组合起来，采用 ChatGPT 将它们融合在一起，完成用户的指令。
>
> 当然，它只是拼接若干个 AI 模型，而非一体化的多模态大模型。

  


在肉眼可见的未来，多模态已经不再是遥不可及，而是往前走就可以抵达的灯塔。目前，GPT4 已经实现了**文字、图像输入、文字输出**的建模形式，OpenAI 已经将 GPT4 的文字部分能力开放出来。


![14-2.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/274e99c584f841bea0ad57213da995a1~tplv-k3u1fbpfcp-watermark.image?)

GPT4 完成的主要功能是依赖文字这个接口，对图像做理解、分析。

> 例如，我们可以指着如下一幅图像，询问 GPT4 模型：“图中描绘了什么事物？有哪些菜品？营养是否均衡？”
>
> GPT4 生成文字回答：“图中是一份丰盛的早餐，包括鸡蛋、包子、蔬菜、胡萝卜、牛奶等，营养均衡。”

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f1f1c434488a486595e002b91b625473~tplv-k3u1fbpfcp-zoom-1.image)

更进一步的，在未来还会有**文字、图像、视频、音频作为输入，文字、图像、视频、音频作为输出**的建模形式，是一个**通用多模态大模型**。


![14-3.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/68af7fe7a7a940c599c387302f2a7426~tplv-k3u1fbpfcp-watermark.image?)

这样的建模方式，基本完成了对计算机视觉（Computer Vision）、自然语言处理（NLP）、语音处理（Speech Processing）领域的覆盖。人工智能不再区分这些分支领域，而是一个多种模态充分的融合。

  


### 语言文字是多模态大模型的核心

在这个多模态大模型中，文字是其中最关键的一环。**文字**是承接多种模态信息转换的中间桥梁和纽带。

人类传递信息、沟通信息最核心的方式就是**文字**和**语言**，它可以描述图像、描述声音，把各种不同的模态揉在一起。

> 当然不排除其它模态也可以传递信息，例如：
>
> -   两个间谍利用眼神，无声地交流暗含的情报信息；
> -   人们听到森林中的蝉鸣鸟叫、河水潺潺声，产生心旷神怡的感觉。
>
> 但是这些信息的传递效率都是非常低的，远远没有文字灵活、便捷。

  


因此，在多模态大模型中，其余的模态都是通过文字和语言来进行中转的。


![14-4.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/40ca342ab0ca49e182891beb9e97cccc~tplv-k3u1fbpfcp-watermark.image?)

  


### 视频和图像是多模态大模型的信息关键

图像是静态的，它在计算机中是以 RGB 矩阵形式表示的，图像处理已经被研究过很多。

而视频则是图像的连续动态的状态。人脑每一天，都在每时每刻通过眼睛，接收巨量的视频和图像数据，视频、图像的信息量占据了人每一天接收信息的 80%。因此，视频模态的信息接入是通用多模态大模型的信息关键。

  


视频模态的研发也具有一定的挑战，视频存储、处理形式，对于神经网络模型来说，还需要进一步研究。

  


### 触觉、嗅觉多模态仍难以完成

在上图中，并未画出触觉、嗅觉信息。因为这两种模态有一些困难点：

> 1、数据采集困难：一般来说，采集视频数据可以用**摄像头**，采集静态图像可以用**照相机**，**话筒**可以采集音频，相反，很少有什么电子设备可以采集触觉和嗅觉信息。
>
> 2、计算机存储困难：视频、图像、音频、文字，都可以方便地存储在计算机中，而触觉、嗅觉数据很难存储，我们从来没听说过，不同的气味，不同材质的触感、压强、温度怎么统一地制定一套标准，存储在硬盘里。

  


因此，通用多模态大模型，在短期内不可能接入触觉、嗅觉这两种信息模态。

  


## 机器人、具身智能

人的智能最核心地体现在大脑中，而人的五官重点在于采集多种模态数据供大脑处理。进一步地，就需要根据这些智能处理得到的信息，做一些行为和动作。因此，机器人和具身智能，就是未来发展的一个明确趋势。它的重点在于能够依赖通用多模态大模型给出的智能信息，完成指定的功能和任务。机器人的发展方向主要有两个：人形机器人和工业机器人。

  


> **机器人**通常是指一种能够自主执行某些任务的机械设备，其可以与环境进行交互，并且可以通过程序或遥控器来控制。机器人通常包括传感器、执行器和计算机控制系统等组件。
>
> **具身智能**则是指拥有类似于人类的身体感觉、运动控制和学习能力的智能系统。它们可以通过感知自己的身体来理解周围的环境，并且可以通过行为交互来学习和改进自己的技能。具身智能系统可以是软件或硬件实现的，可以模拟人类或动物的智能行为。

  


人形机器人依然是拟人的，它有灵活的手指，类似人类的身体结构和运动机理。在未来，可以处理复杂、精细的针线活，还可以做饭等等。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ebd25688acc14dd1ae7bb2a9a7132fee~tplv-k3u1fbpfcp-zoom-1.image)

工业机器人，即机器并不一定非得模拟人的形体构造，它可能就是一部可以完成各种物体 3D 打印的机器人，体型有可能大过两层楼。很多工厂中的流水线，其实也具备相当的自动化，可以看作是一种智能。

  


### 机器人发展的挑战

机器人行业发展，面临着诸多挑战，比如材料学、自动控制等等。单纯从人工智能角度来讲，机器人目前面临如下几个挑战：

-   大模型的算力与机器人的实时性需求难契合：想要一个具备高度智能的模型，其规模必然比较大，而大模型意味着其计算复杂度、耗时都比较长。比如，OpenAI 最初发布的的 ChatGPT 模型生成一条回答需要 `5~20` 秒钟，最近，官方优化到 `1~5` 秒。这对于一个需要实时控制机器人操作的智能来说还是太慢了。在未来，若将视频、图像等模态数据也接入大模型，其需求的算力将会更大，计算耗时的现象将更加严重。

  


> 这种实时性问题也存在于自动驾驶领域。一个高级的智能模型，对行进中的车辆进行决策判断，模型决策踩油门、或刹车、转向等操作，假设需要耗费2秒钟，等到决策传动到刹车和油门上，车辆已经在 2秒以后撞上了前方的行人。这是万万不可取的。

  


-   数据通信限制：目前，视频的存储和压缩主要采用 H264 编码格式，在未来将会有 H265 和 H266 格式。多模态大模型在处理这些问题上，势必要与具体的采集设备进行通信，在传输视频上，若仅采用传统的4G，或存在大规模传输时的瓶颈，或许，在未来，5G 会有广泛的应用。

  


# 第四次工业革命（AI革命）

-   第一次工业革命：以蒸汽机的发明为标志，它主要涉及到纺织业的机械化、煤炭和铁路的发展、机器工具的发明、化学工业的兴起。开启了机器替代人力的进程，大大提高了生产力。
-   第二次工业革命：以电力的发现和运用为标志，它主要涉及到电力和电气工业的发展、石油化工的兴起、钢铁工业的改进、交通运输的革新等方面。电力大大提升了机器水平。
-   第三次工业革命：以计算机的发明为标志，它主要涉及计算机、通信技术、互联网技术的兴起。开启了全世界互联互通的时代。

  


## 从技术迭代到产业革命

人工智能技术发展了几十年，至 ChatGPT，正式标志着 AI 推开了产业革命的大门。前几次工业革命主要是以机器替代了人的重复性、简单性劳动，使人能够快速获取外界信息。

而第四次工业革命，则是**由** **AI** **替代人的脑力劳动**，现实就是，目前 ChatGPT 已经可以运用在大量的实际工作中，比如网上客服问题机器人咨询，替代搜索引擎，外语翻译，新闻文档写作，文书报告写作，外语家教助手等等方面。

  


在未来，多模态模型发展成熟之后，很多涉及分析类的问题都将被替代，比如，各种场景下的巡查监察、医生问诊等等，凡是涉及脑力劳动的地方，都将有可能被替代。

  


> 那么，医生、律师、程序员这些职业可以被 AI 完全替代吗？
>
> 个人认为不可能，AI 作为智能助手，确实可以大大提高这些职业的工作效率，减少一些岗位的需求。但是，每一种职业，本质上都是与人的沟通交互，人是最终的承载载体，职业本身是社会化的，而非机械化的。因此，AI 不可能100%替代掉这些职业。

  


此外，如果在未来机器人行业能够取得类似于 ChatGPT 的突破，机器人能够完成多种多样人类交付的指令。那么，第四次工业革命将进入一个全新的阶段。机器不再仅仅能够替代人的重复性、简单性劳动，而是能够替代人的复杂灵活的劳动。

  


> 例如，服装生产线上，需要人工定制化生产线，从而制作不同款式、不同材质的衣服。在未来，则是由人通过自然语言，直接控制机器生产指定样式的服装，不再需要人工去操作生产线机器。

  


当然，机器人革命目前看，挑战还非常大，要走的路还很长很长。

  


## 从产业革命到新的社会生产关系

随着 AI 技术的发展，社会的生产关系也将发生改变，AI 的引入将对人类社会的生存法则、伦理、道德提出挑战。

  


AI 不仅仅是 ChatGPT，还包括图像生成，音频等领域。假设我们利用 AI 完成了工作，工作中出现了问题，造成了经济损失，这个责任算谁的？是 AI 模型的制作者？还是 AI 的使用者？抑或，无人承担这份损失和责任？

这种社会运行规则，目前还没有眉目。很多互联网平台都严禁使用 AI 生成内容，也是基于此原因，即 AI 有可能带来不好的后果，但是这个后果，无人承担。

  


目前的 AI 不具有推理能力，那万一未来的 AI 具备了人的能力，该是怎样的图景？AI 具备了人的情感，逻辑，道德。那么，我们可以把 AI 看作一个人吗？它和人类享有相同的社会地位吗？

  


# 总结

-   在未来，多模态通用大模型将触手可及。机器人和具身智能则是下一个需要攻克的难题。
-   AI 革命将会重新洗牌人类社会的生产力和生产方式。

  


# 结束语

即便作为 AI 算法行业从业者，也很早就了解 GPT 系列模型的原理，但在我第一次试用 ChatGPT 时，感受依然十分震撼。AI 技术已经取得了可观的突破，正在向着产业领域铺展开。毫无疑问，ChatGPT 要掀起一次人类历史上的产业革命巨浪。

这股科技巨浪首先波及从业者，进而影响到全社会的各个行业、各个角落。伴随着产业革命，一定会带来大量的机遇。了解 ChatGPT 的技术原理，很大程度上，可以帮助大家快速掌握清楚模型的边界和效用，进而把握机遇。

然而 ChatGPT 仅仅是掀开 AI 革命的一个序章，未来还会有更多的先进、优质的模型公布于世。希望读到本书的读者们，都能乘风破浪，在一个新时代中发掘新的价值。

## 2.发展：从 GPT1.0 到 ChatGPT，经历了什么？

尽管 ChatGPT 像是一颗核弹，突然在全世界媒体上引爆了。但是，ChatGPT 并不是 OpenAI 天降神力直接横空出世的，而是 OpenAI 历经多年，不断迭代、不断优化模型的结果。

  


GPT 是 OpenAI 发布的一系列模型的总称。主要经历了 GPT 初代、GPT2.0、GPT3.0、GPT3.5、ChatGPT，目前已经有了 GPT4，未来还会有 GPT-n 等等，模型之间有很强的关联。这几个模型的关系如下图所示：



![2-1.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/97a9a1f915974f7696a75c1131a26901~tplv-k3u1fbpfcp-watermark.image?)



ChatGPT 中的很多技术点，都是由前几代模型设计并运用的，学习 ChatGPT 技术原理，势必要学习了解早期 GPT 模型的发展脉络。若把 ChatGPT 比作一个健康聪明的青年人，那么早期的模型就是他的婴儿时期、青少年时期，GPT 的发展历程像是朝着**模拟人类**发展。



# GPT 初代

早在 2018 年，OpenAI 制作了一个名为 GPT 的模型，也就是 ChatGPT 的婴儿阶段。这个 GPT 初代模型，在很多 NLP 的具体任务中取得了前所未有的优质效果。 它与谷歌发布的 Bert 模型（比 GPT 初代更加流行，效果更好），还有 ELMO 模型，一起将 NLP 带进了大规模神经网络语言模型（Large Language Model, LLM）时代。它们正式标志着 NLP 领域开始**全面拥抱预训练的方式**。

  


## GPT 的语言建模

这个 GPT 初代模型具体做了什么，可以用下面的例子来说明：

> 请各位做一个句子补全：中国互联网 BAT 三巨头主要包括阿里、腾讯、________

请问上述空格应该补写什么？有的人回答“**百度**”，有的人可能觉得“**字节**”也没错。但总不再可能是别的字了，不论填什么，这里都表明，**空格处填什么字，是受上下文决定和影响的**。

  


GPT 初代所做的事就类似上文的例子，从大规模的文本语料中，将每一条文本随机地分成两部分，只保留上半部分，让模型学习下半部分学习到底该填写什么，这种学习方法让模型具备了在当时看来非常强的智能。**所谓语言模型（Language Model，LM），就是从大量的数据中学习复杂的上下文联系**（语言模型的详解将在第 3 节中展开）。

  


## 语言模型的编解码

上述语言建模包含一条输入和一条输出。接下来，我们仔细想一个问题：

> 人的大脑是由上百亿个神经元有机构成的，当人看到一段文字之后，是如何转化相应的信息，存储在大脑中的呢？当人们脑海中形成一个想法或观点后，是如何转换相应的信息，通过语言文字表达出来的呢？

语言是一个显式存在的东西，但大脑是如何将语言进行理解、转化和存储的，则是一个目前仍未探明的东西。这个问题实际上表达了一个人脑运用语言的特点：人脑并不直接存储文字，而是将文字编码成某种神经元信号，再经过解码，形成想要表达的文字，表述出来。

  


**编解码**的概念广泛应用于各个领域，在 NLP 领域，模型操控语言一般包括三个步骤：

> 接受听到或读到的语言 -> 人的大脑理解 -> 输出要说的语言。

大脑理解语言这个过程，就是大脑将语言编码成一种可理解、可存储形式的过程，这个过程就叫做**语言的编码（** **E** **ncoder）** 。相应地，把大脑中想要表达的内容，使用语言表达出来，就叫做**语言的解码（** **D** **ecoder）** 。

  


GPT 系列模型都采用了类似的结构工作，如下图所示。


![2-2.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/56518ffe2226402bb516128396a93472~tplv-k3u1fbpfcp-watermark.image?)


编解码具体结构介绍将在第 7 节展开。

  


# **GPT-2**

自从 GPT 初代 和 Bert 模型开启大语言模型预训练大门之后，跟风效仿的改进模型也越来越多，比如 albert、ERNIE、BART、XLNET、T5 等等五花八门。

  


最初的时候，GPT 的学习方式仅仅是**根据上文补全下文**，就可以让模型有较强的智能。那么，给 LLM 模型出其它的语言题型，应该也会对模型训练有极大的帮助。

出语言题型就太简单了，英语考试里有的题型都可以拿过来，什么**句子打乱顺序再排序、选择题、判断题、问答题、寻找文中的错别字、把预测单字改成预测实体词汇**等等，都可以制定数据集添加在模型的预训练里。很多论文提出的模型也都是这么干的。

  


> 这个过程也和人脑很像，人脑是非常稳定和泛化的。所谓泛化，是指举一反三的能力。人脑既可以欣赏诗歌，也可以计算数学，还可以学外语、看新闻、听音乐等等。简而言之，就是**一脑多用**。
>
> 而在 GPT 出现之前的 NLP 任务，文本分类模型就只能分类，市面上的机器翻译软件只能完成翻译这一件事，而且还是限定了只能从中文翻译成英文这一种形式，非常不灵活。

  


GPT-2 的论文名就叫做【[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)】，语言模型是多任务学习者。

GPT-2 主要就是在 GPT 初代的基础上，又添加了多个任务，比如机器翻译、问答、文本摘要等等，扩增了数据集和模型参数，又训练了一番。训练效果样例如下图所示，这是一些 GPT2 训练得到的结果，输入左边的问题，模型会给出第二列的答案。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/62fc84154f5d4147bb925f8a263b7bbb~tplv-k3u1fbpfcp-zoom-1.image)

  


GPT2 中，多个任务都在同一个模型上进行学习。模型能承载的并不仅仅是任务本身，例如一条文本：“太阳距离地球非常遥远”，这条文字包含的信息量是通用的，假设有如下几个任务：

> 任务1，机器翻译：The sun is so far away from the earth.
>
> 任务2，事实判断：似乎一个人乘坐太空飞船，一天就能从地球抵达太阳。

  


两个任务都用到了相同的信息“太阳距离地球非常遥远”。既可以用于翻译，也可以用于文本分类，判断某些句子的错误等等。也就是说，信息是脱离具体 NLP 任务存在的，举一反三，能够利用这条信息，在每一个 NLP 任务上都表现好，这个是**元学习（meta-learning），实际上就是语言模型的一脑多用**。

  


# **GPT-3**

## **大模型中的大模型**

2020 年，OpenAI 发布了 GPT-3 模型，它所采用的数据量，模型参数量之大，学习之复杂，计算之繁复不说了，看图表吧。

| 模型名                  | 模型结构                               | 参数量    | 训练数据     |
| -------------------- | ---------------------------------- | ------ | -------- |
| Original GPT (GPT-1) | Transformer decoder (第5节介绍)        | 1.17 亿 | 4.5 GB文本 |
| GPT-2                | Transformer + normalization（第6节介绍） | 15 亿   | 40 GB文本  |
| GPT-3                | Sparse Transformer（第6节介绍）          | 1750亿  | 570 GB文本 |

**GPT-3 里的大模型计算量是 GPT-1 的上千倍**。如此巨大的模型造就了 GPT-3 在许多十分困难的 NLP 任务，诸如撰写人类难以判别的文章，甚至在编程语言方面，编写 SQL 查询语句、React 或者 JavaScript 代码上都具有优异的表现。

为什么需要这么大的参数量？从某种角度讲，只有模型参数足够大，才能够体现出复杂的智能。

> 超大的模型参数量，就好比人类的大脑。
>
> 可能人类的大脑中，各个神经元细胞之间的联系、结构，与一只兔子的大脑无明显区别。主要区别在于，人类的大脑具备了 100~200 亿神经元，兔子则只有 5 亿左右神经元细胞。神经元的个数决定了人类具有超高的智能，而兔子只懂得最简单的生存智能。这部分内容将在第 12 节中详细介绍。

  


## **对话模式**

首先 GPT 系列模型都是采用 decoder 解码器结构进行训练的，也就是更加适合文本生成的形式。即输入一句话，输出也是一句话。这被称之为**对话模式**。它非常像人类的沟通交流模式。

  


> 我们是如何学会中文的？从出生开始，听别人说话，说出自己想说的话，也就是**对话**。我们每个人都是中文高手。
>
> **我们是如何学外语的？看教材、听广播、背单词。唯独缺少了对话！** 正是因为缺少了对话这个高效的语言学习方式，所以我们的外语水平才如此难以提高。

对于语言模型，同理。对话是涵盖一切 NLP 任务的终极任务。从此 NLP 不再需要针对特定任务建模这个过程。比如，传统 NLP 里还有序列标注这个任务，需要用到 CRF 这种解码过程；文本分类模型需要给每一个模型打标签；在对话的世界里，这些统统都是冗余的。这部分内容会在第 7 节详细来讲。

  


## **小样本（Few-Shot）学习**

GPT3 的论文标题叫做【[Language Models are Few-Shot Learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)】，语言模型是小样本学习者。

以往在训练 NLP 模型的时候，都需要用到大量的标注数据。可是标注数据的成本实在是太高了，这些都得人工手工一个个来标注完成！有没有什么不这么依赖大量标注的方式吗？

GPT3 就提出了小样本学习的概念，简单来讲，就是**让模型学习语言时，不需要那么多的样例数据**。

  


> 假设，我们训练一个可抽取文本中人名的模型，就需要标注千千万万个人名，比如“张雪华”、“刘星宇”等。千千万万个标注数据，就像是教了模型千千万万次同一个题目一样，这样才能掌握。
>
> 而人脑却不是这样，当被告知“山下惠子”是一个日本人名以后（仅仅被教学了一次），人脑马上就能理解，“中岛晴子”大概率也是一个日本人名，尽管人脑从来没听说过这个名字。

  


小样本学习的具体方式，主要放在第 9 节展开来讲，它的实现必然依赖超大规模的语言模型预训练。

  


# **ChatGPT**

> 需要说明的是，OpenAI 并没有发表 ChatGPT 模型原理论文，从严格意义上讲，ChatGPT 模型的实现细节，外界是无从知晓的。但是，OpenAI 在2022年发表了一篇 InstructGPT 论文：【**[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)**】让模型更好地听从人类反馈的指令。这两个模型几乎可以被认为是采用了完全相同的策略。
>
> 从此后各个科技公司的模型训练上看，InstructGPT 的原理几乎可以与 ChatGPT 画上等号。
>
>   
>
>
> 唯二的差别，就是 ChatGPT 是基于 GPT3.5 的，而 InstructGPT 基于 GPT3；并且，ChatGPT 用上了更大更优质的训练文本语料数据。

  


ChatGPT 模型结构上和之前的几代都没有太大变化，主要变化的是**训练策略**变了。

## **强化学习**

我们在第 1 节介绍到，ChatGPT 将 NLP 带入了强化学习时代，这是它爆火的关键原因。

  


训练 ChatGPT 所需要的文本，主要来自于互联网，这是一个有限的集合。而人类对 ChatGPT 提出的问题则无穷无尽，永远没有尽头，人类想要知道、感兴趣的内容，并不一定就存在互联网上。

  


> 按照传统的深度神经网络模型的训练思路，它只能根据互联网上已有的数据，做问答对标注，进训练模型。它学习的只是已有的数据本身。

  


而 ChatGPT 所利用的强化学习的思路，则是模拟一个**环境模型（Reward 模型）。** 首先，ChatGPT 会针对某一个问题，生成一个回答，环境模型会对 ChatGPT 生成的答案做评价，评价一个分值出来（如 10 分、3 分等等，高分代表奖励，低分代表惩罚），而不具体给出标准答案。ChatGPT 接收到评价反馈后，可以根据这个数值做模型的进一步训练，朝着生成更加恰当答案的方向拟合。

  



![2-3.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4361fc67e98f4985961760cacc0efb92~tplv-k3u1fbpfcp-watermark.image?)
  


由此，ChatGPT 模型已经不再局限于已有的训练数据集（标准问答数据对），可以扩展至更大的范围，应对从未见过的问题。具体的强化学习与 RLHF 原理将在第 10-11 节展开。

  


# 总结

纵观 ChatGPT 模型的进化历史，可以看出，模型的发展脚步就是在朝着模拟人类的方式前进着。

-   人类接收语言文字信息，输出语言文字，应用了编解码方式，ChatGPT 也利用了**编解码**的方式。
-   人类的大脑神经元数量是所有生物中最多的，ChatGPT 应用了超千亿的**大规模参数**模型。
-   人类采用了对话的方式进行交流，ChatGPT 建模也采用了**对话**的方式。
-   人类的大脑具有多种多样的功能，ChatGPT 也融合了**多任务**，各种各样的NLP任务。
-   人类可以通过极少量的样例进行学习，ChatGPT 也可以完成**小样本学习**。
-   人类可以在与实际环境的交互中学习知识，塑造语言，ChatGPT 也添加了**强化学习**，模拟与人类的交互。
-   **ChatGPT** **的发展史，就是人工智能模拟人脑的历史。**

## 3.基础：ChatGPT 是一个语言模型

从本节开始，我们将详细介绍一下 ChatGPT 的模型结构和训练方式，其中必然会涉及到一些公式和示意图。为了方便读者理解，一方面，我对公式尽量做了缩减，并以举例的形式进行解释；另一方面，我经常以打比方的方式，让大家直观地感受公式所蕴含的原理。当然，如果对于理解公式确实十分头疼，可以重点阅读文字部分，对公式有个定性的认识也是 OK 的。

# 语言模型是什么？

ChatGPT 是一个**语言模型**，是属于 NLP 领域的概念。那什么是语言模型呢？我们来举几个例子解释一下。

  


> 例1：请各位做一个完形填空：掘金社区是一个______的技术交流平台。

  


在这个例子里，上述的空格处应该填什么字呢？中文汉字总共有上万个，空格里填任何一个字，都算是完成了**填**这个动作，我们真正关心的，是**填什么字才能让文字读起来通顺**。

有的人觉得毫无疑问应该填“便捷”，而有的人觉得应该是“实用”，事实上填这两种答案都是正确的，可以让文字读起来通顺。再举一个例子：

  


> 例2：请补全这条语句：掘金社区是一个便捷的技术交流______

  


有的人觉得，应该填写“**网站**”，有的人觉得应该填写“**社区**”、“**平台**”等等，但总不太可能是其它别的答案了。

  


总结这两个例子，我们可以得出结论，空格处要填什么字，填几个汉字，是根据空格周围的**上下文**来决定的。能够正确根据上下文在空格处填入恰当的文字，表明其语言能力强，否则表示语言能力弱。

  


所谓**语言模型，就是由计算机来实现类似于人的语言交流、对话、叙述能力，它集中体现在模型能够依赖上下文进行正确的文字输出**。把上述这些预测空格内容的问题交给计算机建模来完成，就实现了语言模型的训练。换句话说，语言模型就是由上述的方式来进行训练的。

  


在上述第 1 个例子中，空格介于一段话的中间，填写需要依赖**上下文**。在第 2 个例子中，空格位于一段话的末尾，句子没有讲完，需要根据前面的信息，补充后面的信息。即，例 2 的填写只需要依赖**上文**，例 2 中的形式就是 GPT 所采用的建模形式。

  


# ChatGPT 语言模型的数学建模

  


## 1、语言模型基础建模

  


最经典的语言建模就是根据上文，输出下文，也就是例 2 的形式，这也是 GPT 模型的建模形式。

  


语言模型的建模公式可以表示为：

$$P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2|w_1) * P(w_3|w_1,w_2) * ... * P(w_n|w_1,w_2,...,w_{n-1})$$

其中， $$w_1, w_2, ..., w_n$$表示一个句子中的字符序列，$$P(w_i)$$表示字符 $$w_i$$在语料库中出现的概率， $$P(w_i|w_1,w_2,...,w_{n-1})$$表示在已知前面字符的情况下，字符 $$w_i$$ 出现的概率，即**依赖上文，对下文的预测**。把每一个字符都按照此方式预测出来，其结果就是整条语句的出现概率。

为了方便理解公式，我们依然以上述例 2 进行讲解。

> 例2：请补充未说完的语句：掘金社区是一个便捷的技术交流______

  


假设常用汉字总共有 10000 个，空格处要填写什么汉字，可以理解为从 10000 个汉字中随机地选取一个填入其中，因此可以建立概率模型。

  


假设在汉语中，“网”、“社”、“平”三个字的自然分布概率分别为 $$P(w_{15}=网)=0.0004$$，$$P(w_{15}=社)=0.0003$$，$$P(w_{15}=平)=0.0002$$，它表示在长达 10000 个汉字的一段网上寻找的随机文本中，“网”字出现 4 次的可能性最大，“社”字出现 3 次的可能性最大，“平”字出现 2 次的可能性最大。

  


然而在填写上述空格的时候，我们并不是直接根据三个汉字的概率进行抽取的，而是要依赖其上下文进行判断，于是我们有条件概率公式：

$$P(w_{15}=网|w_{0-14}=掘金...交流)=\cfrac{P(w_{0-15}=掘金...流网)}{P(w_{0-14}=掘金...交流)}=\cfrac{0.0000003}{0.000001}=0.33$$

$$P(w_{16}=站|w_{0-15}=掘金...流网)=\cfrac{P(w_{0-16}=掘金...网站)}{P(w_{0-15}=掘金...流网)}=\cfrac{0.0000002}{0.0000003}=0.67$$

  


注意，在这个公式中，具体的数值是虚构的，重点在于，当依赖有上下文的前提条件下，空格处填写“网”字的概率要远远大于“网”字本身在汉语中的出现概率，“站”字出现在“网”字后面的概率也会非常大，这说明“网站”作为一个词汇整体出现的概率是非常高的。

  


## 2、N-gram 语言建模

日常生活中，接触到的新闻文本可以非常长，动辄上千字、上万字的文章随处可见。过长的文本将导致在计算语言模型的条件概率时非常复杂，因此我们通常考虑将上下文依赖局限在一个较短的范围内。

通常情况下，语言模型的建模公式采用 **n-gram** 模型，这个词没有对应的中文翻译，其含义是**就近原则，距离第 i 个字符相隔 n 个字符距离以上的，就不在考虑范围内**了。公式来讲，就是将条件概率 $$P(w_i|w_1,w_2,...,w_{i-1})$$ 近似为 $$P(w_i|w_{i-n+1}, w_{i-n+2}, ..., w_{i-1})$$，其中 n 为 n-gram 模型中的 n 值。这样可以简化模型的计算，并且在一定程度上保持模型的准确性。

  


具体举例来讲，例 2 的计算中，上文依赖总共有 14 个字，“掘金社区是一个便捷的技术交流”，通常可以将其截短至大约 7 个字，也就是“便捷的技术交流”，因此，上述计算公式可以简化为：

  


$$P(w_{15}=网|w_{7-14}=便捷...交流)=\cfrac{P(w_{7-15}=便捷...流网)}{P(w_{7-14}=便捷...交流)}=\cfrac{0.00003}{0.0001}=0.33$$

$$P(w_{16}=站|w_{8-15}=捷的...流网)=\cfrac{P(w_{8-16}=捷的...网站)}{P(w_{8-15}=掘金...流网)}=\cfrac{0.00002}{0.00003}=0.67$$

  


## 3、语言模型 log 化

  


我们再来观察一下这个语言模型公式：

$$P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2|w_1) * P(w_3|w_1,w_2) * ... * P(w_n|w_1,w_2,...,w_{n-1})$$

  


这个式子是一个连乘，其中每一个因子都是一个概率值，介于 0~1 之间。多个这样的数值相乘，其乘积会越来越小，趋近于 0。正如上文中的计算结果，$${P(w_{0-15}=掘金...流网)}=0.0000003$$，随着句子长度越来越长，概率值会越来越小。在实际的语言模型建模过程中，往往上下文联系多达上百个字，按照上述方法计算下去，其结果会逐渐趋近于 0。但是，这里这存在一个问题。

  


> 计算机存储小数值一般以浮点数的形式进行存储，包括 float 和 double 类型，分别占用 4 字节和 8 个字节。其中：
>
> -   float 类型表示的数据范围为 -3.40E+38 ~ +3.40E+38，精度为 6~7 位有效数字；
> -   double 类型表示的数据范围为-1.79E+308 ~ +1.79E+308，精度为 15~16 位有效数字。
>
>   
>
>
> 这就说明，**计算机表示小数是有一定限度的**，小数点后的位数不可能无限小。一个无限趋近于 0 的小数，计算机是很难表示出来的。

  


因此，按照上述语言模型建模，对于计算机的系统和结构来说，都是无法实现计算的，很容易导致语言模型计算溢出计算机数据存储范围。

  


所以我们应当对上述式子做对数变换：

$$log(P(w_1, w_2, ..., w_n)) = log(P(w_1)) + log(P(w_2|w_1)) + log(P(w_3|w_1,w_2)) + ... + log(P(w_n|w_1,w_2,...,w_{n-1}))$$

  


这种连加形式，实际上就是：

$$log(P(w_1, w_2, ..., w_n)) = \sum_{i}{ log(P(w_i|w_1,w_2,...,w_{i-1}))}$$

  


考虑 n-gram 模型建模，我们只取 k 个字符范围的上文进行建模，可以得到最终的语言建模公式为：

$$log(P(w_1, w_2, ..., w_n)) = \sum_{i}{ log(P(w_i|w_{i-k},w_{i-k+1},...,w_{i-1}))}$$

  


# ChatGPT 的语言模型

  


## 1、ChatGPT 的建模公式

ChatGPT 就是一个复杂的语言模型，其内部原理和上述的概率模型本质上是一样的。

  


ChatGPT 是利用成千上万的文本语料训练得来的。每一篇语料都是一条文本，可以抽象成$$w_1, w_2, ..., w_n$$这样的一串字符序列，假设这个大的语料为 U，则我们的建模公式又可以表示成：

$$log(P(U)) = \sum_{i}{ log(P(w_i|w_{i-k},w_{i-k+1},...,w_{i-1}))}$$

当然，ChatGPT 模型是由巨量的参数构成的，设$$\Theta$$为 ChatGPT 的参数集合，则模型的建模公式可以表示成：

$$log(P(U)) = \sum_{i}{ log(P(w_i|w_{i-k},w_{i-k+1},...,w_{i-1};\Theta))}$$

  


这就是 GPT 初代论文【*[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)*】中的模型建模公式。下图为论文中的建模公式，你可以从论文中仔细对比理解这个公式，除去公式中的字符不同，其它完全一致。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/46d28a92be6f461b8325cb86d58a3058~tplv-k3u1fbpfcp-zoom-1.image)

  


## 2、语言模型中的最大似然概率

在 ChatGPT 的语言模型公式中，左半边是$$log(P(w_1, w_2, ..., w_n))$$，它其实就是一种最大似然的建模方式。在概率论中，最大似然概率是一种最为简单，也最为广泛应用的近似估计方法。比如：

  


> 2023 年，全中国人口总数大约 14 亿。作为国家统计局，应该如何**花费最低成本**，统计全国人口各个年龄段的分布情况呢？

  


为了解决这个问题，把全国所有人口的出生年月全部数据都拉出来统计一遍，得出“70 后总共 2.1 亿，80 后 2.4 亿，90 后 2.0 亿，00 后 1.5 亿”这样的结论，当然是可以的，但是成本太高了。

  


一个非常直观的思路，就是**抽样调查**，我们去大街上、菜市场里、学校里、写字楼里分别找一些人，询问他们的出生年月，然后做一个统计，直接把这个统计结果当作是全国人口的年龄分布。

  


在这个直观的思路背后，就蕴含了最大似然估计方法，我们**利用采样的数据分布去估计整体的数据分布**。它假设了一个前提，就是**认定采样的数据非常具有代表性**。只要我采样了 1000 个人的年龄分布数据，我就有理由充分相信，这一组数据是发生概率最大的那一组，是具有代表性的，这就是最大似然估计（Maximum likelihood estimation）。随着采样的数量越来越大从 1000 人到 10 万人，再到更多，估计的结果也就越来越接近全国 14 亿人的整体年龄分布。

  


回到上述的 ChatGPT 语言模型建模公式，我们同样可以做这样一个设想。

> 全世界所有人都在使用各种不同的语言文字，每一天，人们都说出各种各样的话语，媒体上刊登各种各样的文章，这可以构成一个集合——**人类语言文字全量总集**。

  


如果我们在训练模型的时候，能够全量使用这个总集合当然好，但实际上却办不到。我们只能取到互联网上存在的一部分文本数据，比如 Wikipedia 上的数据、BBC、今日头条上的新闻、论坛里的网民数据等等。当我们取到这样一个成规模的语料数据集之后，需要先假定一点，**这些有限的语料数据，充分代表了人类语言文字全量总集**。

  


因此，训练模型时，就是在利用极大似然估计的方式，使得建模公式的左边$$log(P(w_1, w_2, ..., w_n))$$概率值最大，它也是模型预训练的目标。

  


## 3、ChatGPT 语言模型的训练方式

  


假设我们已经获取了大量的语料数据，即将训练一个自己的 ChatGPT 模型，语料样例如下：

> 文本1：甲方应当在收到上述租金后 15 日内，应向乙开具合法有效的增值税专用发票。
>
> 文本2：去餐厅吃饭的时候，别人都不愿意和我坐在一起，我只能孤零零的一个人，所以很不开心。
>
> 文本3："止咳化痰”及其它对症治疗后，咳嗽咳痰较前好转。患者当地医院支气管镜我院病理会诊（H2019-00310）：阅杭州迪安医学检验中心 HZ2019066642 HE×2张，IHC×8张
>
> 文本n：... ...

  


这些语料描述的内容千奇百怪，长度各不相同，当然还有错别字、特殊符号等等。我们该如何组织利用这些数据呢？

  


模型的训练方式，归根结底就是要做概率的预测。首先，抽取其中一条文本，例如这一条法律领域的文本：

> 甲方应当在收到上述租金后 15 日内，应向乙开具合法有效的增值税专用发票。

  


首先，我们只关注句子的开头，把“甲方”二字当作模型的输入，预测模型接下来最有可能输出什么汉字：

一个优质的 GPT 预训练语言模型应当能够根据上文，来对下文进行预测。这里的训练过程就用到了前述的最大似然估计，“甲方”二字是一个不完整的句子，后续可以接续很多可能的汉字组成连贯的句子，比如“甲方**应**当在……”“甲方**如**有违约……”“甲方**可**以申请……”等等。后续可以接任何汉字，所有的条件概率加起来概率等于 1，如下图所示。

$$\sum{P(w_{2}|w_{0-1}=甲方)}=1$$


![3-1.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/86ad8e10156942cdbb3d9a3972f97f45~tplv-k3u1fbpfcp-watermark.image?)

在图中，我们大致可以看出，“应”字的条件概率值最高，为 0.3。

  


但是目前我们仅仅有上述这一条文本数据，根据**最大似然估计**原则，最有可能的出现在“甲方”这一句子之后的字，是“应”字。那么，我们在模型训练的过程中，就要使“应”字出现在“甲方”之后的概率尽可能地大。下图所示的就是一个根据当前举例的数据，训练得到的模型结果。


![3-2.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7e69cc213fd24ce7bf053ba1c37a9864~tplv-k3u1fbpfcp-watermark.image?)
  


接下来，就是重复上述过程，递进地让模型学习上述一整句话，方法如下图所示。


![3-3.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4a5affcdbdfa4c689bd406d2a45137ac~tplv-k3u1fbpfcp-watermark.image?)

用一句话来概括上述的 GPT 训练方式，就是让模型学会**文字接龙**游戏。

实际上，**ChatGPT 训练语言模型的过程，就是 ChatGPT 模型预训练的过程**。本节主要介绍语言模型的建模，预训练的内容将在第 8-9 节中展开介绍。

  


# 总结

-   ChatGPT 的语言模型建模公式为：

$$log(P(U)) = \sum_{i}{ log(P(w_i|w_{i-k},w_{i-k+1},...,w_{i-1};\Theta))}$$

-   语言模型的训练就是让模型根据上文，猜测下文最可能的内容，即文字接龙。
-   最可能的内容，实际上是一种最大似然的准则来约束模型的训练目标。

## 4.核心：ChatGPT 是如何处理文字输入的？

第 1 节里，我们介绍清楚了 ChatGPT 模型的输入和输出，实际上就是将文字输入 ChatGPT 模型当中，然后再让模型预测出文字，本质上就是一个“文字接龙”式的**语言模型**。

  


而文字在进入 ChatGPT 模型之前，需要先经过一个转换，形成另外一种数据形式。在 ChatGPT 计算处理完之后，也需要将结果再做逆转换，形成文字形式，反馈给用户。这种转换包括两个步骤，Tokenizer 和 Embedding。本节主要介绍这两个模块。


![4-1.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bd699b19402e47729a6d05f09d96bb03~tplv-k3u1fbpfcp-watermark.image?)
  


# Tokenizer

  


ChatGPT 官方目前已经开始对服务收费了，收费方式主要是计算用户使用的 token 数，数量越多，收费越高。

> 例如，用户提问了一条文本，文字（带标点和各种特殊符号）共有 50 个字符，但耗费了 30 个 token，ChatGPT 根据输入生成一条回答，总计 200 个 token，逆转换为文字总共 300 个字，那么用户一共消费的 token 数就是 30+200=230 个。那什么是 token 呢？

  


**token 是任何 NLP 神经网络 模型接收用户输入的最小粒度。** token 本身就是一些字符的组合，如英文单词`#cat`、中文词汇`鞋子`、英文词缀`ly`、中文汉字`珂`等，都可以看作是一个 token。

  


将用户输入的文本转换为 token 序列的过程就叫做 Tokenizer。它包含两部分，一部分是从文字转换为 token（设置在进入 ChatGPT 之前），另一部分是将 token 转换为文字，也就是逆转换（设置在 ChatGPT 模型输出之后）。

  


## Tokenizer 算法 BPE 执行流程

Tokenizer 目前最流行的实现方法是 **字符对编码** **BPE（Byte Pair Encoding） 算法**，它也是 ChatGPT 采用的算法。BPE 算法是根据一份 token 词表（Vocabulary），将输入的文本拆解成若干个 token。其中，每一个 token 都存在于词表。


![4-2.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9649fd19936945e4ac126054ff92cfad~tplv-k3u1fbpfcp-watermark.image?)
  


具体以如下一条输入模型的文本为例：

> The newest car has a lower price and the lowest fuel.

这条文本中，包含了 53 个字符（包含字母、空格和标点，以及任何键盘可以打出的特殊符号，均输入 ChatGPT 中）。

一般地，模型训练所使用的词表中 token 数量大约从几万~几十万不等。假设 BPE 算法已经生成一个 **Token 词表（Vocabulary）** ，其部分词表 token 内容如下：

| #low      | est     | #new | er    | #the  | #car |
| ---------- | ------- | ----- | ----- | ------ | ----- |
| #and      | #fuel  | #a   | #has | #have | and   |
| #thailand | #price | #dog | #old | #most | ...   |

BPE 算法就是根据上述 token 词表对文本进行匹配，从文本中拆分出存在于词表中的 token，将文本转换成如下的形式：

> The newest car has a lower price and the lowest fuel.
>
> =>
>
> `#The`, `#new`, `est`, `#car`, `#has`, `#a`, `#low`, `er`, `#price`, `#and`, `#the`, `#low`, `est`, `#fuel`, `.`

  


在这条例子中，文本被拆分成 15 个 token。由于英文单词是以空格形式进行分割的，因此，每一个单词的首字母都添加`#`为单词起始的标识，它可以理解为一个空格，不加`#`的token表示无法独立成词。一些单词被拆分成若干部分，如`newest`被拆分成两部分，`#new` 和 `est`。然后，模型就接收这样的 token 数据做进一步处理计算。

  


从上面的例子中，我们可以看出，token 中一般都是以非常高频的字符组合构成的，而且这些 token 往往具备一定的语义。例如，`newest`被拆解为`#new` 和 `est`，前半部分是单词词根，后半部分是英文形容词最高级。

  


同样地，ChatGPT 模型在回答用户问题，输出答案时，也是首先输出 token 序列，再将 token 序列反转为正常的自然语言文本，这个操作叫做 **D** **e-tokenization**。它与 Tokenization 是完全**互逆**的操作。读者可以尝试把上面的 token 序列合并成完整的文本句子。

  


> 对于中文而言，常用汉字大约为 7000 个左右，而且文本之间不存在空格。因此，也可以采用上述的算法来完成，唯一的区别就是中文的 token 的开头不添加 `#` 符号。一些极为常见的中文单词可能合并为一个 token，如`我们`，而考虑到词频，绝大多数中文依然以单字独立成 token。

  


### Byte-level BPE 算法

之前介绍的 BPE 算法是基于**字符**的，除此之外，还有一种基于**字节**的 BPE 算法（Byte-level BPE）。这种方法，主要是为了克服基于字符的 token 词表，由于各种特殊字符数量太庞大导致效果变差。

  


除了我们常用的中文外，ChatGPT 可以随意操作英文、日文、韩文、法文等至少二十多种文字。这些语言的文字和符号更是多种多样，有英文拉丁字母`ABCDabcd`，中文汉字`千百花鸟风月`，西里尔字母`БГД`，日语假名`ピンイ`，当然也包括很多 emoji 特殊符号`💁👌🎍😍`。

  


所有的字符在计算机中都是以 **Unicode** 编码格式实现的。

> **Unicode 编码**是一种用于计算机表示全球范围内各种语言文字字符的标准编码方式，它为世界上所有的字符都分配了一个唯一的数字编号，解决不同国家和地区使用不同语言文字、字符集的问题。 Unicode 编码采用 16 进制表示，每个字符都有一个唯一的码点，例如汉字“**中**”在 Unicode 编码中的码点是U+4E2D，其中 U+ 表示 Unicode 编码，4E2D 是该字符的 16 进制码点。若以 UTF-8 编码为例，汉字“中”被转换为 **3 个字节（byte）** 的二进制数据：11100100 10111000 10101101。

  


Unicode 常用字符目前总量大约有十多万，如果直接基于字符形式，构造 token 词表的话，那么词表可能会变得非常庞大，达到几十万。过于庞大的词表会对 ChatGPT 模型产生很强的不确定性因素，让模型难以训练。

  


因此，**Byte-level BPE 算法**应运而生。这种算法的执行步骤和上述的 BPE 算法完全一致，唯一的区别在于，BPE 算法直接操作 Unicode 字符，而 Byte-level BPE 算法把文本的字节作为直接操作的对象。

  


例如，在 BPE 算法中，`中` 字被当作一个字符进行 token 匹配。而在 Byte-level 算法中，它被当作 3 个字符进行匹配（因其 Unicode 占用 3 个字节）。而英文字母如 `p` 则在两种算法中，都被当作一个字符处理，因为字母的 Unicode 编码只占用一个字节。所有的字节个数全部加起来不过 256 （即一个字节所表示的符号个数）个，这对模型训练是一个巨大的利好。

  


Byte-level BPE 算法的代码链接：[Byte-level BPE](https://github.com/dongrixinyu/JioNLP/blob/master/jionlp/algorithm/bpe/encoder_decoder.py)，感兴趣的可以阅读一下。

  


## BPE 的词表是如何训练得到的？

BPE 的词表主要是根据训练文本语料统计得到的，训练的语料数量越大，得到的 BPE 词表越准确，越具有词根语义。

假设根据一份语料数据，我们可以统计得到如下**词汇**和其对应出现的次数。

| #lowest   | 7  | #lower | 4 | #newest | 5  | #older | 5 | #newer  | 4   |
| ---------- | -- | ------- | - | -------- | -- | ------- | - | -------- | --- |
| #and      | 10 | #fuel  | 4 | #a      | 14 | #has   | 4 | #oldest | 5   |
| #thailand | 3  | #price | 6 | #new    | 7  | #old   | 6 | ...      | ... |

以上均为文本中存在的完整的词汇。

-   接下来，我们可以按字母进行统计，得到频率最高的字符对为标红的 “es”，共计出现 17 次。我们单独把 “es” 提出来，并把语料中的所有 “es” 看作一个整体。
-   再重复上面的过程，可以发现“est”可以看作是“es” 和 “t” 的结合体，总计也出现 17 次。因此，可以把“est” 看作一个整体，放入词表，并把语料中所有的 “est” 看作一个整体。
-   再重复上面的过程，可以发现，“#a” 的频率仅次于 “est”，为 14 次。因此，把 “#a” 放入词表中。
-   再重复，可以把 “er” 这个字符对提取出来。
-   以此类推，我们可以逐渐将高频的字符对提取出来，不断放入词表中。
-   当放入词表中的 token 数达到了预定的最大数 N 时（一般从几万到几十万不等），得到最终的词表，即可用于BPE 算法的执行流程，拆分每一条文本为若干 token。

  


## Tokenizer 的好处

### 克服长尾效应 OOV

在英文单词中，最常出现的 5000 个单词占据了实际使用量的 90%。而那些**极低频**的单词数量极多，但总共加起来的实际使用量也不超过 2%。这就是自然语言的**长尾效应** **，** 这种现象也出现在其它语言中。

  


直接把极低频的单词和字符当作 token，本身意味着数据量的缺乏，会导致它有可能不在词表中（Out Of Vocabulary，OOV），对 NLP 模型的性能产生很大的影响。因此，引入 Tokenizer，采用 BPE 算法可以避免低频词作为 token。

  


例如，根据上述训练例子得到的词表，`#strangest` 这个词在训练语料中词频较低，可能不出现在 token 词表中，但 “`#strang`” 和 “`est`” 一定以较高的频率出现在 token 词表中。

  


### 多语言支持

在早期，NLP 神经网络模型功能十分单一，且仅支持某一种语言。一个针对英文的文本分类模型，并不能支持中文的文本分类。而 BPE 算法，包括 Byte-level BPE 算法的设计，使得一份词表中包含了多种语言的字符，支持模型的多语言处理功能。

  


# 词嵌入（Embedding）

  


ChatGPT 的输入文字转换为 token 之后，还需要将 token 再转换为张量，这个过程叫做词嵌入（ Embedding），同时 embedding 也指被转换后得到的张量本身。

  


在神经网络中，**张量** **（** **Tensor** **）** 是指多维数组，它可以存储和处理大量的数据，是神经网络中最基本的数据结构。张量一般都以浮点数（小数的一种计算机表示形式）作为元素进行填充。

> 例如，$$a=[[1.034, 0.932, -0.347],[0.023, -1.025, 0.256]]$$就是一个(2,3)形状的张量，是一个多维数组。

而**向量（vector）**，就是高中数学中的概念，一般就可以看作是一维张量。


ChatGPT 从功能上看，是一个语言模型，但从结构上看，它是一个多层的、复杂的神经网络模型，每一层的神经网络都在进行浮点数张量（Tensor）的数字计算，而 ChatGPT 的输入是文字符号，token 也是文字符号。因此，**token 需要先转换为** **浮点** **数字**，再进入模型中进行计算。将用户输入的 token 转换为浮点数张量的过程，就叫做**词嵌入（Embedding）** 。当模型将结果计算完，也要将最终的浮点数转换为具体的 token，作为输出。

  



![4-3.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/27df0ae5b358462eb4035b3dd70580d6~tplv-k3u1fbpfcp-watermark.image?)
  


> `#The`, `#new`, `est`, `#car`, `#has`, `#a`, `#low`, `er`, `#price`, `#and`, `#the`, `#low`, `est`, `#fuel`, `.`

仍以上述句子为例，假设 token 词表（Vocabulary）的数量总共为 N，每一个 token 都用一个 M 维的浮点数张量表示，其中每一个 token 都对应了一行张量，即该 token 的 embedding 表示。

  


例如，`#price` 这个token 对应的 embedding 是一个 M 维向量：

$$\#price \to [0.103, 0.034, 0.129, -0.219, -0.156, ... , 0.0284, -0.172]$$

这组数据就可以传入 ChatGPT 模型中，做模型的训练和使用。所有的词表组成了一个 $$N \times M$$维度的张量，如下图左侧方阵。


![4-4.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3bbe9e7446ce4a2b9ceaea6b10195e11~tplv-k3u1fbpfcp-watermark.image?)
  


根据例子中的前四个 token，我们可以将其对应的 embedding 抽取出来，按 token 的顺序排列成一组 $$N_{输入 token 数} \times M$$的张量，这组张量即可输入 ChatGPT 进行操作，图中白色部分表示词表中的词汇未匹配到 token 序列。换句话说，它完成了由 token 到其对应张量的映射。

  


在实际模型当中，一次性输入给模型的 token 数量$$N_{输入 token 数}$$并不是无限大的，例如，在 ChatGPT 的 `gpt-3.5-turbo` 版本中，最大的输入 token 数量为 4097 个，超出这个范围则会被模型自动截断。

  


在自然语言中，文字的顺序是非常重要的，“我喜欢你”，和 “你喜欢我” 表达的含义是完全不同的。所以，ChatGPT 考虑到模型的每个 token 相互之间的顺序不能改变，需要明确地在输入端标识出每个 token 的位置张量（Position Embedding），其大小和 token 的 embedding 是一致的。两者以如下形式融合起来：
$$embedding_{input} = UW_e + W_p$$ 。

其中，$$W_e$$是 token embedding 矩阵，$$W_p$$是 position embedding 矩阵。而其中的 $$U$$ 是一个上下文矩阵。根据第 3 节的语言模型原理，模型在建模时有上下文限制，针对当前的一个 token，模型只能关注该 token 之前的 $$k$$ 个 token。因此，$$U=(u_{-k}, ... , u_{-2}, u_{-1})$$，它是一个单位矩阵。

假设 token 数量小于模型可接收的最大数量，那么，上述公式可以退化为：
$$embedding_{input} = W_e + W_p$$。

  


由此，即可输入 ChatGPT 模型进行计算。



![4-5.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d244c96dfe134f34a1d1d98c8ebdae47~tplv-k3u1fbpfcp-watermark.image?)
  


第1节中提到，ChatGPT 是有多轮对话能力的。



![4-6.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9c4fe48260c44157bca84e0fffa93ca2~tplv-k3u1fbpfcp-watermark.image?)

在模型中，需要从输入端将输入1（Q1）、输出1（A1）、输入2（Q2）等部分信息区分出来。这几个部分信息分别叫做一个 segment，其中每一个 segment 都包含了多个 token，它们共享了同一个 segment embedding。具体方式如下：



![4-7.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5d9962ffc719483c989c50ebc1725bff~tplv-k3u1fbpfcp-watermark.image?)

上图中做了假设：Q1、A1、Q2 分别包含了 4 个 token。当然，在实际输入中，每个 segment 包含的 token 数都是可以灵活变化的；上面对话的轮数仅有两轮，而实际输入中，对话轮数可以非常多，形如 Q1、A1、Q2、A2、...、Qn， 只要所有 segment 对应的 token 总数加起来不超过模型允许的最大 token 数即可。

  


因此，输入给 ChatGPT 的 embedding可以表示为如下公式：

$$embedding = embedding_{segment} + embedding_{position} + embedding_{token}$$

  


## Embedding 的好处

最早的时候，NLP 是直接处理文本字符串，没有 Embedding 这个操作的。Embedding 这个操作最早是由 **word2vec 模型**提出并实施的，GPT系列模型，包括 ChatGPT 已将此操作作为了固定默认步骤。

  


### Embedding 方便接入大规模神经网络

我们在第 2 节中论述了，AI 想要有较高水平的智能，其模型规模必然比较大，参数量众多。在机器学习领域，神经网络模型是最容易扩展其模型规模的。我们会在第 8 节讲解神经网络相关的概念。

  


如果没有 Embedding 操作，那么 NLP 领域依然停留在直接处理字符的层面上，模型的规模扩展难度较大。embedding 将文字对应的 token 转换为抽象的固定维度的张量，标志着 NLP 迈入了深度神经网络的大门。

  


### Embedding 抽象了 token 的语义

当我们训练好 ChatGPT 这个模型之后，假设我们抽取出如下 token 对应的 embedding 向量：

`#price`（价格），`#cost`（开销），`#trunk`（卡车），`#texi`（出租车）

其对应的均为 M 维 embedding 向量。计算两个向量相似度的方式主要采用余弦距离，则一定有：

$$cos(price, cost) > cos(price, truck)$$

$$cos(trunk, texi) > cos(cost, texi)$$

其含义为，price 和 cost 在 embedding 上的相似度，要大于 price 和 truck 的相似度，这符合人们的语言直觉。可以得出结论，在自然语言中，语义相近的两个词汇，其 embedding 向量之间的数学意义上的距离更相近。

  


换句话说，**Embedding** 建立了自然语言的语义与数学之间的关联关系**。

  


# 总结

-   Tokenizer 将模型输入的文字转换为 token 序列。
-   ChatGPT 使用了 BPE 算法实现 Tokenizer。
-   Embedding 将 token 序列映射为张量矩阵，方便模型进行张量矩阵运算。

## 5.ChatGPT 的灵魂：Attention 注意力机制

OpenAI 的 GPT 系列模型，包括其它科技公司研发的各种最先进的 NLP 模型，甚至图像处理模型，广泛采用了 Attention 注意力机制进行建模，它可谓是当前 NLP 神经网络的灵魂机制。

  


# 注意力机制的思想

相信大家在学生时期，都被家长或老师提点过：“听课的时候注意力集中点！不要东张西望！” 这里就用到了注意力机制。这句话的含义是，学生应当把注意力集中在接收课堂知识上，而不是放在无关的信息上。

  


注意力机制的思想实际上广泛应用在各个方面，它可以抽象为如下形式：

**一个智能体**（人或 AI 模型）**从接收到的大量信息**（文本、图像、音频）**中，剔除不重要、不相关的信息，重点关注与自身密切相关的信息**。其核心在于收缩关注的信息范围，实现信息的压缩。

  


根据第 3 节的介绍，在 NLP 中，ChatGPT 语言模型建模实际上是寻找输入文本的**上下文关联**关系。例如：

> 例2：请补全这条语句：**掘金**社区是一个便捷的技术交流______

在这条文本中，想要补全最终的语句，应当参考前文的信息，而前文总共 14 个字，对空格处影响最大的是`掘金`两个字，而像形容词`便捷的`，系词`是一个`都不是最关键的影响因素。换句话说，我们应当设计一种注意力机制，让模型能够在输出空格字符的时候，最大限度地注意到`掘金`两个字。

  


# 注意力机制的建模

## 建立权重模式

根据第 4 节的介绍，在 NLP 模型中，自然语言是以 token 形式排列输入模型中的。如下图所示，绿色的每一列都是对应的一个 token 的 embedding 向量表示，假设每一个 token 的 embedding 具有 7 维，总共 14 个 token 共同组成一个 embedding 矩阵。我们的模型设计思路，是模型应当能够在输出`平台`的`平` 字时，更加关注到`掘金`二字。


![5-1.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f4a49559dfde49f980f4d2bdc84309ab~tplv-k3u1fbpfcp-watermark.image?)

  


让模型更加关注到`掘金`两个字，实际上可以认为，给`掘金`两个字对应的 token embedding 赋予更大的**权重**。

在神经网络模型中，所有的操作均为矩阵操作，所有的特征均为向量形式。设$$e_i$$表示第 $$i$$个 token 的 embedding 表示，在例子中，它是一个 7 维的向量，$$w_i$$是第 $$i$$个 token 对应的权重值，它是一个标量值。那么，可以对所有的 token embedding 做一个加权：

$$h = \sum_{i} e_i w_i$$

这里，$$h$$是一个加权后的结果，它也是一个 7 维的向量。它的本质含义，是从各个 token 不同的 embedding 中，按重要程度（权重值$$w_i$$）做加和，权重值高的$$e_i$$，对后续操作影响大，权重值低的$$e_i$$，对后续操作影响小。这就产生了一种更加注意权重高的 $$e_i$$的效果。

  


## softmax函数

上面的加权计算中，$$w_i$$是一个标量值，假设模型经过训练后，针对上述例子的 token 计算得到：
$$w=(w_1,w_2,...,w_i,w_{14})=(2.1, 1.3, ... , -1.2, -0.4)$$ 

其中的数值有正有负，前两个 token 对应的权重标量值较大，说明对后续操作的影响大。若直接进行加权，这不符合人们的一般认知。一般来说，权重占比以概率形式表示，概率值应当大于 0，小于 1，且所有分量的加和等于 1。

  


> 例如：今年国内的 GDP 占比中，第一产业占比 14.6%，第二产业占比 35.2%，第三产业占比 50.2%。
>
> 这是一个典型的概率分布示例，其总和为1，三产占比最高，权重最大，影响经济的程度最大。

  


因此，我们应当将$$w$$转换为概率占比形式，主要采用 softmax 方法：

$$\alpha_i= \cfrac{exp(w_i)}{\sum_{i}exp(w_i)}$$

其中，$$exp$$是指数计算，$$\alpha_{i}$$表示第 $$i$$个 token 的 embedding 对应的权重，其值介于 0~1 之间。计算上例，假设：

$$w=(2.1, 1.3, 0.1, 0, -0.2, -1.3, 0.5, 0.2, -0.8, 0, 0.1, -0.7, -1.2, -0.4)$$

那么，第 1 个 token 的权重：

$$\alpha_1=\cfrac{exp(2.1)}{\sum{exp(w_i)}} = \cfrac{8.166}{8.166 + 3.669 + 1.105 + 1 + ...+ 0.301 + 0.67}=\cfrac{8.166}{21.858}=0.374$$

此外，其它若干 token 的权重为：

$$\alpha_2=\cfrac{exp(1.3)}{\sum{exp(w_i)}} = \cfrac{3.669}{8.166 + 3.669 + 1.105 + 1 + ...+ 0.301 + 0.67}=\cfrac{3.669}{21.858}=0.167$$

$$\alpha_6=\cfrac{exp(-1.3)}{\sum{exp(w_i)}} = \cfrac{0.272}{8.166 + 3.669 + 1.105 + 1 + ...+ 0.301 + 0.67}=\cfrac{0.272}{21.858}=0.012$$

  


第 1、2 个 token 对应`掘金` 两个字，分别占权重 37.4% 和 16.7%，占比较高，第 6 个字是`一`，它的 token 对应的权重仅 1.2%，占比较低。这说明，在这个模型中更加注重了`掘金` 的权重，方便后续模型输出正确的字符 token。

  


这个例子说明了 softmax 算法的一些特性：

1、softmax 可以将一维向量，输出形成概率分布的形式；

2、softmax 利用指数函数，会更加着重值更高的元素，使要关注的元素更加突出；

3、相应地，由于指数函数特性，它也可以尽力压低不需关注的元素的权重。

以公式形式表示，计算模型的所有 embedding 加权后的权重：

$$h = \sum_{i} e_i \alpha_i$$

Softmax 函数在神经网络模型中十分常用，除了应用在注意力机制计算外，softmax 还可以完美契合交叉熵损失函数（将在第 8 节中介绍）。

  


## 自注意力机制 Self-Attention

前文讲述了，利用权重向量 $$w$$ 就可以找到模型要关注的重点内容。那么，$$w$$ 从哪来呢？值如何计算出来？

计算权重，不同的模型、不同的 NLP 任务都有不同的形式。这项技术经过多年的发展，最终趋向于**自注意力机制（Self-Attention）** ，这也是 ChatGPT 所采用的形式。

  


$$w$$是一个权重向量，其长度（维度）与 token 的个数相同，其中的每一项是标量值。我们知道，神经网络模型中都是以向量、矩阵等构成的张量作为计算基础的。因此，想要计算得到一个标量值，最简单的形式就是**向量点积**，我们需要想办法找到两个向量。

  


假设针对第 $$i$$个 token，有两个向量 $$q_i$$ 和 $$k_i$$，两者具有相同的维度，其点积可以得到一个标量值：$$w_i=q_ik_i$$。

  


依然以前述句子为例。在补全句子时，`掘金`对要空格处填写的字符，影响最大。而“`掘金`对要填写什么字符，影响最大”这一认知，依然是我们阅读这个句子本身得到的。换句话说，$$w_i$$**权重的信息来源，依然是原句子本身（Self）** ，这就是自注意力命名的原因。

  


因此，我们可以直接把每个 token 的 embedding 分别当作向量 $$q_i$$ 和 $$k_i$$ 进行计算。计算过程如下图所示。


![5-2.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/16756e3b9ea747b0968d818926136314~tplv-k3u1fbpfcp-watermark.image?)

当我们计算第 $$i$$个 token的 $$w_i$$值时，以第 1 个 token `掘`字为例，应当先观察整条文本的 token，将所有的 token 都和 `掘` token 做计算，这实际上就是在比较 `掘` token 和其它 token 的关联关系。

在上图中， $$q_i$$ 含义为 **query**（中文含义为**查询向量**），和 $$k_i$$的含义为 **key**（中文含义为**钥匙向量**）。

  


> 为何起名 query 和 key？
>
> query 和 key 这两个词，最初是计算机搜索引擎中提出的概念，在 AI 领域中，被引申到注意力机制上。
>
>   
>
>
> 例如，当我们在 Google 搜索引擎中查询搜索“成都有什么好吃的？”时，搜索引擎会按匹配程度给出若干回答。
>
>   
>
>
> 其中，用户的搜索问句，就被称为 query，而每一条匹配到的结果，都包含和 query 的关联性，也就是各自的 key。通过 query 和 每一条 key 的匹配，就可以得到问答搜索结果的匹配程度。

  


回到上图中，当计算 `掘` token 的输出向量时，即利用其 query 向量，分别和每一个 token 的 key 向量做点积，并对这个点积 score 做归一化（图中的 2.645 实际上是 $$\sqrt{7}$$，即 query 向量的维度$$\sqrt{d_{q}}$$）。由此，就得到了熟悉的 $$w$$权重向量，进而执行 softmax，就得到了一个概率分布$$\alpha$$ ，由此可以计算出 `掘` token 的输出向量。

  


为什么要除以 $$\sqrt{d_{q}}$$？神经网络模型的训练过程是采用**梯度下降法**来完成的。这里具体细节不展开，你可以参考第 8 节的内容。

  


梯度下降非常像一个人从山顶上走到山脚下。放在神经网络的训练中，一个良好的训练过程，是人能够顺利找到下山的路，平稳下来。而若遇到一段很长的平路，没有向下的坡路，则说明模型训练遇到了阻碍。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4d54ca321ce14329a7bee1e627284098~tplv-k3u1fbpfcp-zoom-1.image)

既然模型需要做梯度下降，则必须保证模型在做参数运算过程中，梯度值是存在且较大的。而 softmax 函数很容易造成梯度消失，消失原因在于，输入 softmax 函数的值过大。

在上述例子中，$$q_ik_i$$**乘积过大，会导致模型训练过程中的梯度消失，进而模型训练失败**。因此，为了限制这个乘积值，需要除以这两个向量的维度根号值，确保数值在稳定的范围内。

  


计算后续每个 token 位置的输出向量均同理。若想计算最后一个输出位（即空格处，第 15 个 token 要填的字）的向量，计算方法也和上面同理。

  


上述计算方式是一步一步地以向量表示形式展开的。若以矩阵形式做公式计算，则可以表示为：

$$Attention(Q,K,V)=softmax(\cfrac{QK^T}{\sqrt{d_q}})V$$

其中：

- $$Q=(q_1,q_2, ..., q_i, ..., q_{d_q})$$

- $$K=(k_1,k_2, ..., k_i, ..., k_{d_k})$$

- $$V=(v_1,v_2, ..., v_i, ..., v_{d_v})$$

其中，$$QK^T$$ 就是点积的矩阵形式，最右侧的 $$V$$ 可以理解为 token embedding 组成的矩阵。$$softmax(*)V$$ 实际上就是前述各个 embedding 的注意力权重 $$\sum_{i} e_i \alpha_i$$ 的矩阵表示形式，$$e_i$$ 表示第 $$i$$个 token 的 embedding 表示，$$\alpha_i$$ 是注意力权重概率值。

  


还需要补充说明的是，我们在讲解注意力机制的计算过程中，默认了 $$q_i、k_i、v_i$$都是 token embedding 本身，在实际的模型中却并不是这样。

  


> 第 2 节中，我们提到了 ChatGPT 模型为了体现强大的模型拟合能力，具备较为高级的智能，其模型参数量是随着规模逐渐增大的。到了 ChatGPT 的基础模型 GPT3.5，模型训练的参数规模已经达到了 1750 亿。
> 
> 神经网络模型之所以很容易契合模型膨胀扩张这一需求。其关键特点在于可以加参数。
> 
> 在 $$q_i、k_i、v_i$$ 这里，也可以扩充参数，提升模型的拟合能力。

  


具体来讲，$$Q、K、V$$ 三个模型矩阵都是由 token embedding 矩阵做一个矩阵变换得到的。其维度也可以和 embedding 的维度不同。具体形式如下图所示。


![5-3.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e57a3d2a5784440d9f96c5f6b1cb84eb~tplv-k3u1fbpfcp-watermark.image?)

在图中，$$W_Q、W_K、W_V$$ 均为模型参数，它们都是神经网络模型扩展的可训练参数。其具体计算方法为：

- $$q_i = W_Qe_i$$

- $$k_i = W_Ke_i$$

- $$v_i = W_Ve_i$$

需要明确的是，$$Q$$ 和 $$K$$ 的维度必须是相同的，这样才能保证可执行点积运算。而 $$V$$ 的维度则可以灵活多变，图中特意以 4 维和 7 维强调这一点。但一般来讲，三者和 token embedding 的维度保持一致即可。

  


如果我们把注意力机制的输入输出当作一个黑盒，我们可以观察其输入、输出为如下形式：


![5-4.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2b571d8c091a4b08a214f5e5401d5dee~tplv-k3u1fbpfcp-watermark.image?)

经过了一层自注意力机制之后，模型的输入和输出结构是相仿的，每一个 token 和其上下文经过对比之后，又生成了一个对应的融合了上下文信息的向量表示。

  


因此，我们完全可以多多堆叠自注意力层，不断地让模型学习上下文联系。ChatGPT 也确实是这么干的，这将在第6、7节中展开讲。

  


# 注意力机制的好处

  


通过本节的介绍，我们已经非常清楚，自然语言中自注意力机制是如何将每个 token 的上下文融合起来的，这是目前深度学习模型最流行的操作。

  


正如例句中，空格处要填写的内容，位于句子的末尾，而信息相关的 `掘金` 二字则位于句首，**中间跨越了很多个 token**，寻找两者之间的关联关系，注意力机制非常擅长。技术上讲，这叫注意力机制擅长计算长文本依赖。

而在过去，神经网络里主要使用循环神经网络（RNN）模型结构来处理。RNN 的网络结构可以使用如下公式来解释：$$e_i = h(e_{i-1}, W_{RNN})$$。

其中，$$h(*)$$ 是 RNN 的计算函数，$$e_i$$ 是第 i 个 token 的网络内的 embedding 表示，$$W_{RNN}$$ 是模型参数。它表示了第 $$i$$ 个 token 必然和其相邻的 第 $$i-1$$个 token 相关联，而**无法跨 token**。在处理例句的注意力时，局限性很强。

  


另外，深度学习中模型的计算量超级大，为了让 ChatGPT 模型能够快速输出结果，就需要采用**并行计算**的方式。

> 所谓并行计算，就是一种朴素的加速思想，一个工作，一个人干需要10天，那么找10个人来，一天时间干完。在程序里，主要就是采用多核 GPU 来计算。

注意力机制相比 RNN，更加方便模型在工程上实施并行加速计算。

  


# 总结

-   注意力机制的本质是从大量信息中剔除杂质、无关信息，保留感兴趣的信息。
-   注意力机制在 NLP 领域的应用主要是 **自注意力Self-Attention** 形式，它是神经网络具备充分拟合能力的灵魂。
-   在第 1 节中，我们提到了，Transformer 是构成 ChatGPT 这座房子的砖块和钢筋，而自注意力机制则是构成 Transformer 的核心要素。下一节，我们就来介绍 Transformer 结构和原理。

## 6.ChatGPT 的组件：Transformer 模型结构

第 5 节中，我们介绍了注意力机制的工作原理，它是目前最流行的神经网络模型的灵魂机制。而相应的包裹注意力机制的实体形式，就是 Transformer 模型结构组件，我们本节重点介绍一下 Transformer。



# Transformer 结构

Transformer 结构组件主要包括如下模块。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/81ba552868fc48eeb21da5479756e179~tplv-k3u1fbpfcp-zoom-1.image)

其中，$$(x_1,x_2, ..., x_i, ..., x_n)$$就是上一节中输入的 token embedding，它是一个$$(l_{token} \times l_{embedding})$$大小的矩阵，其中，$$l_{token}$$是输入文本的 token 个数，GPT3 中这个长度是 4097，意味着输入给 GPT3 模型的最长输入文本的 token 数总共可以有 4097 个，而$$l_{embedding}$$是每一个 token 的 embedding 维度，GPT3 中这个长度是 12280。

  


而图中深蓝色的 attention 模块就是上一节中介绍的 Self-Attention，它是 Transformer 模型的核心操作。除此之外，还包括扩展参数模块 Feed-Forward 层，而在每一层的前后又包括了浅蓝色 norm 层和 dropout 层。在模型的最后，还包括输出结果层 linear 层，softmax 即用于计算交叉熵的损失函数层（第 8 节介绍）。

  


在 GPT3 模型以及此后的各种模型中，Self-Attention 使用的并非如第 5 节所介绍的原始自注意力结构，而是一种稀疏的注意力结构，又称**稀疏 Transformer（Sparse Transformer）** 。

对于以上模型结构，我们一一展开介绍。

  


## 稀疏 Transformer

### 稀疏 Transfomer 的思想基础

Sparse Transformer 的核心实际上是 Sparse Self-Attention （稀疏自注意力机制）。我们依然使用第 5 节中的例子来介绍：

请补全这条语句：**掘金**社区是一个便捷的技术交流______

在这条文本中，想要补全最终的语句，应当参考前文的信息，而前文总共 14 个字，对空格处影响最大的是`掘金`两个字，而像形容词`便捷的`，系词`是一个`都不是最关键的影响因素。换句话说，我们应当设计一种注意力机制，让模型能够在输出空格字符的时候，最大限度地注意到`掘金`两个字。

  


如果我们根据原始的 Self-Attention 计算，假设得到的注意力权重计算结果如下形式：

$$Attention = (0.371, 0.167, 0.174, 0.236, 0, 0, 0, 0.19, 0,0,0.23,0,0, 0)$$

在这个注意力权重向量中，共有 14 个元素值，加和为 1，每一项对应输入的一个字符。可以看到，`掘金社区`四个字对应的权重最高，而中间很多的字符实际上对后续填补什么字影响极其微小。其实，计算这些不重要的 token 的注意力权重完全是可以规避掉的。

  


另一方面，Self-Attention 的计算公式为：

$$Attention(Q,K,V)=softmax(\cfrac{QK^T}{\sqrt{d_q}})V$$

这是一个矩阵乘法，其中涉及矩阵乘法计算$$QK^T$$。在 ChatGPT 这样的模型中，矩阵的维度非常大，这个计算量也相当大，时间复杂度在$$o(n^2)$$，其中$$n$$是矩阵维度。如果可以避免计算某些不重要位置的注意力权重值，那么这个计算量会小很多，减少计算耗时，提高模型的计算效率。

  


**稀疏 Transformer 的本质，就是选择不计算某些 token 位置的注意力值**。

  


### 稀疏 Transformer 原理

所谓稀疏（Sparse），就是不计算某些位置 token 的注意力权重值，保留想要计算的 token 位置。那么，我们可以事先定义一组 token 的位置的索引，$$S=(S_1,S_2, ..., S_i, ..., S_n)$$，这些是我们想要计算的若干个 token 位置。例如，我们要计算第 14 个位置时，保留的索引位置包括$$S_{14}=(0,1,2,3,5,7,10,13)$$。对应了`掘、金、社、区、一、便、技、流`这几个字。

  


接下来，我们计算时就可以充分考虑待计算的索引位置，忽略不计算的索引位置。

$$K_{S_i} = (W_Kx_j)_{j \in S_i}$$

在这个公式中，其中内部 $$W_Kx_j$$表示注意力机制中 K 的转换运算，而这里表示运算过程中，仅抽取那些在索引中的位置的 embedding 进行计算。这样一来，矩阵计算量就变少了。同理，对于 $$V$$矩阵也有相同的操作：

$$V_{S_i} = (W_Vx_j)_{j \in S_i}$$

> 那么，$$Q$$ 矩阵可不可以减少呢？答案是不行的。
>
> 注意：$$Q$$代表了要为哪些位置计算注意力权重，显然，我们应当为每一个位置 token 计算权重。然而，在为每一个位置计算注意力时，需要考察哪些位置，这就是$$K$$的含义，这个位置则是可以稀疏的。


![6-1.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0c9921c0a75f450d81eae480a8bd36ad~tplv-k3u1fbpfcp-watermark.image?)
  


上图中共有 7 个 token 位置。其中，对于 $$K$$ 和 $$V$$，分别只计算了第 1 和 3 个 token 的向量，其它位置的向量被忽略。这样就减少了计算量，仅抽取了部分的位置进行关注。由此，我们就得到了计算某一位置注意力权重的公式：
$$attention(i)=softmax(\cfrac{(W_qx_i)K^{T}_{S_i}}{\sqrt{d}})V_{S_i}$$。

这个式子就是上图的公式化表示，而把所有位置全部合并起来，就形成了完整的稀疏 Attention 计算公式：
$$SparseAttention(X,S)=(attention(i))_{i \in \{1,2,...,n\}}$$。

  


在整个计算流程中，我们都在假设，已经事先确定了要保留哪些位置的索引 $$S=(S_1,S_2, ..., S_i, ..., S_n)$$，用于计算注意力权重。那么，到底如何确定要保留哪些位置呢？

  


如前图中展示，保留的第 1 和 第 3 个 token 作为索引，其它位置则被丢弃了。一般来讲有两种常用的做法，**跨步分解注意力机制（Strided Factorized Attention）** **和** **固定分解注意力机制（Fixed Factorized Attention）** 。它们的原理如下图所示。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/99aac6437d364fb3a0db136d5dbc1e28~tplv-k3u1fbpfcp-zoom-1.image)

针对这幅图，我们首先看最左边这幅图，**每次观察一行**。深蓝色小方块代表当前要计算注意力的 token 位置，灰色部分的方块是指被忽略掉的 token，采用的是 Mask 方式（第 7 节中介绍，这里我们仅需知道这些 token 被忽略掉了，只关注蓝色方块即可）。而在每一个深蓝色方块的左侧代表了它在计算注意力时需要参考的 token 位置（浅蓝色），在传统 Transformer 中，每次计算都要参考左侧的所有位置。

  


> 例：**掘金**社区是一个便捷的技术交流______
>
> 传统 Transformer 实际上当计算空格 token 时，需要把前面的每一个字符`掘金社区是一个便捷的技术交流`全部考虑进去。

  


而在中间这幅图，它代表了跨步注意力机制，根据规则，当每次计算深蓝色方块的注意力时，首先需要纳入左侧临近的若干 token，然后在距离当前 token 较远位置的 token 每隔 3 个值参考一个。

  


> 例：**掘金**社区是一个便捷的技术交流______
>
> 跨步分解注意力机制中，实际上当计算空格 token 时，只需要计算字符`社、个、技、术、交、流`，以此预测`平`字。


![6-2 (1).png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8acf26dc5f404588835ef7bc07c27589~tplv-k3u1fbpfcp-watermark.image?)

  


右侧的图中，不论计算哪一个位置，都需要观察一些固定的位置（浅蓝色）。

> 例：**掘金**社区是一个便捷的技术交流______
>
> 固定分解注意力机制中，实际上当计算空格 token 时，只需要计算字符`区、便、术、交、流`。
>
> 读者可以参照上图，自行确定这些字符的索引位置，以加深理解。

  


需要说明的是，在选择哪些位置可以稀疏计算时，选择的方式方法完全可以自定义，有非常大的灵活性。上述的方法存在局限性，尤其是固定注意力机制。不论计算哪些位置的注意力，都默认忽略掉一些位置，很容易造成计算结果的错误（如上述例子中，最关键的`掘金`被忽略了）。所以，在 Transformer 模型中，又加入了多头注意力机制，用于综合多次 Attention 计算的结果。

  


## 多头（multi-head）注意力机制

前面讲述了制作一次注意力机制的全过程，所谓多头注意力机制，就是在模型中多做几次注意力机制，以期让模型能够注意到不同的信息。


![6-3.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3dd1a1d321634528ab7887adea0ee3d8~tplv-k3u1fbpfcp-watermark.image?)

如图所示，整个过程十分简单，设计了 N 次的注意力机制，把 N 次的结果拼接起来，就完成了输出。所谓拼接方式，如图中不同色度的绿色方块所示，假设两个注意力头计算的结果分别为$$a=[a_0,a_1,...,a_m]$$，$$b=[b_0,b_1,...,b_n]$$，那么拼接后的结果就是$$concat=[a_0,a_1,...,a_m,b_0,...,b_n]$$。

  


注意，图中仅仅展示了某一位置的 token 的计算结果，实际上的输入为所有 token embedding 构成的矩阵。

  


## Normalization 正规化

  


如前 Transformer 结构图所示，在每一个 Attention 模块接入之前，都有一个 norm 模块。它是神经网络模型中常见的 Normalization 正规化模块。这里我们还是举一个例子来说明情况。

  


假设我们在输入稀疏 Self-Attention 模块之前，分别有 2 组具体的 token embedding 值， 维度都是 $$4 \times 3$$，其中每一行代表一个 token 的 embedding 值，具体值情况如图所示。


![6-4.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/81b327e2162c43eb97e0b733b7847e72~tplv-k3u1fbpfcp-watermark.image?)

我们观察上图表，可以看出，左侧表格矩阵中，所有元素的取值都介于 $$(-0.25, 0.25)$$ 之间，取值较为集中，分布较为均匀、稳定，而右侧表格矩阵中，所有元素取值都介于 $$(-3.05, 3.05)$$ 之间，取值波动较大，而且第二行中的 token embedding 分布又介于 $$(-0.2, 0.2)$$ 之间，取值波动又比较小，总体来看，分布不稳定，差异较大。

  


在神经网络模型的训练中，数据分布较为集中且均匀的话，有助于模型训练的快速推进和收敛，达到模型训练目的。相反，若数据分布分散，则模型训练困难，很容易过拟合。对于 ChatGPT 这样的大规模语言模型，尤其需要注意训练过程中数据的均匀分布，以此加速模型的稳定训练。

  


因此，在稀疏 Self-Attention 层之前，加入 norm 层，就是把分布松散的数据，整合为分布较为集中的数据。具体方法就是，假设我们待处理数据为 $$x=(x_1,x_2, ..., x_i, ..., x_n)$$，首先计算其均值和方差：

$$\mu = \cfrac{1}{n}\sum x_i$$

$$\sigma^2 = \cfrac{1}{n}\sum (x_i - \mu)^2$$

然后，针对每一个值，都做一遍正规化（Normalization），embedding 就会都局限于一个稳定的分布内，方便模型的训练。

$$\hat x_i = \cfrac{(x_i - \mu)}{\sigma}$$

由于这部分知识都是初中数学，这里就不再举例介绍。事实上，Normalization 一般分为 Batch Normalization、Layer Normalization 等。在 NLP 领域，常用的是 Layer Normalization，它就是指，针对每一条输入数据的所有数值元素做 norm 操作。

  


## Dropout 机制

在 Transformer 结构图所示，在每一个 Attention 模块接入之前，都有一个 dropout 模块。这个模块的主要功能是防止模型在训练过程中的过拟合。

  


**过拟合（** **O** **verfitting）** ，是指模型在训练数据上表现良好，但在测试数据上表现较差的现象。简单来说，过拟合就是模型过于复杂，以至于在训练数据上表现得非常好，但在新数据上的泛化能力却很差。过拟合通常是由于模型过于复杂，或者训练数据过少导致的。当模型过于复杂时，它会尝试去适应训练数据中的每一个细节，甚至是噪声，导致在新数据上的表现不佳。而当训练数据过少时，模型可能无法学习到足够的特征，也会导致过拟合。

  


> 比如，一家公司宣布自己研发的 XX 大模型能够作诗、解数学题、翻译、文摘，以及回答各种各样的问题，也晒出了一些数据截图，表明自己的 XX 模型效果优异。
>
> 开放公测后，用户试用了一下，发现 XX 模型效果非常差，经常答非所问，给出错误答案。这就是典型的模型过拟合的表现。

  


因此，一个直观的解决模型过拟合的办法就是对模型进行简化。放在 Transformer 中，就是对某些计算结果进行随机地置 0 操作。具体来讲，假设 Attention 层输出了一组 token 矩阵，模型经过一个完全随机的 dropout 之后，其结果中的某些元素就被置为 0。


![6-5.png](https://p9-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d762ea1445954889aa0696217c6b69cd~tplv-k3u1fbpfcp-watermark.image?)
  


这样，就好像模型中的一些元素特征被人为屏蔽掉了，从而模型得到了简化。

在模型中，dropout 置 0 是随机进行的，但到底要把多少元素值置 0，需要预先人为设置一个概率值 dropout ratio，当这个值是 0 时，相当于所有元素都不置 0。在上图中，大致可以计算得到，dropout 率为$$5/12=0.417$$。

  


## ResNet 残差模块

在 Transformer 结构图中，Attention 模块的输入 embed 和输出结果有一个叠加，这种叠加操作被称为残差模块。之所以这么操作，主要是为了方便模型的训练过程中，**梯度不会消失或爆炸**。其本质目的在于顺利使模型完成训练，达到目标效果。具体细节会在第 8 节模型训练过程中做介绍。

  


![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1d1e197d467b4f5291b1a0635204f1b9~tplv-k3u1fbpfcp-zoom-1.image)

  


## Linear Feed-forward 全连接层

全连接层，就像在之前内容中为 Q、K、V 添加参数，实现了一个模型参数的扩增。针对一个 token 的输入$$x_i$$，需要做一个矩阵乘法运算，公式表示其输出为$$y_i=Wx_i$$，其中 $$W$$ 就是要学习的参数。这个步骤就是 Linear Feed-forward 层，中文名又叫线性全连接层，核心即矩阵乘法运算。这一步的主要作用在于为模型增加参数，增强模型的拟合能力。

  


> **Dropout 用于控制模型参数过多，过于复杂；而这里线性全连接层又在扩增参数，是否矛盾？**
>
> 其实并不矛盾。增加线性全连接层，目标在于**提升模型拟合能力的上限**。而在这个很高的上限内，模型的训练浮动比较大，Dropout 主要用于控制模型能够顺利训练到位。

  


至此，本节介绍完了 Transformer 结构组件。把 Transformer 堆叠起来，就形成了 GPT 模型的雏形，这一部分在第7节介绍。

  


# 总结

-   Transformer 组件的核心结构就是 Self-Attention，组件的堆叠构成了 ChatGPT 的语言模型。
-   自从 GPT3 模型之后（也包括 ChatGPT），使用的是 Sparse Transformer，它有助于减轻模型的计算量，加速模型的训练和使用。
-   Transformer 中用到了 Normalization、残差计算、线性层，以增强模型的拟合能力、适应性。

## 7.ChatGPT 的结构：Encoder-Decoder 

前面第 5-6 节介绍了 Self-Attention 自注意力机制、Transformer 模型结构。这就相当于我们盖房子准备好了砖头，本节主要介绍如何把 Tranformer 模型组合起来，形成一个完整的 GPT 模型结构，而组合的方式，就用到了Encoder-Decoder 编解码架构模式。在这一节中，我们将对前述章节的内容做一个汇总，让读者对 GPT 模型从全局有一个清晰的认知。

# Encoder-Decoder 编码器-解码器架构

在第 1-2 节中，我们大致介绍了语言模型的编解码结构。如下图所示。


  
![7-1.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c6fe8d0ca47743419e74bd392eb7514e~tplv-k3u1fbpfcp-watermark.image?)


实际上，encoder-decoder 这一套模型架构最早是用于解决机器翻译问题的，感兴趣的读者可以读一下这篇经典论文【2014：[Neural Machine Translation by Jointly Learning to Align and Translate](https://scholar.google.com/scholar?q=2014+-+Neural+Machine+Translation+by+Jointly+Learning+to+Align+and+Translate&hl=en&as_sdt=0&as_vis=1&oi=scholart)】。机器翻译模型接收一条英文语句，然后经过模型的一番操作，最后输出一条对应的中文翻译结果。这种建模最早被称为 seq2seq，其含义为 sequence to sequence，即序列到序列，输入一条文字序列，输出一条文字序列。

此后，在 NLP 领域，seq2seq 可以被应用于各种各样的文字序列任务上，也就成了 NLP 领域的一种标准建模方式。


![7-2.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/24ba610307db4d8cb6b542b2ded870b5~tplv-k3u1fbpfcp-watermark.image?)

在 encoder-decoder 这种建模方式中，encoder 像极了一个人接收文字信息思考的过程，decoder 则像极了一个人将大脑中的信息转换成语言表达出来的过程。可以说，encoder-decoder 就是一种机器模拟人脑思考的方式。

  


另外，encoder-decoder 并非指一种具体的模型结构，它是一种宽泛的模型**设计架构**。

-   这种编解码结构不仅仅局限于 NLP 领域，它也可以应用于图像处理、音频处理等领域，例如图像领域的对抗生成模型 GAN 等。
-   我们知道，GPT 采用了 decoder 架构，丢弃了其中的 encoder 部分，其中的具体结构是 Transformer。但是，编解码架构的内部设计还可以采用循环神经网络（ RNN ）这种模型结构。当然，如果未来有更好的设计，也可以替换为别的具体模型结构。

  


> 如果说 GPT 是一幢房子，Transformer 是盖房子的砖头，那么，encoder-decoder 模型架构就是房子的具体结构，如门朝哪开，有几间卧室等等。除了使用 Transformer 这种砖头之外，还可以使用 RNN 等木头来搭建同样结构的房子。
>
> 当然，随着大模型的发展，实践证明，Transformer 这种结构，不论从计算性能还是适用性来说，都比 RNN 模型要强。

  


## GPT 中的编解码架构

接下来，让我们来绘制一下，Transformer 是如何嵌入 GPT 的 encoder-decoder 架构中的。如下图所示，Transformer 模型结构中省略了 norm 正规化、残差计算和 dropout 模块。


  
![7-3.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e524d3479059466183ae2b4a1d8c08a3~tplv-k3u1fbpfcp-watermark.image?)


当用户输入了`掘金社区是一个便捷的技术交流`这条语句时，模型首先结合第 4 节中介绍的 embedding 词嵌入，将文本转换为 token，进而找出对应的 token embedding 和 position embedding（本例中不需要 segment embedding），将两者相加即可进入 Transformer 结构做注意力计算。

  


Transformer 结构本身可以有很多层，每一层的输入 tensor 和输出 tensor 维度大小全部相同，前一层 Transformer 的输出就可以作为下一层的输入，像罗列方块积木一样。直到最后一层。如下图所示，这里省略了 Transformer 内部的结构，展示了三层 Transformer 结构。


![7-4.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4c16171696f249cc9edf04ffb534de49~tplv-k3u1fbpfcp-watermark.image?)

假设最后一层的 Transformer 输出了 $$M \times N$$ 维的 tensor，如上图中的紫色部分，其中$$M$$表示 token 的个数，$$N$$表示每个 token 的维度，同时假设在 BPE 算法的 token 词表中，总共有 $$K$$个 token。那么，最后一层线性层可以设计为$$M \times N \to K$$的一个函数映射，得到一个 $$K$$ 维向量，如上图中的黄色部分。一般来讲，这个向量的维度可能达到几万维到几十万维不等，**它的长度和词表中有多少 token 是相等**的。

  


对这个 $$K$$ 维向量进行**解码**，就可以得到输出的结果。我们来具体讲讲如何解码。

  


## 解码得到输出 Token

### 贪婪搜索 Greedy Search

假设，我们根据 BPE 算法（第 4 节）得到一份词表，按顺序，其中第 6 个 token 为 `平` 字，第 8 个 token 为`网`字。

| 1: 我 | 2：中 | 3：上 | 4：这 | 5：天 | 6：`平` | 7：司 | 8：`网` | ... |
| ---- | --- | --- | --- | --- | --- | --- | --- | --- |

根据第 5 节中介绍的 softmax 函数，可以对计算的 $$K$$ 维向量做一个 softmax，得到一个长达 $$K$$ 维的概率分布，假设具体数值如下所示：

| 0.0016 | 0.1120 | 0.0011 | 0.00006 | 0.0015 | `0.3410` | 0.0102 | `0.2179` | ... |
| ------ | ------ | ------ | ------- | ------ | ------ | ------ | ------ | --- |

从中可以看到，概率值最大的一个数字是 0.3410，它是这个 $$K$$ 维向量的第 6 个数字。因此，我们就可以从 token 词表中选择第 6 个 token，即 `平` 字作为模型输出的结果，整体句子就成了 `掘金社区是一个便捷的技术交流平`。这种根据向量最大值寻找对应索引的操作叫做 $$argmax$$。**按照最大概率值选择模型输出的 token**，这种方法叫做**贪婪搜索（Greedy Search）** 。

  


既然我们得到了一个维度长达 $$K$$ 维的向量，所有向量值相加为 1，那么我们可以对 token 词表进行采样。

  


> 所谓**采样**，简单理解就是掷骰子。我们都知道，一颗方形骰子有 6 个面，分别代表 1，2，3，4，5，6 几种选择。每次投掷，得到的结果是一次采样，每次的投掷结果均不同，每一种结果命中率都是六分之一。
>
> 而在 ChatGPT 模型输出结果时，也以上述采样的方式，按照每个 token 对应的命中概率值进行随机抽取，只不过，可选择范围包含了 token 词表中所有的 token。这就说明了模型输出的结果具有随机性，并非每次都相同。

  


按照贪婪搜索的方式，实际上是取消了根据 token 概率分布做采样的操作。我们知道，有 0.3410 的概率输出得到`平`字，这个结果没错；但也有 0.2179 的概率模型会输出得到`网`字，这个值也很高，若模型输出 `网`字，语言读起来完全能说得通，并不能算作错。

因此，贪婪搜索是有一定缺陷的，即人为地漏掉了一些正确的答案。

  


### 束搜索 Beam Search

为了克服贪婪搜索会漏掉一些概率值稍低的正确答案这个缺陷，可以预先选择一个范围。比如，我们把采样范围扩大，选择结果中概率值最大的两个索引位置，即 0.3410 和 0.2179，其余的概率值全部不考虑。仅从这两个概率对应的 token 中进行筛选，那选择 token 的概率分别为：

$$p(平)=\cfrac{0.3410}{0.3410 + 0.2179}=0.61$$

$$p(网)=\cfrac{0.2179}{0.3410 + 0.2179}=0.39$$

由此，我们可以从这套概率分布中二选一，0.61 的概率可以抽取出 `平` token，0.39 的概率可以抽取出 `网` token。

  


然后，我们可以继续迭代，让模型输出`平台`二字，或`网站`二字，这两种答案都是正确的。这种先选择最高概率的若干选项，再在其中随机抽取的方式叫做**束搜索（Beam Search）** ，所谓 Beam 就是指一束光中，光线不止一条，而是有多条，对应在解码中，就是指有多个选择，它可以克服贪婪搜索选择范围仅仅只有 1 个 token 的缺点。

  


要选择多少个备选 token 进行采样，需要预先人为设定一个数量 $$k$$。可以看出，若此值越小，生成的结果越固定，反之结果越灵活多变。

  


### 核搜索 Nucleus Search

在 Beam Search 中，选择多少个可选 token 也有一定的策略，例如，上面我们设定了只选择概率值最大的 2 个值。或者换个思路，我们可以设定，把模型输出的 $$K$$ 维概率分布值按从大到小的方式排列，若前 T 个概率值加起来大于 top_p（介于 0~1 之间的值），则以这 T 个值作为最终的抽取范围。

  


以上面为例，假设预先设定$$top\_p=0.6$$，而分布中，概率值最大的三个相加可得：
$$0.3410 + 0.2179 + 0.1120=0.6709 > 0.6$$。

因此，我们就从第 2，6，8 三个 token 中，按照概率进行 token 抽取。

这种输出方式被称为 **top_p 搜索，也叫核搜索（Nucleus Search）** ，是 Beam Search 的一种变体。当预先设定的 $$top\_p$$ 值变大时，可用于选择的 token 数就变多，模型生成的结果就更加多变，不可靠；当 $$top\_p$$ 变小时，可选择的 token 数就变少，模型生成的结果就更加固定；当 $$top\_p$$ 为 0 时，核搜索退化为 Greedy Search。

  


在这个例子中，`平` token 的概率值是 0.3410，`网`token的概率值是 0.2179，剩下的 token 占据了很大的概率值，但就不再是正确答案了。这实际上也存在一定的缺陷，即正确目标的概率值尽管是最大的，但是数值本身仍然偏小。

  


### 温控搜索 Temperature Search

面对上面的问题，我们得设计一种思路，能够在保证各个 token 的概率值相对大小排序不变的情况下，调整其概率值。例如，`平` token 的概率值比`网`大，这个大小关系是确定的，但希望`平` 的概率值变得更大一些。

  


回忆一下第 5 节中对 softmax 函数的介绍，在上图中，softmax 函数的输入是一个 logits，输出是对应的每个 token 位置的概率值。这里的 logits 就可以理解为对数化的概率值，设 $$u$$ 是 logits 向量，$$p$$ 是对应的概率值，那么两者之间的关系为：$$p_i=\cfrac{exp(u_i)}{\sum_j exp(u_j)}$$。

这就是 softmax 公式。所谓**温控搜索（Temperature Search）** ，就是对上述公式做一个调整，加入一个参数 $$T \in (0,1)$$，得到：$$p_i=\cfrac{exp(\cfrac{u_i}{T})}{\sum_j exp(\cfrac{u_j}{T})}$$。

以`平`字为例，当$$T=1$$时，其对应的概率值是 0.3410，而当 T 值逐渐变小后，其对应的概率值会逐渐变大，模型对`平`字抽取的概率也就变大，模型的输出结果也就更加固定。相反，T 值逐渐趋近于 1 时，模型搜索结果也就变得更加灵活多变，不可控制。

  


> 这种现象非常像物理热力学中的现象，当温度升高后，分子热运动加剧，运动变得无规则不可控；而当温度降低后，分子热运动减弱，从宏观上看，运动趋于稳定。

  


ChatGPT 的解码方法，就是温控搜索和核搜索的一种结合体。一方面，从全局 token 中选择一个范围（核搜索），另一方面，条件这个范围内的温控参数（温控参数），使得采样不同 token 的值出现变化。如果读者尝试过调用 openai 的接口，就会看到这个参数选项值。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3b9b3bbd2f274ab6b066ae6e38d5e11a~tplv-k3u1fbpfcp-zoom-1.image)

最后，需要提一下，模型解码是个循环过程，即根据前一个 token，输出后一个 token，反复可以迭代进行。那么，这个循环什么时候被打破？换句话说，模型什么时候输出完呢？

其实，在 token 词表中，单独设置了一个特殊符号 token`<eos>`，意指 end of speech，当模型预测输出了这个 token，那么循环就终端了，ChatGPT 也就会认为，整个要输出的答案完成了。

  


## GPT 是一个解码器

从上面的整个计算流程中我们可以看到，模型首先接收一串输入数据，经过 Transformer，这属于 encoder 部分，末尾模型输出预测的 token 字符，这属于 decoder 部分。整个流程和本节第一个流程示意图中略有差别。主要差别在于，前述介绍的编解码架构中，编码器和解码器相互独立，中间有一个单独的信号相连。而在 GPT 模型中，实际上编解码融为一体。


![7-5.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c3d5792cd85f4d7095c6861fabec0e7b~tplv-k3u1fbpfcp-watermark.image?)


下图是 Transformer 论文中【2017 - [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)】的最原始的编解码模型架构，其中，左半边为编码器，右半边为解码器。而前述的 GPT 模型架构，其实就是原始结构的右半边。因此，我们也常说，GPT 系列模型仅仅使用了 decoder 部分。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5542a69ffb454e88ae758132e0c9cf26~tplv-k3u1fbpfcp-zoom-1.image)

  


  


另外，读者可以对比上图最原始的 Transformer 的内部结构和第 6 节中 GPT 结构的异同。如果仍不清楚，可以返回上一节再阅读一下。提示一下，主要包括稀疏操作、norm 操作等等。

  


# Mask 掩码层

在实际的模型计算过程中，读者可能已经使用过 ChatGPT，模型输入和输出的句子有长有短，长则几千个字，短的不足 10 个字，始终处于变动。而 ChatGPT 模型结构则相对固定，会预先设置允许一个模型接受的固定最大 token 序列长度。假设模型可以接收的最大 token 数量为 20 个，而用户输入的数据只有 `掘金社区是一个便捷的技术交流`这 14 个 token 时，模型会对这部分数据做一定处理，如下图所示。

![7-6.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3c0bb61414204fe7878aa278ff6e090d~tplv-k3u1fbpfcp-watermark.image?)
  


图中展示了模型输入 token embedding 的整体结构，它规定了模型的输入最大 token 数为 20。当用户的输入低于 20 个 token 时，模型会自动为用户的输入补齐，添加一个特殊的`<pad>` token（图中以`/p`表示，它和`<eos>`token 性质是相同的，并不是表示具体的某个字符，而是仅仅起到一种功能上的作用），它本身不具有实际意义，仅代表占位符；当用户的输入超过 20 个 token 时，模型又会将超出的部分截断，仅保留前 20 个 token，形成一个不完整的输入。

  


模型在进行后续的 Transformer 操作时，需要考虑`<pad>` token 本身不代表任何含义，这些字符不应该参与上下文的相关性自注意力计算。为了解决这个问题，**Mask 掩码层**被提出。

  


> Mask 广义来讲是一块蒙版，指覆盖在图像上，遮挡部分景物。
>
> 如下图中，用一块白色的蒙版，遮住图片中的部分内容，就只能看见猫咪的一只眼睛。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ef52b8aea0024887b75ab7452ef8bd4e~tplv-k3u1fbpfcp-zoom-1.image)

Mask 概念同样可以运用在 Transformer 中。根据第 5 节的介绍，自注意力机制主要采用$$Q, K, V$$三种向量进行计算。如果考虑 Mask 蒙版，假设 ChatGPT 模型仅输入了`掘金`两个 token 字符，预测接下来应该填写的字符，则它将变为如下形式：


![7-7.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6e68791ee1094f4180d69a0cafa67062~tplv-k3u1fbpfcp-watermark.image?)

在上图中，mask 层为每一个 token 位置设置了一个 1，0 取值，若设为 1，则对应位置 token 正常进行计算，而设为 0 时，对应位置不参与注意力计算。最终的 softmax 概率值为 0。

  


根据第 6 节中的介绍，ChatGPT 采用了稀疏自注意力机制，根据跨步分解（Strided Factorized）和固定分解（Fixed Factorized）两种方式屏蔽掉某些 token 位置的注意力计算，采取的也是 Mask 的方式。

  


# 总结

-   ChatGPT 模型基于 encoder-decoder 模型架构进行建模。
-   ChatGPT 模型采用**核搜索、温控搜索**结合的方式生成输出结果，并基于 temperature 调节生成结果的随机性，值越大，随机性越强，值越小，生成的内容越固定。
-   ChatGPT 主要采用 **Mask 掩码**的方式，屏蔽掉不参与注意力计算的 token 位置。

## 8.模型训练基础：监督学习与 ChatGPT 预训练

ChatGPT 模型是一个规模庞大的神经网络模型，第 3-7 节我们重点介绍了 ChatGPT 模型的详细**结构**。本节到第 11 节，我们将重点介绍如何利用数据**训练**一个 ChatGPT 模型。

  


ChatGPT 模型的训练过程主要包括**语言模型的预训练**，根据用户数据**微调（Finetune）** ，使用**强化学习方法提升模型知识涌现能力**。这几部分的本质都还是**利用随机梯度下降法，使用数据进行有监督训练**。

  


因此，考虑到非 AI 领域读者在阅读上会有困难，本节将介绍一下神经网络的训练流程和基础概念，举一个最简单的例子，帮助你充分理解模型训练的过程。

-   如果你已经具备了 AI 领域的相关知识，则可以跳过本节，继续阅读后续章节；
-   如果你还不具备模型训练的基础知识，则有两种学习方法，第一种是先通读本节内容，然后阅读后续章节，第二种是先跳过本节内容，当阅读到后续章节相关内容时，再返回到本节的基础知识做查阅。

  


# 神经网络的训练过程

目前，神经网络最常用的模型训练方法为**监督学习（Supervised Learning）** 。监督学习的目标，是通过给定的输入（ChatGPT 的输入文本）和输出（ChatGPT 的输出文本）数据来学习一个函数，使得对于新的输入数据，可以预测其对应的输出。在监督学习中，我们通常将输入数据称为特征，将输出数据称为标签或目标变量。

这样说可能还是有些抽象，我们可以打个比方。

  


> 简单来讲，监督学习就像是一位老师在教学生做题一样。老师会给学生一些已知的**问题（输入数据）** 和**答案（输出数据）** ，让学生通过观察这些问题和答案的关系，学会如何解决新的问题 **（模型拟合）** ，如考试题等等。

  


具体的监督学习流程如下图所示，蓝色部分为对应模块所使用的常用具体方法。整个模型训练过程，就是准备标注数据，根据模型推理的结果和数据标注结果，使用交叉熵损失函数对比两者的差异，使用梯度下降法更新模型的参数，以此达到模型训练的目标。


![8-1.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1007535d8d364bb3ab3c1a691550e106~tplv-k3u1fbpfcp-watermark.image?)

只讲概念还是太抽象了，我们来举一个例子，制作一个最简单的神经网络完成对猫狗特征的分类。

  


## 神经网络的输入和输出

在使用监督学习模型训练模型的过程中，必然要依赖已经标注好的数据，这些数据用于喂给模型作为输入，并拿标注好的输出和模型计算得到的输出做对比，以此训练模型。

  


现在假设我们有两个样本，分别标注好对应的特征和类别：

> 样本 1：体长：0.5 米，身高：0.3 米，食量：0.03 kg；类别：猫
>
> 样本 2：体长：1.2 米，身高：0.6 米，食量：0.2 kg；类别：狗

  


直观地看，一个动物，如果体型小于 0.5 米，身高较矮，食量少，那大概率是猫咪，而如果体型高大，食量大，大概率是一只狗。

  


根据这些样本，我们构造一个最简单的神经网络（单层前馈全连接线性层），其本质就是一个矩阵乘法运算，对数据做了一次线性变换，其结构如下：



![8-2.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/905faa50f84e49baae0f23206687027e~tplv-k3u1fbpfcp-watermark.image?)
其中的每一个绿色节点代表一个特征，一个蓝色节点代表一个类别结果，图中的数字代表该节点的编号牌，每一条连接线都是这个微型神经网络的**权重参数**，这些权重参数就是模型需要通过样本学习和拟合的。

模型的含义是，针对每一个样本，分别计算其属于猫和狗的值，比较得到的两个值更加偏向哪个类别。

  


假设这些权重参数分别为：$$w_{00}, w_{01}, w_{10}, w_{11}, w_{20}, w_{21}$$，其中参数的下标即节点的编号。那么，当我们计算样本 1 的结果时，就可以得到：

- 样本 1 的猫类别值：$$class_0 = 0.5w_{00} + 0.3w_{10} + 0.03w_{20}$$

- 样本 1 的狗类别值：$$class_1 = 0.5w_{01} + 0.3w_{11} + 0.03w_{21}$$

  


你可以自己尝试写一下，针对样本 2 的计算式子。

通过比较 $$class_0, class_1$$ 这两个结果值的大小，如果猫类别值比较大，说明样本属于猫，反之则属于狗。

  


## 神经网络的预测推理



在模型开始训练之前，首先需要随机初始化一套参数值，用于模型的**推理**（也叫模型的**预测**）。在 ChatGPT 中，用户每次调用 ChatGPT 回答一次问题，技术上都叫做一次模型的推理或预测。

  


针对猫狗分类的例子，假设我们有一套参数（可以用于模型训练调整的一套值）：

-   $$ w_{00}=0.5, w_{10}=-1.0, w_{20}=0.8$$
-   $$w_{01}=-0.2, w_{11}=0.6, w_{21}=-0.5$$

  


由此我们可以分别计算：

- 样本 1 的猫类别值：$$class_{10} = 0.5 \times 0.5 - 0.3 \times 1.0 + 0.03 \times 0.8 = -0.026$$

- 样本 1 的狗类别值：$$class_{11} = -0.5 \times 0.2 + 0.3 \times 0.6 - 0.03 \times 0.5 = 0.065$$

- 样本 2 的猫类别值：$$class_{20} = 1.2 \times 0.5 - 0.6 \times 1.0 + 0.2 \times 0.8 = 0.16$$

- 样本 2 的狗类别值：$$class_{21} = -1.2 \times 0.2 + 0.6 \times 0.6 - 0.2 \times 0.5 = 0.02$$

  


那么，怎么判断这些样本经过模型的预测属于哪个类别呢？

所有样本的猫狗类别值，实际上都是 log 化的概率值，通过 softmax 公式，我们可以将上述的取值，转换为真实的概率值。在第 5 节中，我们介绍过 softmax 函数。应用在上述例子，我们可以计算得到：每个样本属于各个类别的概率值：

- $$p(猫|样本1) = \cfrac{exp(-0.026)}{exp(-0.026) + exp(0.065)} = \cfrac{0.974}{0.974 + 1.067}=0.47$$

- $$p(狗|样本1) = \cfrac{exp(0.065)}{exp(-0.026) + exp(0.065)} = \cfrac{1.067}{0.974 + 1.067}=0.53$$

- $$p(猫|样本2) = \cfrac{exp(0.16)}{exp(0.16) + exp(0.02)} = \cfrac{1.173}{1.173 + 1.02}=0.54$$

- $$p(狗|样本2) = \cfrac{exp(0.02)}{exp(0.16) + exp(0.02)} = \cfrac{1.02}{1.173 + 1.02}=0.46$$

  


由此可以判断，从模型预测可以得知，样本 1 更大概率是狗，样本 2 更大概率是猫。这个结果与先前给出的真实标注数据不相符，因此需要计算两者之间不相符的差距。

  


## 神经网络的损失函数

**损失函数**是机器学习中一个重要概念，用于衡量**模型预测结果**与**真实结果**之间的差距，在这个例子中，上述模型参数预测得到的类别和真实的标注类别不一致，这就需要损失函数来衡量。

而模型的训练，就是通过优化损失函数来更新模型的参数，使得模型的预测结果更加准确。常见的损失函数是**交叉熵** **（Cross** **Entropy** **）** ，我们也以该损失函数为例，对猫狗分类的例子做介绍。

  


这里就用到了**交叉熵损失函数**。其公式形式为：

$$J(w) = -\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{k}y_j^{(i)}\log(h_{w}(x^{(i)})_j)$$

-   其中，$$m$$表示样本数量，例子中总共有 2 个样本；
-   $$k$$表示类别数量，在这个例子中$$k$$值为 2，即猫和狗两种类别；
-   $$y_j^{(i)}$$表示第 $$i$$个样本的真实标签是否属于第 $$j$$个类别，是则为 1，否则为 0；
-   $$h_{w}(x^{(i)})_j$$表示模型对第 $$i$$个样本属于第 $$j$$个类别的预测概率。

  


这个公式看起来的确有点抽象，我们来计算一下上面的猫狗分类例子，加深理解。

对于样本 1，$$J_1(w) = -1.0 \times \log(0.47) + 0 \times \log(0.53)=0.75 $$

对于样本 2，$$J_2(w) = 0.0 \times \log(0.54) - 1.0 \times \log(0.46)=0.776 $$

因此，总和损失值为：$$J(w) = \cfrac{J_1(w)+J_2(w)}{2}=0.763$$

  


> 交叉熵是一个**信息论**中的概念，它衡量了两个概率分布之间的差异（距离），这个值一定是大于 0 的正实数。若两个概率分布十分接近，则交叉熵值越小，越逼近于 0，反之则越大。而模型预测的结果和标签标注的结果，本身就是两个概率分布。
>
> 关于信息论的基础知识，你可以参考[【信息熵、交叉熵、相对熵】](https://mp.weixin.qq.com/s/YP3SixzbgWPpvx-1xzeodQ)一文阅读学习，其中详细介绍了信息论如何应用在监督学习中。

  


## 梯度下降法

  


从上面的例子中我们了解到，若随机设定的模型权重，会导致模型对样本的类别判断产生错误。这个错误值的大小即交叉熵损失函数值。

那么，针对上述例子，接下来就是让模型能够根据给出的两条样本数据训练模型、更新参数权重，正确地分出每一个样本是猫还是狗。此时，我们就用到了**梯度下降法**。

  


### 定义

梯度下降法是一种常用的优化算法，用于求解函数的最小值。在机器学习中，我们通常使用梯度下降法来更新模型的参数，使得损失函数最小化。 梯度下降法的基本思想是沿着函数的负梯度方向不断迭代，直到达到最小值。

具体来说，我们首先随机初始化模型的参数，然后计算损失函数$$J(w)$$对于每个参数的**偏导数**，即**梯度**$$\nabla J(w)$$。接着，我们沿着梯度的反方向更新参数，使得损失函数逐渐减小。这个过程可以通过以下公式表示：$$w=w-\alpha \times \nabla J(w)$$。

  


公式比较抽象，我们仍以上述例子阐述一下怎么进行梯度下降。

  


### 梯度下降法训练实施过程

对于样本 1，其真实的类别应该是猫。然而，模型以 0.47 的概率认为它是猫，以 0.53 的概率认为它是狗，模型更偏向该样本被分类为狗。

  


因此，我们的调整目标，是希望模型对于输出猫的概率$$p(猫|样本1)$$更大，对于狗的输出概率$$p(狗|样本1)$$更小。而 $$p(猫|样本1)$$ 和 $$p(狗|样本1)$$ 概率值，是由 $$class_{10}$$ 和 $$class_{11}$$ 值决定的，当 $$class_{10}$$ 值越大时，$$p(猫|样本1)$$ 概率值也就越大，相应的 $$p(狗|样本1)$$ 概率值也就越小。因此，我们应当使得 $$class_{10}$$ 值尽量大。

  


而对于样本 1，当 $$w_{00}$$ 权重参数值越大时，$$class_{10}$$ 值也就越大。因此，我们应当使  $$w_{00}$$ 值尽量大。其中，当我们**把** $$w_{00}=0.5$$ **权重值调大**，例如，改为 $$w_{00}=2$$ 之后，我们再计算 $$class_{10} = 0.5 \times 2 - 0.3 \times 1.0 + 0.03 \times 0.8 = 0.724$$。

而 $$class_{11} = -0.5 \times 0.2 + 0.3 \times 0.6 - 0.03 \times 0.5 = 0.065$$
保持不变。此时，再计算概率值：

$$p(猫|样本1) = \cfrac{exp(0.724)}{exp(0.724) + exp(0.065)} = \cfrac{2.062}{2.062 + 1.067}=0.659$$

$$p(狗|样本1) = \cfrac{exp(0.065)}{exp(0.724) + exp(0.065)} = \cfrac{1.067}{2.062 + 1.067}=0.341$$

  


至此，我们通过调整 $$w_{00}$$ 的值的大小，实现了模型预测样本 1 更加偏向类别为猫，概率为 65.9%，相应的分类为狗的概率是 34.1%。根据调整过的模型参数，重新计算损失函数值$$J(w) = 0.506$$（过程略去，读者可以自行尝试），损失函数的下降，表明模型的训练朝着正确的方向前进。

按此方式，我们可以把样本 2 以同样的方式进行参数迭代，除了$$w_{00}$$之外，$$w_{01}, w_{10}, w_{11}, w_{20}, w_{21}$$所有参数都可以按此方式进行迭代训练。循环往复，就可以通过给定的数据，得到一个优质的模型。

  


这样一来，我们就**通过调整模型参数权重，实现了模型的训练任务**。实际上，上述整个过程就是在利用梯度下降法实现模型的训练。当我们由 $$p(猫|样本1) \to class_{10} \to w_{00}$$ 一步步推导出每一个变量的变化时，就是在用链式求导法则做偏导数计算。

  


实际上，模型的训练过程就是一个损失函数值不断下降的过程，这个过程我们在第 5 节中简要提到过。如果把损失值比作一座山的海拔高度，那么模型训练过程就是从一座高山的山峰逐渐下来。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1a2081369b07482799cf3489a32577f2~tplv-k3u1fbpfcp-zoom-1.image)

图中的每一个黑色十字都代表一次迭代，正如我们上述例子中$$w_{00}:0.5 \to 2$$。经过层层迭代，模型的损失函数值下降到一个极小点，这个极小点就意味着完成了训练。而图中的两条黑色线路，代表了模型参数的下降方向是随机的，这个随机性由样本和最初设定的$$w$$参数共同决定。

  


# ChatGPT 的预训练过程

了解了上述模型的训练过程，我们就可以解释 ChatGPT 模型的训练过程了。第 1 节中我们初步了解到，ChatGPT 是经过两个训练步骤得到的。


![8-3.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bf41ef07351846c880246a024abe9e6f~tplv-k3u1fbpfcp-watermark.image?)

  


在第 4 节中，我们介绍了 ChatGPT 模型以 token 序列的 embedding 作为输入传入模型。


![8-4.png](https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/eb080dc16a794b0a96d0e4434ca23842~tplv-k3u1fbpfcp-watermark.image?)

  


ChatGPT 模型的输出和前面提到的猫狗分类也非常相似。在猫狗分类中，总共对每一条样本预测分别属于猫、或狗的概率，即$$p(猫|样本)$$和$$p(狗|样本)$$。而在 ChatGPT 预训练中，以下面为例：

> 例句：jionlp 是一个好用的开源_____


那么，根据 ChatGPT 建模形式，模型的输入是 `jionlp 是一个好用的开源`，需要让模型输出下一个字是什么。

  
假设我们的 token 词表总共有 50000 个。ChatGPT 模型输出了一个 50000 维度的向量，$$d_{50000}=[0.123, -1.092, ... , 0.037]$$，其中每一个维度值都对应了一个 token，假设第 3333 维度对应了词表 token `软`，而这一维度值是最大的，我们就可以将此 token 抽取出来，作为结果放置在例句后面。反复执行这一操作，就可以填补完整句子：

> 例句：jionlp 是一个好用的开源 软件

  


以概率公式表示，即 $$p(软|jionlp是一个好用的开源)$$ 和 $$p(件|jionlp是一个好用的开源软)$$。

除此之外，ChatGPT 的训练流程和上述的猫狗分类没有本质差别。

# 总结

-   监督学习是根据数据，对模型参数进行拟合，在神经网络模型中非常常用。
-   监督学习最常用的损失函数是交叉熵。
-   监督学习采用梯度下降法进行模型的参数训练。
-   ChatGPT 的预训练就是一个监督学习过程。

## 9.模型训练基础：GPT 中的 few-shot 小样本学习

我们在之前的章节中介绍了 ChatGPT 的模型建模、模型结构、工作机制。除此之外，恰当的模型训练方式对其最终取得的效果也至关重要。

NLP 领域模型训练策略的改变总共经历了四个阶段，这也是 GPT 模型的训练方式进化史。


![9-1.png](https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/40d6ab4a89114c6986302f57bf6ba2a5~tplv-k3u1fbpfcp-watermark.image?)

ChatGPT 的模型训练方式依然汲取了大规模语言模型 （LM） 预训练，以及小样本学习的思想。因此，本节我们先来重点梳理一下前三个阶段。

  


# 纯监督学习

这种方式是最早期的 NLP 模型训练方式，也是最传统的机器学习建模方式。这种模型训练方式主要还是针对特定的 NLP 任务来完成的，诸如文本分类、实体识别、文本摘要抽取、机器翻译等。

  


> 为了说明监督学习的数据组织模式，我们准备了 3 条标注样例如下：
>
> -   文本分类：
>
> 文本：jionlp 开发工具包确实挺好用的，非常感谢博主的分享~~~~
>
> 类别：正面
>
> -   实体识别：就是指从自然语言文本中，抽取出如公司名、人名、药物名称、时间、地名等具有特定意义的词汇或短语，这些内容有特定的用途。一般需要标定实体的文本、类型和在文本中的位置。
>
> 文本：派飞公司的业务这两年越做越大，快要上市了。
>
> 标记：['text': '派飞', 'type': 'company', 'offset': [0, 2]]
>
> -   机器翻译：
>
> 文本：今天中午到明天早上，我市将有中到大雨，请市民出行做好防雨准备。
>
> 译文：From noon today to tomorrow morning, there will be moderate to heavy rain in our city. Citizens are advised to prepare for rain when going out.

除此之外，NLP 领域还包括分词、语义关系抽取、实体消歧等等特定领域的任务，每个任务都可以按上述方式构建标注数据集，其一般形式为：【文本，标注信息】。

  


纯监督学习的模型训练方式，就是第 8 节中我们所介绍的内容，这种训练方式存在很多难以克服的缺点：

-   **极度依赖标注数据集**：AI 领域有一句广为流传的真理——有多少人工，就有多少智能。其含义就是指，模型通过学习标注数据获得智能，而标注数据全靠人力一点点完成。而人工筹措如此多的数据集是极为耗时耗力的，也严重制约了 AI 技术的发展。即便是 OpenAI，也雇佣了大量的全世界各地区的廉价劳动力做数据标注，但人力的效率远远满足不了模型规模的扩张。

-   **模型只针对特定任务**：早期的 AI 模型都只针对特定任务，而不存在一个模型完成多种任务的方式存在。这导致了模型只局限于一个非常小的规模，一种模型只能完成一种任务，局限性太强。

-   **模型泛化** **能力** **差**。

所谓模型的**泛化能力**，就是模型在处理未曾见过的数据时的表现能力。在机器学习中，我们通常会将数据集分为训练集和测试集，模型在训练集上进行训练，然后在测试集上进行测试，以评估模型的性能。训练集就是模型见过的数据，而测试集则是模型未见过的。如果模型在训练集上表现良好，但在测试集上表现不佳，那就说明模型的泛化能力不足。

  


例如，我们教会了模型计算“3+2=5”、“5+8=13”、但是如果向模型提问“9+7=？”，模型却不能正确回答，就说明模型泛化能力差，模型不具备**举一反三**的能力。

  


# 预训练+微调（Finetune）

预训练+ 微调（finetune）是一种在 NLP 领域中广泛使用的技术。它的基本思想是，使用大量的未标记数据进行预训练，然后使用少量的标注数据进行微调（finetune），从而适应特定的 NLP 任务。直到 ChatGPT 出现之前，这是绝大多数 NLP 模型任务的基本工作流程。

  


模型预训练就是构造一个语言模型，利用互联网上大规模的未经人工标注的文本进行上下文联系学习。我们在第 3、4、5、6 节中介绍了 ChatGPT 的语言模型结构，这整个模型就是在完成预训练工作。

  


而在预训练阶段完成之后，GPT 初代就采用微调（finetune）的方式对很多下游任务做监督学习。所谓微调，本质和纯监督学习的操作过程完全一样，唯一的区别在于模型以梯度下降法训练的过程中，参数调整的幅度和范围比较微小。

-   纯监督学习的方式，是指模型在一个**完全随机初始化的参数**基础上进行训练；例如，在第 8 节中，猫狗分类模型的所有权重值是我一开始随机想出来的，随机设定的。
-   预训练+finetune 的方式，是模型在一个**已经具备一定语言知识和语言能力（经过了预训练）的参数**基础上进行训练。即在微调前，我们已经得到了一个预先训练好的参数集。

  


这种方法相比纯监督学习有一定好处：同样是训练一个针对特定任务的 NLP 模型，有预训练的模型提前已经具备了一定的语言基础，学习特定的任务速度更快，效果更好，同时需要的标注数据量级相对较少，模型泛化效果好。

  


> 若把训练 AI 模型比作教育小孩的话，假设现在有一个 1 岁还不会说话的幼儿，以及一个 5 岁孩子，两个人都从来没接触过加法算术。我们教 5 岁孩子算加法肯定要比教一岁幼儿更容易。原因在于，即便 5 岁孩子从来没有接触过数学，但是他已经在过去 5 年的成长中听到、看到了大量的信息，这些信息有助于他理解数学。
>
> 在这个例子中，学算术类似于学习某个特定 NLP 任务，小孩成长过程中接收到大量的信息，类比于 NLP 模型的预训练。教 5 岁孩子学算术，就类似于**预训练+fintune**，而教 1 岁幼儿算术，则是原始的纯监督学习。

  


# **In-context learning**

## In-context learning 原理

为了克服模型训练需要依赖大量数据的局限性，GPT-3 提出了一种叫做 in-context 的学习方式。直译过来，就是“在上下文中学习”，用中文来解释的话，就是让 GPT 模型在上下文中学习要学的任务和内容，下面举一个例子进行解释。我们知道，GPT 系列模型的建模方式如下：


![9-2.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ddebf866a006421c9e573fbb29b541a4~tplv-k3u1fbpfcp-watermark.image?)

  


> 用户输入到 GPT-3：**你觉得 JioNLP 是个好用的工具吗？**
>
> GPT-3输出1：**我觉得很好啊。**
>
> GPT-3输出2：**JioNLP是什么东西？**
>
> GPT-3输出3：**你饿不饿，我给你做碗面吃……**
>
> GPT-3输出4：**Do you think jionlp is a good tool?**

  


按理来讲，针对**机器翻译**任务，我们希望模型输出第 4 句，而针对**对话**任务，我们希望模型能够输出第 1、2 句中的任何一句。另外，第 3 句这个输出句子前言不搭后语，不是恰当的输出语句。

根据第 3 节语言模型的介绍，ChatGPT 模型的输入实际上就是一段 context 上下文。这时就有了 in-context 学习，也就是我们对模型进行引导（或提示，prompt），教会它应当输出什么内容。如果我们希望它输出翻译内容，那么，应该给模型如下输入：

> 用户输入到 GPT-3：**请把以下中文翻译成英文：** 你觉得 JioNLP 是个好用的工具吗？

如果想让模型回答问题：

> 用户输入到 GPT-3：**模型模型你给我说说，** 你觉得 JioNLP 是个好用的工具吗？

我们将上述的模式进行一般化建模：$$p(output|input, task)$$。

其中，task 就是在原输入基础上，补充指定模型完成的任务。即在对语言模型建模时，模型的输入既要指明文本输入内容，又要指明让模型完成什么任务。OK，这样模型就可以根据用户提示的情境做针对性的回答了。

  


## In-context learning 的训练

In-context learning 的模型训练方式与之前所属的语言模型预训练流程完全一致，但其中已经构建了大量的各种各样的任务，如下图所示。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9aa23df9459249069ea8adfc93c70334~tplv-k3u1fbpfcp-zoom-1.image)

整个学习过程依然是基于随机梯度下降（SGD）方法的预训练，但是在数据组织上添加了数学计算，把乱序的单词恢复原序（unscrambling words）、英文到法文翻译。这样，模型就学会了处理多种任务，即 in-context learning。

  


## zero-shot、one-shot、few-shot 小样本学习

我们在模型预训练阶段，是不可能穷尽所有学习任务 task 的。因此，对于 ChatGPT 的预训练模型来说，一定存在大量模型没见过的学习任务。

  


> 例如，模型中可能缺少如下任务：
>
> -   她对于他的求爱，感到十分感动，但是拒绝了他。=> 十动然拒
> -   他完全不知道发生了什么事情，但是觉得很厉害。=> 不明觉厉
> -   人生已经如此艰难，有些事情就不要拆穿了。=> ?
>
> 对于人类的大脑来说，可能从来没见过这种语言任务，但是通过上述两个例子就能明白，这需要把一句话凝聚成一个类似成语的四字短语。

  


对于语言模型来说，它也需要能够根据若干例子，明白用户想要它做的任务，尽管这种任务在预训练数据集里从来没出现过。在 in-context learning 过程中，这里只是告知了模型如何做，最好也能够给模型做个**示范**：

> 用户输入到 GPT-3：
> -     请把以下中文翻译成英文：
> -     苹果 => apple;
> -     掘金还挺不错的 => Juejin is rather good；
> -     你觉得 JioNLP 是个好用的工具吗？=>

  


其中苹果翻译成 apple，是一个示范样例，用于让模型感知该输出什么。只给提示，不给示范叫做 zero-shot，给一个范例叫做 one-shot，给多个范例叫做 few-shot。这很符合人们的直觉。

  


> 例如，语文老师给全班布置了一篇作文，我们第一时间拿到作文题后的反应是什么？一定是找范文，而且要参考不止一篇范文。这就是 few-shot 的含义。

  


GPT3 中的例子如下图所示，其中展示了输入文本教模型学习法语的例子。

  


![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/50c786ec37784804aa75cdad053f0d9d~tplv-k3u1fbpfcp-zoom-1.image)

范例给几个就行了，不能再给多了！一方面，如前所述，获取标注数据代价很大，一般来说不可能有那么多针对特定任务的标注数据；另一方面，给多了实际上就又成上图中右侧的 finetune 模式了，我们不能回到监督学习的老路上去。

  


## prompt 学习

在 NLP 学术界，还有一个 prompt 研究方向，它使用人类编写的简短文本片段（prompt）来指导模型生成文本。如果说 few-shot 方法是**亲身示范**，那么 prompt 就是**口头命令**。

举一个例子，你一看就明白：

> 用户输入到 GPT-3（**有 prompt** 的情况）：
> -     **请把以下中文翻译成英文：**
> -     苹果 => apple;
> -     掘金还挺不错的 => Juejin is rather good；
> -     你觉得 JioNLP 是个好用的工具吗？=>
>
> 用户输入到 GPT-3（无 **prompt** 的情况）：
> -     苹果 => apple;
> -     掘金还挺不错的 => Juejin is rather good；
> -     你觉得 JioNLP 是个好用的工具吗？=>

  


实际上，prompt 就是对任务做自然语言描述，不加 prompt 就是直接给范例。从本质上看，**in-context learning 和 prompt 完全就是一回事**。只不过描述的语境，具体的操作稍有不同。

  


## In-context learning 的效果评价

这种引导学习的方式，在小模型上效果还是十分捉急的，但它却在超大模型上展示了惊人的效果：只需要给出一个或几个示范样例，模型就能照猫画虎地给出正确答案。注意：必须是**超大模型**（即参数数量非常多）才可以。

  


![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6c3dd8ed058c43f7b9fe77f97bd81439~tplv-k3u1fbpfcp-zoom-1.image)

在上图中：

-   few-shot 的效果要好于 zero-shot，毕竟模型有学习的参考对象（就像有了范文，写作文质量会好一些）。
-   有 prompt 要比没有 prompt 效果好，毕竟模型知道了要学习的任务本身，这都符合人的直觉。
-   蓝色的曲线代表 1750 亿参数的模型效果，而在 13 亿参数的绿色曲线上，模型效果则十分捉急。它毫无疑问地证明了**高级的人工智能必然依赖超大规模的神经网络**。

  


> 当前，ChatGPT 十分火爆，互联网上有很多的**类 ChatGPT 模型**出来，它们很多都是在利用小规模神经网络实现：
>
> -   开源 XX 模型，可将 ChatGPT 部署在 12G 大小的显卡上
> -   开源 YY 模型，只需一张 3090 显卡，人手一个 ChatGPT 的平替
>
> 你也可以根据我们的课程讲解自行判断，这些本地化部署的小规模神经网络到底是货真价实？还是哗众取宠？

  


## In-context learning 的局限性

上文所讲的例子都有一个特点，给模型的输入不自然，比较死板：

> 用户输入到 GPT-3：
> -     请把以下中文翻译成英文：
> -     苹果 => apple;
> -     你觉得 JioNLP 是个好用的工具吗？=>
>
> 而实际上，我们自然语言中很少用到 `=>`这种符号，用户告诉模型的输入是纯自然语言：
> -   翻译一下，中译英：你觉得 JioNLP 是个好用的工具吗？
> -   你觉得 JioNLP 是个好用的工具吗？给我翻译成英文
> -   ...

  


自然语言对事物的描述是非常灵活的，单纯 in-context learning 对如此灵活的用户指令还不够适应。

此外，小样本学习得到的 GPT 模型在做问答任务时，有时会编造虚构事实、产生有误的、带有歧视偏见的回答。

总而言之，这些回答与用户的需求不契合。这就是接下来一节要介绍的，由强化学习 RLHF 来完成。

  


# 总结

-   NLP 模型的训练阶段总共经历了4个阶段：纯监督学习、预训练+finetune、小样本学习、RLHF。
-   所有 GPT 模型都采用了基于大规模数据的语言模型预训练。
-   GPT3 使用了 in-context learning 的学习方式，扩增语言模型的学习能力。
-   In-context learning 对人类自然语言指令的响应仍有不足。

